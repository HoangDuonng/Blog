[{"content":"Dockerfile Contest 2025 – Extreme Python Optimization Dockerfile Contest 2025 encourages the Vietnamese DevOps community to rethink how Dockerfiles are written to achieve security, optimization, and clarity. Below is a dedicated summary for the Python category (FastAPI backend services).\nI. PYTHON Category (Optimization for Backend Services) The Python category focuses on image size optimization, security (CVE patching), and runtime performance for FastAPI apps. Solutions range from distroless, minimal Alpine, to wheel-based builds.\n1. Dockerfile TOP 1 (Python) – Thanh Nguyen The Technique Author\u0026rsquo;s explanation Reference UV Package Manager Use uv instead of pip to speed up dependency installs and manage virtual environments more efficiently. Distroless Base Image Use gcr.io/distroless/base-debian12:nonroot to reduce attack surface, without shell, package manager, or unnecessary tools. Multi-arch Support Support amd64 and arm64 by copying shared libraries per architecture. Shared Libraries Copy Copy required libraries (libc, libm, libz, libgcc_s) from the builder stage to run in distroless. Security Patching Upgrade starlette to 0.49.1 to fix CVE-2025-62727 and CVE-2025-54121 without editing pyproject.toml. LD_LIBRARY_PATH Set an env var so the system can find shared libraries in a custom folder. Dockerfile TOP 1 (Python)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 FROM ghcr.io/astral-sh/uv:python3.13-bookworm-slim@sha256:6b8ac7bb76766ffe9f6cc20f56789755d539e8d0e605d8983131227c5c8b87a1 AS builder ENV UV_LINK_MODE=copy ARG TARGETARCH # Copy shared libraries đủ để chạy ứng dụng trong môi trường distroless # Kiểm tra shared libraries cần thiết với lệnh: # ldd $(which python3) và các thư viện khác trong virtual environment sau khi cài đặt các package cần thiết (kiểm tra trước khi build) # Mỗi kiến trúc sẽ đặt thư viện trong các thư mục khác nhau, ví dụ: /lib/x86_64-linux-gnu/ cho amd64, /lib/aarch64-linux-gnu/ cho arm64 # do đó cần xác định kiến trúc và copy từ thư mục tương ứng. # TARGETARCH là built-in arg của docker buildx, tự động nhận giá trị (amd64 hoặc arm64) khi build multi-arch # Shared libraries copy ở lệnh phía dưới là chưa đủ để chạy ứng dụng, tuy nhiên gcr.io/distroless/base-debian12 (image sử dụng làm base image cho runtime tại runtime state) đã có sẵn một số shared libraries nên chỉ cần copy những thư viện còn thiếu. # gcr.io/distroless/base-debian12:nonroot không có shell, kiểm tra shared libraries bằng cách sử dụng gcr.io/distroless/base-debian12:debug # gcr.io/distroless/base-debian12:debug tương tự gcr.io/distroless/base-debian12:nonroot nhưng có thêm shell để phục vụ debug. RUN if [ \u0026#34;$TARGETARCH\u0026#34; = \u0026#34;amd64\u0026#34; ]; then \\ LIBARCH=\u0026#34;x86_64\u0026#34;; \\ elif [ \u0026#34;$TARGETARCH\u0026#34; = \u0026#34;arm64\u0026#34; ]; then \\ LIBARCH=\u0026#34;aarch64\u0026#34;; \\ else \\ LIBARCH=\u0026#34;unknown\u0026#34;; \\ fi \u0026amp;\u0026amp; \\ mkdir -p /lib/multi-arch \u0026amp;\u0026amp; \\ cp /lib/${LIBARCH}-linux-gnu/libc.so.6 /lib/multi-arch/ \u0026amp;\u0026amp; \\ cp /lib/${LIBARCH}-linux-gnu/libm.so.6 /lib/multi-arch/ \u0026amp;\u0026amp; \\ cp /lib/${LIBARCH}-linux-gnu/libz.so.1 /lib/multi-arch/ \u0026amp;\u0026amp; \\ cp /lib/${LIBARCH}-linux-gnu/libgcc_s.so.1 /lib/multi-arch/ WORKDIR /build # Sử dụng cache để tăng tốc độ build # Cài đặt dependencies trong uv virtual environment # Sử dụng mount type=bind để bind các file uv.lock và pyproject.toml từ host vào container mount thay vì copy. # --frozen để đảm bảo chỉ cài đặt đúng phiên bản dependencies trong uv.lock, không update uv.lock # --no-install-project để không cài đặt project hiện tại (chỉ cài đặt dependencies) # --no-dev để không cài đặt dev dependencies # --no-editable để không cài đặt editable mode # starlette 0.46.2 dính CVE-2025-62727 CVE-2025-54121, nâng cấp để vá lỗi bảo mật (do thay đổi pyproject.toml và file uv.lock sẽ vi phạm nội quy nên chạy lệnh install riêng) RUN --mount=type=cache,target=/root/.cache/uv \\ --mount=type=bind,source=uv.lock,target=uv.lock \\ --mount=type=bind,source=pyproject.toml,target=pyproject.toml \\ uv sync --frozen --no-install-project --no-dev --no-editable \u0026amp;\u0026amp; \\ uv pip install \u0026#34;starlette==0.49.1\u0026#34; --no-deps # Sử dụng distroless làm base image cho runtime để đảm bảo tính bảo mật và tối ưu kích thước image # Chọn distroless thay vì alpine vì alpine sử dụng musl libc, trong khi python và nhiều thư viện phổ biến trong python được biên dịch với glibc, dẫn đến các vấn đề tương thích. # distroless giúp ứng dụng chạy ổn định hơn và cũng rất nhẹ. # cc-debian12 có nhiều shared libraries cần thiết cho python và các package phổ biến hơn so với base-debian12 # tuy nhiên khi đã kiểm tra kỹ các shared libraries cần thiết (với lệnh ldd) và copy đầy đủ từ builder stage thì base-debian12 sẽ giúp tối ưu kích thước image hơn mà vẫn đảm bảo ứng dụng chạy ổn định. FROM gcr.io/distroless/base-debian12:nonroot@sha256:10136f394cbc891efa9f20974a48843f21a6b3cbde55b1778582195d6726fa85 AS runtime LABEL maintainer=\u0026#34;Thanh Nguyen The\u0026#34; LABEL maintainer.email=\u0026#34;thanhnt.devops@gmail.com\u0026#34; LABEL maintainer.company=\u0026#34;VIETNAM NATIONAL CYBER SECURITY TECHNOLOGY CORPORATION\u0026#34; LABEL maintainer.youtube=\u0026#34;DevOps Mentor\u0026#34; LABEL image.description=\u0026#34;Secure, minimal Python app using UV and Distroless\u0026#34; WORKDIR /app # Copy các thư viện và python từ builder stage COPY --from=builder /lib/multi-arch/ /lib/multi-arch/ COPY --from=builder /usr/local/lib/libpython3.13.so.1.0 /usr/local/lib/libpython3.13.so.1.0 COPY --from=builder /usr/local/lib/python3.13/ /usr/local/lib/python3.13/ COPY --from=builder /usr/local/bin/python /usr/local/bin/python3 # Copy virtual environment từ builder COPY --from=builder --chown=nonroot:nonroot /build/.venv/ /app/.venv/ # Copy source code - chỉ copy những gì cần thiết COPY --chown=nonroot:nonroot src/ ./src/ # Thiết lập environment variables # Do runtime limit là 1 vCPU, 512MB RAM nên thiết lập WORKERS=2 thay vì 3 (nguy cơ OOM). Công thức worker = (2 x số lượng vCPU + 1) chỉ áp dụng trong trường hợp \u0026gt; 1GB RAM # LD_LIBRARY_PATH để hệ thống có thể tìm thấy các shared libraries cần thiết tại thư mục mới thay vì thư mục mặc đinh (/lib/x86_64-linux-gnu hoặc /lib/aarch64-linux-gnu) # shared libraries không có trong /lib/multi-arch sẽ tiếp tục được load từ thư mục mặc định của hệ thống ENV PATH=\u0026#34;/app/.venv/bin/:$PATH\u0026#34; \\ PYTHONPATH=\u0026#34;/app/src/\u0026#34; \\ LANG=C.UTF-8 \\ PYTHONUNBUFFERED=1 \\ PYTHONFAULTHANDLER=1 \\ PYTHONDONTWRITEBYTECODE=1 \\ PYTHONHASHSEED=random \\ HOST=0.0.0.0 \\ PORT=8080 \\ WORKERS=2 \\ LOGGING__LEVEL=INFO \\ LOGGING__FORMAT=PLAIN \\ COFFEE_API__HOST=\u0026#34;https://api.sampleapis.com/coffee/\u0026#34; \\ APP_VERSION=0.1.0 \\ GIT_COMMIT_SHA=sha \\ LD_LIBRARY_PATH=/lib/multi-arch # nonroot user mặc định đã được sử dụng trong distroless base-debian12:nonroot nên không cần thiết phải thêm lệnh phía dưới # USER nonroot:nonroot # Expose port mặc định EXPOSE 8080 # Command để chạy ứng dụng ENTRYPOINT [\u0026#34;python\u0026#34;, \u0026#34;src/python_service_template/app.py\u0026#34;] 2. Dockerfile TOP 2 (Python) – newnol Technique Author\u0026rsquo;s explanation Reference Alpine Base Image Use python:3.13-alpine for a smaller image than Debian-based images. Security Patches Update packages with CVEs: starlette, fastapi, aiohttp, pydantic, structlog, uvloop, uvicorn to safe versions. Ultra Aggressive Optimization Strip all .so files, remove __pycache__, tests, docs, examples, typing stubs, license files to reduce size. Python Stdlib Cleanup Remove modules not needed like pip, setuptools, wheel, tkinter, distutils, lib2to3, idlelib, test, unittest. Non-root User Create UID 10001 user with minimum privileges for security. Healthcheck Use wget to check /health/ endpoint with short timeout. Dockerfile TOP 2 (Python)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 # syntax=docker/dockerfile:1.7 # ============================================================================= # DOCKERFILE ULTRA OPTIMIZED + SECURITY PATCHED # Mục tiêu: Nhẹ (\u0026lt;110MB) + Bảo mật cao (0 CVEs) # ============================================================================= # ----------------------------------------------------------------------------- # Stage 1: Dependencies Builder # ----------------------------------------------------------------------------- FROM python:3.13-alpine@sha256:e5fa639e49b85986c4481e28faa2564b45aa8021413f31026c3856e5911618b1 AS deps ENV PIP_NO_CACHE_DIR=1 \\ PIP_DISABLE_PIP_VERSION_CHECK=1 \\ PYTHONDONTWRITEBYTECODE=1 \\ PYTHONUNBUFFERED=1 RUN --mount=type=cache,target=/var/cache/apk \\ apk add --no-cache --virtual .build-deps \\ build-base \\ python3-dev \\ cargo # Install dependencies với PATCHED versions để fix CVEs # Note: FastAPI 0.116+ required for starlette 0.49.1+ support RUN --mount=type=cache,target=/root/.cache/pip \\ python -m pip install --no-cache-dir --prefix=/install \\ \u0026#34;aiohttp\u0026gt;=3.12.14,\u0026lt;4.0.0\u0026#34; \\ \u0026#34;asgi-correlation-id\u0026gt;=4.3.4,\u0026lt;5.0.0\u0026#34; \\ \u0026#34;fastapi\u0026gt;=0.116.0\u0026#34; \\ \u0026#34;prometheus-fastapi-instrumentator\u0026gt;=7.0.0,\u0026lt;8.0.0\u0026#34; \\ \u0026#34;pydantic\u0026gt;=2.11.0,\u0026lt;3.0.0\u0026#34; \\ \u0026#34;pydantic-settings\u0026gt;=2.9.1,\u0026lt;3.0.0\u0026#34; \\ \u0026#34;structlog\u0026gt;=25.3.0,\u0026lt;26.0.0\u0026#34; \\ \u0026#34;uvloop\u0026gt;=0.21.0,\u0026lt;0.22.0\u0026#34; \\ \u0026#34;uvicorn[standard]\u0026gt;=0.30.0,\u0026lt;0.31.0\u0026#34; # ULTRA AGGRESSIVE optimization RUN apk add --no-cache binutils \\ # Strip ALL .so files aggressively \u0026amp;\u0026amp; find /install -type f \\( -name \u0026#39;*.so*\u0026#39; -o -name \u0026#39;*.a\u0026#39; \\) -exec strip --strip-all {} + 2\u0026gt;/dev/null || true \\ # Remove all bytecode \u0026amp;\u0026amp; find /install \\( -type d -name __pycache__ -o -type f -name \u0026#39;*.py[co]\u0026#39; \\) -delete 2\u0026gt;/dev/null || true \\ # Remove test/doc/examples \u0026amp;\u0026amp; find /install -type d \\( -name tests -o -name testing -o -name test -o -name doc -o -name docs -o -name example -o -name examples \\) -prune -exec rm -rf {} + 2\u0026gt;/dev/null || true \\ # Minimize .dist-info \u0026amp;\u0026amp; find /install -name \u0026#39;*.dist-info\u0026#39; -type d -exec sh -c \u0026#39;cd \u0026#34;$1\u0026#34; \u0026amp;\u0026amp; find . -type f ! -name \u0026#34;METADATA\u0026#34; ! -name \u0026#34;top_level.txt\u0026#34; ! -name \u0026#34;RECORD\u0026#34; -delete\u0026#39; _ {} \\; 2\u0026gt;/dev/null || true \\ # Remove typing stubs, headers, C files \u0026amp;\u0026amp; find /install -type f \\( -name \u0026#39;*.pyi\u0026#39; -o -name \u0026#39;*.c\u0026#39; -o -name \u0026#39;*.h\u0026#39; -o -name \u0026#39;*.cpp\u0026#39; -o -name \u0026#39;*.cc\u0026#39; \\) -delete \\ # Remove license files \u0026amp;\u0026amp; find /install -type f \\( -name \u0026#39;LICENSE*\u0026#39; -o -name \u0026#39;COPYING*\u0026#39; -o -name \u0026#39;NOTICE*\u0026#39; -o -name \u0026#39;AUTHORS*\u0026#39; -o -name \u0026#39;CHANGELOG*\u0026#39; -o -name \u0026#39;README*\u0026#39; \\) -delete 2\u0026gt;/dev/null || true \\ \u0026amp;\u0026amp; apk del binutils .build-deps # ----------------------------------------------------------------------------- # Stage 2: Runtime (ULTRA MINIMAL + SECURE) # ----------------------------------------------------------------------------- FROM python:3.13-alpine@sha256:e5fa639e49b85986c4481e28faa2564b45aa8021413f31026c3856e5911618b1 AS runtime LABEL org.opencontainers.image.title=\u0026#34;Python Service Template\u0026#34; \\ org.opencontainers.image.description=\u0026#34;Production-ready FastAPI service - Optimized \u0026amp; Secured\u0026#34; \\ org.opencontainers.image.version=\u0026#34;0.1.0\u0026#34; \\ org.opencontainers.image.authors=\u0026#34;newnol \u0026lt;contact@newnol.io.vn\u0026gt;\u0026#34; \\ maintainer=\u0026#34;newnol\u0026#34; \\ security.scan=\u0026#34;trivy-passed\u0026#34; ENV PYTHONDONTWRITEBYTECODE=1 \\ PYTHONUNBUFFERED=1 \\ PIP_NO_CACHE_DIR=1 \\ HOST=0.0.0.0 \\ PORT=5000 \\ WORKERS=1 \\ PYTHONPATH=/app/src \\ TZ=UTC # Install ONLY wget for healthcheck RUN --mount=type=cache,target=/var/cache/apk \\ apk add --no-cache wget WORKDIR /app # Create non-root user RUN addgroup -g 10001 -S app \\ \u0026amp;\u0026amp; adduser -u 10001 -S -G app -h /app -s /sbin/nologin app # Copy dependencies COPY --from=deps --chown=app:app /install /usr/local # Copy source (minimal) COPY --chown=app:app src/ ./src/ # Permissions RUN chmod -R 550 /app # EXTREME Python stdlib cleanup RUN rm -rf \\ /usr/local/lib/python3.13/ensurepip \\ /usr/local/lib/python3.13/site-packages/pip* \\ /usr/local/lib/python3.13/site-packages/setuptools* \\ /usr/local/lib/python3.13/site-packages/wheel* \\ /usr/local/lib/python3.13/distutils \\ /usr/local/lib/python3.13/lib2to3 \\ /usr/local/lib/python3.13/idlelib \\ /usr/local/lib/python3.13/tkinter \\ /usr/local/lib/python3.13/turtledemo \\ /usr/local/lib/python3.13/test \\ /usr/local/lib/python3.13/unittest/test \\ /usr/local/bin/pip* \\ /usr/local/bin/2to3* \\ /usr/local/bin/idle* \\ 2\u0026gt;/dev/null || true # Clean up more unused stdlib modules RUN cd /usr/local/lib/python3.13 \u0026amp;\u0026amp; rm -rf \\ turtle.py \\ pydoc_data \\ 2\u0026gt;/dev/null || true USER app EXPOSE 5000 # Heathcheck HEALTHCHECK --interval=15s --timeout=3s --start-period=10s --retries=2 \\ CMD wget --no-verbose --tries=1 -O /dev/null http://127.0.0.1:${PORT}/health/ || exit 1 STOPSIGNAL SIGTERM CMD [\u0026#34;python\u0026#34;, \u0026#34;src/python_service_template/app.py\u0026#34;] 3. Dockerfile TOP (Python) – Wheel-based Build Technique Explanation Reference Wheel-based Installation Build all dependencies into wheels, then install offline to speed up builds and ensure reproducibility. Dynamic Security Patching Use a Python script to parse pyproject.toml and upgrade fastapi and starlette to safe versions without editing the original file. PIP_ONLY_BINARY Use wheel files only, avoid building from source, faster builds and fewer compile errors. Offline Installation Install from local wheels without internet in the runtime stage. Native Healthcheck Use Python http.client instead of external tools like curl/wget, reducing dependencies. Non-root User Create UID 10001 user with a dedicated home directory for security. Dockerfile TOP (Python)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 # syntax=docker/dockerfile:1.7 # ############################## # builder ############################## FROM python:3.13-slim AS builder ENV PIP_DISABLE_PIP_VERSION_CHECK=1 \\ PIP_NO_CACHE_DIR=1 \\ PIP_ONLY_BINARY=:all: \\ PYTHONDONTWRITEBYTECODE=1 \\ PYTHONUNBUFFERED=1 WORKDIR /app # Pre-cache manifests COPY pyproject.toml README.md LICENSE* ./ # Fix HIGH vulnerable issue: CVE-2025-62727 by upgrading starlette and fastapi. RUN --mount=type=cache,target=/root/.cache/pip python - \u0026lt;\u0026lt;\u0026#39;PY\u0026#39; import tomllib, pathlib, re def parse_req(s:str): m = re.match(r\u0026#39;^\\s*([A-Za-z0-9_.-]+)(\\[[^\\]]+\\])?\\s*(.*)$\u0026#39;, s) if m: name, extras, rest = m.group(1), (m.group(2) or \u0026#39;\u0026#39;), (m.group(3) or \u0026#39;\u0026#39;) return name, extras, rest name = re.split(r\u0026#39;[\u0026gt;\u0026lt;=~!; ]\u0026#39;, s, 1)[0] return name, \u0026#39;\u0026#39;, s[len(name):] data = tomllib.loads(pathlib.Path(\u0026#39;pyproject.toml\u0026#39;).read_text()) deps = data.get(\u0026#39;project\u0026#39;, {}).get(\u0026#39;dependencies\u0026#39;, []) safe = [] present = set() for d in deps: name, extras, rest = parse_req(d) norm = name.lower().replace(\u0026#39;_\u0026#39;,\u0026#39;-\u0026#39;) if norm == \u0026#39;fastapi\u0026#39;: safe.append(f\u0026#39;fastapi{extras}\u0026gt;=0.118,\u0026lt;0.121\u0026#39;) else: safe.append(d) present.add(norm) if \u0026#39;starlette\u0026#39; not in present: safe.append(\u0026#39;starlette\u0026gt;=0.49.1,\u0026lt;0.50\u0026#39;) pathlib.Path(\u0026#39;/requirements.safe.txt\u0026#39;).write_text(\u0026#39;\\n\u0026#39;.join(safe) + \u0026#39;\\n\u0026#39;) print(\u0026#39;Resolved safe deps:\u0026#39;, *safe, sep=\u0026#39;\\n- \u0026#39;) PY # Wheel ALL dependencies from the safe list RUN --mount=type=cache,target=/root/.cache/pip \\ pip wheel --wheel-dir /wheels -r /requirements.safe.txt # Build wheel of the project itself COPY src/ ./src/ RUN --mount=type=cache,target=/root/.cache/pip \\ pip wheel --wheel-dir /wheels . ############################## # runtime ############################## FROM python:3.13-slim AS runtime ARG VERSION=0.1.0 ARG VCS_REF=sha ARG BUILD_DATE LABEL org.opencontainers.image.title=\u0026#34;python-service-template\u0026#34; \\ org.opencontainers.image.description=\u0026#34;Dockerfile contest build\u0026#34; \\ org.opencontainers.image.version=$VERSION \\ org.opencontainers.image.revision=$VCS_REF \\ org.opencontainers.image.created=$BUILD_DATE \\ org.opencontainers.image.licenses=\u0026#34;Apache-2.0\u0026#34; ENV PYTHONDONTWRITEBYTECODE=1 \\ PYTHONUNBUFFERED=1 \\ PYTHONOPTIMIZE=2 \\ HOST=0.0.0.0 \\ PORT=5000 \\ WORKERS=1 \\ APP_VERSION=$VERSION \\ GIT_COMMIT_SHA=$VCS_REF # Non-root RUN useradd --create-home --uid 10001 --shell /usr/sbin/nologin appuser WORKDIR /home/appuser # Install offline: install ALL safe deps, then the app wheel with --no-deps COPY --from=builder /wheels /wheels COPY --from=builder /requirements.safe.txt /requirements.safe.txt RUN pip install --no-index --find-links=/wheels -r /requirements.safe.txt \\ \u0026amp;\u0026amp; pip install --no-index --find-links=/wheels --no-deps \\ python-service-template --no-compile \\ \u0026amp;\u0026amp; rm -rf /wheels /requirements.safe.txt EXPOSE 5000 HEALTHCHECK --interval=30s --timeout=2s --start-period=10s --retries=3 \\ CMD python -c \u0026#34;import sys, http.client; c=http.client.HTTPConnection(\u0026#39;127.0.0.1\u0026#39;, int(__import__(\u0026#39;os\u0026#39;).environ.get(\u0026#39;PORT\u0026#39;,\u0026#39;5000\u0026#39;)), timeout=1); c.request(\u0026#39;GET\u0026#39;,\u0026#39;/health\u0026#39;); r=c.getresponse(); sys.exit(0 if r.status==200 else 1)\u0026#34; || exit 1 USER 10001:10001 CMD [\u0026#34;python\u0026#34;, \u0026#34;-m\u0026#34;, \u0026#34;python_service_template.app\u0026#34;] 4. Dockerfile TOP (Python) – Khiem Doan Technique Explanation Reference UV Package Manager Use uv to manage dependencies faster than pip, with cache mounts for quicker rebuilds. Alpine + Tini Use lightweight Alpine Linux and tini as init system for proper signal handling. Security Patching Upgrade starlette to 0.50.0 to fix CVEs without editing pyproject.toml. Non-root User Create nonroot user with UID/GID 14406, an uncommon UID to avoid conflicts. Healthcheck with curl Use curl to check the health endpoint and grep to verify JSON response. OCI Labels Add full OCI labels with metadata: version, build date, revision, source. Dockerfile TOP (Python)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 # syntax=docker/dockerfile:1.19 FROM python:3.13-alpine3.22 AS deps RUN --mount=type=cache,target=/root/.cache/pip \\ pip install --no-compile uv==0.9.2 WORKDIR /app COPY pyproject.toml uv.lock ./ RUN --mount=type=cache,target=/root/.cache/uv \\ uv sync --frozen --no-dev --no-install-project \\ \u0026amp;\u0026amp; uv pip install starlette==0.50.0 FROM python:3.13-alpine3.22 AS final RUN pip install -U pip RUN --mount=type=cache,target=/var/cache/apk \\ apk add --no-cache \\ curl=8.14.1-r2 \\ tini=0.19.0-r3 ARG VERSION=\u0026#34;0.1.0\u0026#34; ARG BUILD_DATE=\u0026#34;2025-11-10T00:00:00Z\u0026#34; ARG REVISION=\u0026#34;unknown\u0026#34; ARG GIT_COMMIT_SHA=\u0026#34;unknown\u0026#34; LABEL org.opencontainers.image.title=\u0026#34;python-service-template\u0026#34; \\ org.opencontainers.image.description=\u0026#34;A batteries-included template for building robust, production-ready Python backend services with FastAPI\u0026#34; \\ org.opencontainers.image.authors=\u0026#34;Khiem Doan\u0026#34; \\ org.opencontainers.image.version=$VERSION \\ org.opencontainers.image.created=$BUILD_DATE \\ org.opencontainers.image.revision=$REVISION \\ org.opencontainers.image.source=\u0026#34;https://github.com/khiemdoan/\u0026#34; \\ org.opencontainers.image.licenses=\u0026#34;MIT\u0026#34; ENV USER=nonroot \\ GROUP=nonroot \\ UID=14406 \\ GID=14406 RUN addgroup -g \u0026#34;$GID\u0026#34; \u0026#34;$GROUP\u0026#34; \\ \u0026amp;\u0026amp; adduser -D -u \u0026#34;$UID\u0026#34; -G \u0026#34;$GROUP\u0026#34; \u0026#34;$USER\u0026#34; USER $USER WORKDIR /app COPY --chown=$USER:$GROUP src/ src/ COPY --from=deps --chown=$USER:$GROUP /app/.venv /app/.venv ENV PATH=\u0026#34;/app/.venv/bin:$PATH\u0026#34; \\ PYTHONPATH=\u0026#34;/app/src\u0026#34; \\ PYTHONUNBUFFERED=1 \\ PYTHONDONTWRITEBYTECODE=1 \\ HOST=0.0.0.0 \\ PORT=3000 \\ WORKERS=1 \\ LOGGING__LEVEL=INFO \\ LOGGING__FORMAT=PLAIN \\ COFFEE_API__HOST=https://api.sampleapis.com/coffee/ \\ APP_VERSION=$VERSION \\ GIT_COMMIT_SHA=$GIT_COMMIT_SHA EXPOSE $PORT HEALTHCHECK --timeout=1s \\ CMD curl -f \u0026#34;http://localhost:${PORT}/health/\u0026#34; | grep \u0026#39;\u0026#34;heartbeat\u0026#34;:\u0026#34;HEALTHY\u0026#34;\u0026#39; || exit 1 ENTRYPOINT [\u0026#34;/sbin/tini\u0026#34;, \u0026#34;--\u0026#34;] CMD [\u0026#34;python\u0026#34;, \u0026#34;src/python_service_template/app.py\u0026#34;] Deployment Notes The Dockerfiles above are architectural references only; when applying them to your Python/FastAPI project, keep the core principles: multi-stage build, security patching, non-root user, pin SHA256 for base images. Distroless vs Alpine: Distroless is more secure (no shell, package manager) but requires manual shared library copying. Alpine is lighter and easier to debug but may have compatibility issues with some Python packages. UV vs PIP: UV is significantly faster than pip (10-100x) and manages virtual environments better, but requires extra setup. Security: Always update dependencies to patch CVEs, especially common packages like starlette, fastapi, uvicorn. Healthcheck: Prefer native Python http.client or curl/wget depending on the base image and security requirements. ","date":"2025-11-23T00:00:00Z","image":"https://tech.nguuyen.io.vn/images/docker-optimization/docker-optimization-python.webp","permalink":"https://tech.nguuyen.io.vn/en/posts/docker-optimization/docker-opt-python/","title":"Optimize Docker for Python"},{"content":"Dockerfile Contest 2025 – Extreme React/Node Optimization Dockerfile Contest 2025 encourages the Vietnamese DevOps community to rethink how Dockerfiles are written to achieve security, optimization, and clarity. Below is a dedicated summary for the React category (Node.js apps built into static assets).\nI. REACT Category (Optimization for Static Web Serving) The React category focuses on reducing image size and speeding up static file delivery. Some teams compile their own HTTP server or run with FROM scratch to achieve the smallest possible footprint.\n1. Lightest Docker Image Award (Top Slim) – Nguyễn Phúc Bảo Lâm Technique Author\u0026rsquo;s explanation Reference Project Selection Leverage the advantage of static apps (React) to reach smaller images than Java/Python. Extreme Base Image Use lipanski/docker-static-website:latest, a BusyBox build trimmed down to only an HTTP server (~92 KB). Maximum Pre-compression Pre-compress all assets with Gzip level 9, delete original files, final image ~300 KB. Healthcheck Write \u0026quot;OK\u0026quot; to dist/health for a quick probe endpoint. Dockerfile – Lightest Docker Image\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 # syntax=docker/dockerfile:1.7 # ============================================================================== # Build Stage - Using Node Alpine for minimal size # ============================================================================== FROM node:22.21.1-alpine3.21@sha256:af8023ec879993821f6d5b21382ed915622a1b0f1cc03dbeb6804afaf01f8885 AS builder # Install pnpm with specific version from package.json and gzip for pre-compression ENV PNPM_HOME=\u0026#34;/pnpm\u0026#34; ENV PATH=\u0026#34;$PNPM_HOME:$PATH\u0026#34; RUN corepack enable \u0026amp;\u0026amp; \\ corepack prepare pnpm --activate WORKDIR /app # Copy package files for dependency installation (optimized layer caching) COPY package.json pnpm-lock.yaml ./ # Install dependencies with cache mount for faster rebuilds # Installs all dependencies (including devDependencies needed for build: typescript, vite, tailwindcss, etc.) RUN --mount=type=cache,id=pnpm,target=/pnpm/store \\ pnpm install --frozen-lockfile # Copy only necessary source files (exclude tests, docs, config files not needed for build) COPY tsconfig.json tsconfig.node.json vite.config.ts tailwind.config.ts postcss.config.js ./ COPY index.html ./ COPY public ./public COPY src ./src # Build the application RUN pnpm run build \u0026amp;\u0026amp; \\ # Verify build output exists test -d dist \u0026amp;\u0026amp; test -f dist/index.html \u0026amp;\u0026amp; \\ # Remove bundle visualizer output (not needed in production, saves ~100KB compressed) rm -f dist/stats.html \u0026amp;\u0026amp; \\ # Create a minimal health check endpoint (1 byte file for ultra-fast response) echo \u0026#34;OK\u0026#34; \u0026gt; dist/health \u0026amp;\u0026amp; \\ # Pre-compress all static files with gzip (level 9 = maximum compression) find dist -type f \\( \\ -name \u0026#34;*.html\u0026#34; -o \\ -name \u0026#34;*.css\u0026#34; -o \\ -name \u0026#34;*.js\u0026#34; -o \\ -name \u0026#34;*.json\u0026#34; -o \\ -name \u0026#34;*.xml\u0026#34; -o \\ -name \u0026#34;*.txt\u0026#34; -o \\ -name \u0026#34;*.svg\u0026#34; \\ \\) -exec sh -c \u0026#39;gzip -9 \u0026#34;{}\u0026#34;\u0026#39; \\; # ============================================================================== # Production Stage - Using lipanski/docker-static-website for extreme minimal footprint (92.5 KB base) # ============================================================================== FROM lipanski/docker-static-website:latest AS production # Add OCI labels for metadata LABEL org.opencontainers.image.title=\u0026#34;Vite React Template\u0026#34; \\ org.opencontainers.image.description=\u0026#34;Production-ready Vite React application with extreme minimal footprint\u0026#34; \\ org.opencontainers.image.version=\u0026#34;0.4.0\u0026#34; \\ org.opencontainers.image.licenses=\u0026#34;MIT OR Apache-2.0\u0026#34; \\ org.opencontainers.image.base.name=\u0026#34;lipanski/docker-static-website:latest\u0026#34; # Copy built assets from builder stage # lipanski/docker-static-website serves from /home/static COPY --from=builder /app/dist /home/static # Expose port (BusyBox httpd uses port 3000 by default) EXPOSE 3000 # The base image already has CMD set to run BusyBox httpd # It automatically serves .gz files when Accept-Encoding: gzip is present # No additional configuration needed - inherited from base image 2. Dockerfile TOP 1 (React) – Nguyễn Hữu Phương Technique Author\u0026rsquo;s explanation Reference FROM SCRATCH \u0026amp; Static Linking Final stage is scratch, so Nginx must be built statically in Stage 2. Nginx Binary Optimization Disable \u0026gt;30 modules, shrink binary ~76% (5.2 MB). Binary Compression (UPX) Use upx --best --lzma, reduce another ~56%. Parallel Compression Run Gzip and Brotli in parallel to shift CPU to build-time. Base Image Security Pin SHA256 for all base images. Minimal Healthcheck Use nginx -t -q, no curl/wget required. Dockerfile TOP 1 (React)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 # syntax=docker/dockerfile:1.7 # Multi-arch support: Automatically provided by buildx ARG BUILDPLATFORM ARG TARGETPLATFORM ARG TARGETARCH ARG BUILD_DATE ARG GIT_COMMIT=unknown ARG NGINX_VERSION=1.26.2 ARG NODE_VERSION=20 ARG ALPINE_VERSION=3.20 # ============================================================================== # Stage 1: Application Build # ============================================================================== FROM node:${NODE_VERSION}-alpine@sha256:2d5e8a8a51bc341fd5f2eed6d91455c3a3d147e91a14298fc564b5dc519c1666 AS builder WORKDIR /app # Setup pnpm with corepack ENV PNPM_HOME=\u0026#34;/pnpm\u0026#34; \\ PATH=\u0026#34;$PNPM_HOME:$PATH\u0026#34; RUN corepack enable \u0026amp;\u0026amp; corepack prepare pnpm@9.12.2 --activate # Install dependencies with cache mount COPY package.json pnpm-lock.yaml .npmrc ./ RUN --mount=type=cache,id=pnpm,target=/pnpm/store \\ pnpm install --frozen-lockfile --prefer-offline # Copy source and build configuration COPY tsconfig.json tsconfig.node.json vite.config.ts ./ COPY postcss.config.js tailwind.config.ts biome.json ./ COPY index.html ./ COPY public ./public COPY src ./src # Build and clean artifacts ENV NODE_ENV=production RUN pnpm build \u0026amp;\u0026amp; \\ find dist -type f \\( -name \u0026#34;*.map\u0026#34; -o -name \u0026#34;.*\u0026#34; \\) -delete \u0026amp;\u0026amp; \\ rm -f dist/stats.html # ============================================================================== # Stage 2: Static Nginx Binary Builder # ============================================================================== FROM alpine:${ALPINE_VERSION}@sha256:765942a4039992336de8dd5db680586e1a206607dd06170ff0a37267a9e01958 AS nginx-builder ARG NGINX_VERSION ARG TARGETPLATFORM ARG BUILDPLATFORM ENV NGINX_SHA256=627fe086209bba80a2853a0add9d958d7ebbdffa1a8467a5784c9a6b4f03d738 # Log build platform info for multi-arch RUN echo \u0026#34;Building on $BUILDPLATFORM for $TARGETPLATFORM\u0026#34; # Install build dependencies RUN apk add --no-cache \\ gcc g++ musl-dev make linux-headers curl \\ pcre-dev pcre2-dev zlib-dev zlib-static \\ openssl-dev openssl-libs-static upx # Download and verify nginx WORKDIR /tmp RUN curl -fSL \u0026#34;https://nginx.org/download/nginx-${NGINX_VERSION}.tar.gz\u0026#34; -o nginx.tar.gz \u0026amp;\u0026amp; \\ echo \u0026#34;${NGINX_SHA256} nginx.tar.gz\u0026#34; | sha256sum -c - # Build fully static nginx with minimal modules RUN tar -xzf nginx.tar.gz \u0026amp;\u0026amp; \\ cd \u0026#34;nginx-${NGINX_VERSION}\u0026#34; \u0026amp;\u0026amp; \\ ./configure \\ --prefix=/usr/local/nginx \\ --sbin-path=/usr/local/nginx/sbin/nginx \\ --conf-path=/etc/nginx/nginx.conf \\ --pid-path=/run/nginx.pid \\ --lock-path=/run/nginx.lock \\ --error-log-path=/dev/stderr \\ --http-log-path=/dev/stdout \\ --user=nobody \\ --group=nobody \\ # Performance features --with-threads \\ --with-file-aio \\ --with-http_ssl_module \\ --with-http_v2_module \\ --with-http_gzip_static_module \\ --with-http_stub_status_module \\ --with-pcre \\ --with-pcre-jit \\ # Static linking and optimization --with-cc-opt=\u0026#39;-static -Os -ffunction-sections -fdata-sections\u0026#39; \\ --with-ld-opt=\u0026#39;-static -Wl,--gc-sections\u0026#39; \\ # Disable unnecessary modules --without-http_charset_module \\ --without-http_ssi_module \\ --without-http_userid_module \\ --without-http_auth_basic_module \\ --without-http_mirror_module \\ --without-http_autoindex_module \\ --without-http_geo_module \\ --without-http_map_module \\ --without-http_split_clients_module \\ --without-http_referer_module \\ --without-http_rewrite_module \\ --without-http_proxy_module \\ --without-http_fastcgi_module \\ --without-http_uwsgi_module \\ --without-http_scgi_module \\ --without-http_grpc_module \\ --without-http_memcached_module \\ --without-http_limit_conn_module \\ --without-http_limit_req_module \\ --without-http_empty_gif_module \\ --without-http_browser_module \\ --without-http_upstream_hash_module \\ --without-http_upstream_ip_hash_module \\ --without-http_upstream_least_conn_module \\ --without-http_upstream_random_module \\ --without-http_upstream_keepalive_module \\ --without-http_upstream_zone_module \\ --without-mail_pop3_module \\ --without-mail_imap_module \\ --without-mail_smtp_module \\ --without-stream_limit_conn_module \\ --without-stream_access_module \\ --without-stream_geo_module \\ --without-stream_map_module \\ --without-stream_split_clients_module \\ --without-stream_return_module \\ --without-stream_set_module \\ --without-stream_upstream_hash_module \\ --without-stream_upstream_least_conn_module \\ --without-stream_upstream_random_module \\ --without-stream_upstream_zone_module \u0026amp;\u0026amp; \\ make -j\u0026#34;$(nproc)\u0026#34; \u0026amp;\u0026amp; \\ make install # Optimize binary: strip symbols + UPX compression RUN strip --strip-all /usr/local/nginx/sbin/nginx \u0026amp;\u0026amp; \\ upx --best --lzma /usr/local/nginx/sbin/nginx \u0026amp;\u0026amp; \\ /usr/local/nginx/sbin/nginx -V # ============================================================================== # Stage 3: Asset Compression # ============================================================================== FROM alpine:${ALPINE_VERSION}@sha256:765942a4039992336de8dd5db680586e1a206607dd06170ff0a37267a9e01958 AS compressor RUN apk add --no-cache brotli gzip findutils WORKDIR /app COPY --from=builder /app/dist ./dist # Parallel compression: gzip + brotli for all text-based assets RUN find dist -type f \\ \\( -name \u0026#34;*.html\u0026#34; -o -name \u0026#34;*.css\u0026#34; -o -name \u0026#34;*.js\u0026#34; -o \\ -name \u0026#34;*.json\u0026#34; -o -name \u0026#34;*.svg\u0026#34; -o -name \u0026#34;*.xml\u0026#34; \\) \\ -print0 | xargs -0 -P\u0026#34;$(nproc)\u0026#34; -I {} sh -c \u0026#39;gzip -9 -k -f \u0026#34;{}\u0026#34; \u0026amp;\u0026amp; brotli -q 11 -f \u0026#34;{}\u0026#34;\u0026#39; # ============================================================================== # Stage 4: Minimal Filesystem Preparation # ============================================================================== FROM alpine:${ALPINE_VERSION}@sha256:765942a4039992336de8dd5db680586e1a206607dd06170ff0a37267a9e01958 AS rootfs # Create directory structure RUN mkdir -p \\ /rootfs/etc/nginx/conf.d \\ /rootfs/usr/share/nginx/html \\ /rootfs/var/log/nginx \\ /rootfs/var/cache/nginx \\ /rootfs/usr/local/nginx/{client_body,proxy,fastcgi,uwsgi,scgi}_temp \\ /rootfs/tmp \\ /rootfs/run \u0026amp;\u0026amp; \\ chmod 1777 /rootfs/tmp # Create minimal user database (nobody user) RUN echo \u0026#34;nobody:x:65534:65534:nobody:/:/sbin/nologin\u0026#34; \u0026gt; /rootfs/etc/passwd \u0026amp;\u0026amp; \\ echo \u0026#34;nobody:x:65534:\u0026#34; \u0026gt; /rootfs/etc/group # Copy nginx configuration COPY nginx.conf /rootfs/etc/nginx/conf.d/default.conf COPY --from=nginx-builder /etc/nginx/mime.types /rootfs/etc/nginx/mime.types COPY --from=compressor /app/dist /rootfs/usr/share/nginx/html # Create main nginx.conf RUN cat \u0026gt; /rootfs/etc/nginx/nginx.conf \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; worker_processes auto; error_log stderr warn; pid /run/nginx.pid; events { worker_connections 1024; use epoll; multi_accept on; } http { include /etc/nginx/mime.types; default_type application/octet-stream; access_log /dev/stdout; # Performance optimizations sendfile on; tcp_nopush on; tcp_nodelay on; keepalive_timeout 65; server_tokens off; # Compression gzip on; gzip_static on; gzip_vary on; gzip_types text/plain text/css application/json application/javascript text/xml application/xml image/svg+xml; include /etc/nginx/conf.d/*.conf; } EOF # Set proper ownership RUN chown -R 65534:65534 \\ /rootfs/usr/share/nginx/html \\ /rootfs/var/log/nginx \\ /rootfs/var/cache/nginx \\ /rootfs/usr/local/nginx \\ /rootfs/tmp \\ /rootfs/run # ============================================================================== # Stage 5: Final Distroless Image (FROM SCRATCH) # ============================================================================== FROM scratch # Re-declare build args for metadata ARG BUILD_DATE ARG GIT_COMMIT=unknown # OCI metadata labels LABEL org.opencontainers.image.title=\u0026#34;Vite React - Distroless\u0026#34; \\ org.opencontainers.image.description=\u0026#34;Distroless minimal image (\u0026lt;6MB) - UPX compressed\u0026#34; \\ org.opencontainers.image.version=\u0026#34;2.2.0-distroless-upx\u0026#34; \\ org.opencontainers.image.created=\u0026#34;${BUILD_DATE}\u0026#34; \\ org.opencontainers.image.revision=\u0026#34;${GIT_COMMIT}\u0026#34; \\ org.opencontainers.image.base.name=\u0026#34;scratch\u0026#34; \\ org.opencontainers.image.source=\u0026#34;https://github.com/riipandi/vite-react-template\u0026#34; \\ org.opencontainers.image.licenses=\u0026#34;MIT OR Apache-2.0\u0026#34; \\ maintainer=\u0026#34;contest-2025-optimized\u0026#34; # Copy static nginx binary and minimal filesystem COPY --from=nginx-builder /usr/local/nginx/sbin/nginx /usr/sbin/nginx COPY --from=rootfs /rootfs / # Run as non-root user (nobody = 65534) USER 65534:65534 EXPOSE 3000 # Lightweight healthcheck using nginx config test HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \\ CMD [\u0026#34;/usr/sbin/nginx\u0026#34;, \u0026#34;-t\u0026#34;, \u0026#34;-q\u0026#34;] STOPSIGNAL SIGTERM ENTRYPOINT [\u0026#34;/usr/sbin/nginx\u0026#34;] CMD [\u0026#34;-g\u0026#34;, \u0026#34;daemon off;\u0026#34;] 3. Dockerfile TOP 2 (React) – Trần Quốc Toàn Technique Author\u0026rsquo;s explanation Reference Native C Healthcheck Write a C program that opens a socket to port 3000, returns exit code 0/1. Collect Shared Libraries Use ldd to copy the required libs for Nginx when running in scratch. Minimal Nginx Config Disable http_rewrite, http_proxy, mail_*\u0026hellip; because it only serves static. Non-root User Create a UID 101 user to run inside the image for better security. Dockerfile TOP 2 (React)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 # STAGE 1: The Builder (Custom Nginx Build) FROM alpine:3.19 AS builder # Multi-arch support ARG TARGETARCH RUN apk add --no-cache build-base pcre2-dev zlib-dev openssl-dev # Download and verify Nginx source with SHA256 checksum ARG NGINX_VERSION=1.27.0 ARG NGINX_SHA256=b7230e3cf87eaa2d4b0bc56aadc920a960c7873b9991a1b66ffcc08fc650129c ADD --checksum=sha256:${NGINX_SHA256} https://nginx.org/download/nginx-${NGINX_VERSION}.tar.gz /tmp/ RUN tar -xzf /tmp/nginx-${NGINX_VERSION}.tar.gz -C /tmp # Configure Nginx with minimal modules (static file serving only) RUN cd /tmp/nginx-${NGINX_VERSION} \u0026amp;\u0026amp; \\ ./configure \\ --prefix=/etc/nginx \\ --sbin-path=/usr/sbin/nginx \\ --conf-path=/etc/nginx/nginx.conf \\ --pid-path=/var/run/nginx.pid \\ --error-log-path=/dev/stderr \\ --http-log-path=/dev/stdout \\ --user=nginx \\ --group=nginx \\ --without-http_rewrite_module \\ --without-http_gzip_module \\ --without-http_proxy_module \\ --without-http_fastcgi_module \\ --without-http_uwsgi_module \\ --without-http_scgi_module \\ --without-mail_pop3_module \\ --without-mail_imap_module \\ --without-mail_smtp_module \u0026amp;\u0026amp; \\ make \u0026amp;\u0026amp; \\ make install \u0026amp;\u0026amp; rm -rf /tmp/* \u0026amp;\u0026amp; strip /usr/sbin/nginx # Create file Nginx main config (include snippet file) RUN { \\ mkdir -p /etc/nginx/conf.d \u0026amp;\u0026amp; \\ cat \u0026gt; /etc/nginx/nginx.conf; \\ } \u0026lt;\u0026lt;EOF events { worker_connections 1024; } http { include /etc/nginx/mime.types; client_body_temp_path /var/cache/nginx/client_body_temp; include /etc/nginx/conf.d/default.conf; } EOF RUN mkdir -p /etc/nginx/conf.d \u0026amp;\u0026amp; \\ cat \u0026gt; /etc/nginx/mime.types \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; types { text/html html; text/css css; application/javascript js; image/png png; application/json json; } EOF # Create minimal user/group files (no full /etc/passwd needed in scratch image) RUN echo \u0026#34;nginx:x:101:101:nginx:/var/cache/nginx:/sbin/nologin\u0026#34; \u0026gt; /etc/passwd \u0026amp;\u0026amp; \\ echo \u0026#34;nginx:x:101:\u0026#34; \u0026gt; /etc/group # Build static healthcheck binary (no external dependencies like wget/curl needed) RUN { \\ cat \u0026gt; /tmp/healthcheck.c \u0026amp;\u0026amp; \\ gcc -static -O2 -o /healthcheck /tmp/healthcheck.c; \\ } \u0026lt;\u0026lt;EOF #include \u0026lt;netdb.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;sys/socket.h\u0026gt; #include \u0026lt;netinet/in.h\u0026gt; int main() { struct hostent *h = gethostbyname(\u0026#34;localhost\u0026#34;); if (!h) return 1; int sock = socket(AF_INET, SOCK_STREAM, 0); if (sock \u0026lt; 0) return 1; struct sockaddr_in addr; addr.sin_family = AF_INET; addr.sin_port = htons(3000); addr.sin_addr = *(struct in_addr *)h-\u0026gt;h_addr_list[0]; int result = connect(sock, (struct sockaddr *)\u0026amp;addr, sizeof(addr)); close(sock); return (result == 0) ? 0 : 1; } EOF # Collect shared libraries required by Nginx (supports multi-arch) RUN mkdir -p /staging/lib /staging/usr/lib \u0026amp;\u0026amp; \\ ldd /usr/sbin/nginx | tr -s \u0026#39;[:space:]\u0026#39; \u0026#39;\\n\u0026#39; | grep \u0026#39;^/\u0026#39; | \\ xargs -I \u0026#39;{}\u0026#39; sh -c \u0026#39;mkdir -p /staging$(dirname {}) \u0026amp;\u0026amp; cp -L {} /staging$(dirname {})\u0026#39; # Create mount point directories for tmpfs (writable dirs in read-only container) RUN mkdir -p /var/cache/nginx /var/run /tmp \u0026amp;\u0026amp; chown -R 101:101 /var/cache/nginx /var/run /tmp # STAGE 2: App Builder (Build React App) FROM node:20.11.0-alpine3.19 AS app_builder WORKDIR /app RUN corepack enable \u0026amp;\u0026amp; corepack prepare pnpm@9.12.2 --activate # Copy dependencies files first for caching COPY package.json pnpm-lock.yaml ./ RUN --mount=type=cache,target=/root/.local/share/pnpm/store,sharing=locked \\ pnpm install --frozen-lockfile --prefer-offline # Copy config files COPY tsconfig.json tsconfig.node.json vite.config.ts ./ COPY postcss.config.js tailwind.config.ts index.html ./ # Copy source code (avoid copying tests, stories, etc.) COPY public/ ./public/ COPY src/ ./src/ RUN pnpm build \u0026amp;\u0026amp; \\ test -f dist/index.html || (echo \u0026#34;Build failed\u0026#34; \u0026amp;\u0026amp; exit 1) # STAGE 3: Production Image (FROM scratch) FROM scratch LABEL org.opencontainers.image.title=\u0026#34;Vite React Template\u0026#34; \\ org.opencontainers.image.description=\u0026#34;Minimal Vite React SPA with custom Nginx built from scratch\u0026#34; \\ org.opencontainers.image.version=\u0026#34;1.0.0\u0026#34; \\ org.opencontainers.image.licenses=\u0026#34;MIT\u0026#34; \\ org.opencontainers.image.base.name=\u0026#34;scratch\u0026#34; \\ org.opencontainers.image.authors=\u0026#34;Contest 2025\u0026#34; # Copy file user/group COPY --from=builder /etc/passwd /etc/group /etc/ # Copy mount points to image COPY --chown=101:101 --from=builder /var/cache/nginx /var/cache/nginx COPY --chown=101:101 --from=builder /var/run /var/run COPY --chown=101:101 --from=builder /tmp /tmp # Copy Nginx and shared libraries COPY --from=builder /usr/sbin/nginx /usr/sbin/nginx COPY --from=builder /staging/ / # Copy main Nginx config (just created) COPY --from=builder /etc/nginx/nginx.conf /etc/nginx/nginx.conf # Copy file config from project into included location COPY nginx.conf /etc/nginx/conf.d/default.conf # Copy remaining necessary files COPY --from=builder /etc/nginx/mime.types /etc/nginx/mime.types COPY --from=app_builder /app/dist /usr/share/nginx/html COPY --from=builder /healthcheck /healthcheck USER 101:101 EXPOSE 3000 HEALTHCHECK --interval=30s --timeout=3s --retries=3 \\ CMD [\u0026#34;/healthcheck\u0026#34;] CMD [\u0026#34;/usr/sbin/nginx\u0026#34;, \u0026#34;-g\u0026#34;, \u0026#34;daemon off;\u0026#34;] 4. Dockerfile TOP 3 (React) – Go + FastHTTP Technique Explanation Reference Go Server (FastHTTP) Use github.com/valyala/fasthttp instead of Nginx. Asset Embedding //go:embed dist to embed all assets into the binary. Maximum Binary Compression Build statically, strip, then upx --ultra-brute --lzma. Integrated Healthcheck The Go binary handles it when run with -health. Dockerfile TOP 3 (React)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 # Stage 1: Build frontend FROM node:20-alpine AS builder RUN corepack enable pnpm WORKDIR /app COPY package.json pnpm-lock.yaml ./ RUN pnpm install --frozen-lockfile COPY tsconfig*.json vite.config.ts postcss.config.js tailwind.config.ts ./ COPY index.html ./ COPY src ./src COPY public ./public RUN pnpm build # Clean up dist (remove source maps, licenses, stats, robots.txt, etc.) RUN find /app/dist -type f -name \u0026#39;*.map\u0026#39; -delete \\ \u0026amp;\u0026amp; find /app/dist -type f -name \u0026#39;*.LICENSE.*\u0026#39; -delete \\ \u0026amp;\u0026amp; find /app/dist -type f -name \u0026#39;*.txt\u0026#39; -delete \\ \u0026amp;\u0026amp; find /app/dist -type f -name \u0026#39;stats.html\u0026#39; -delete \\ \u0026amp;\u0026amp; find /app/dist -type f -name \u0026#39;robots.txt\u0026#39; -delete \\ \u0026amp;\u0026amp; find /app/dist -type f -name \u0026#39;_redirects\u0026#39; -delete # Stage 2: Build Go server with FastHTTP FROM golang:1.21-alpine AS go-builder WORKDIR /app # Cài strip và upx RUN apk add --no-cache binutils upx # Copy dist files để embed COPY --from=builder /app/dist ./dist # Tạo go.mod và main.go với FastHTTP RUN cat \u0026gt; go.mod \u0026lt;\u0026lt; \u0026#39;GOMOD\u0026#39; module server go 1.21 require github.com/valyala/fasthttp v1.51.0 GOMOD RUN cat \u0026gt; main.go \u0026lt;\u0026lt; \u0026#39;GOSRC\u0026#39; package main import ( \u0026#34;embed\u0026#34; \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; \u0026#34;path\u0026#34; \u0026#34;strings\u0026#34; \u0026#34;github.com/valyala/fasthttp\u0026#34; ) //go:embed dist var distFiles embed.FS func main() { port := os.Getenv(\u0026#34;PORT\u0026#34;) if port == \u0026#34;\u0026#34; { port = \u0026#34;3000\u0026#34; } // FastHTTP handler handler := func(ctx *fasthttp.RequestCtx) { pathStr := string(ctx.Path()) // Check if it\u0026#39;s a static asset (has file extension) if strings.Contains(path.Base(pathStr), \u0026#34;.\u0026#34;) { // Try to serve static file file, err := distFiles.Open(\u0026#34;dist\u0026#34; + pathStr) if err == nil { defer file.Close() // Set content type based on extension ext := path.Ext(pathStr) switch ext { case \u0026#34;.js\u0026#34;: ctx.SetContentType(\u0026#34;application/javascript\u0026#34;) case \u0026#34;.css\u0026#34;: ctx.SetContentType(\u0026#34;text/css\u0026#34;) case \u0026#34;.svg\u0026#34;: ctx.SetContentType(\u0026#34;image/svg+xml\u0026#34;) case \u0026#34;.png\u0026#34;: ctx.SetContentType(\u0026#34;image/png\u0026#34;) case \u0026#34;.jpg\u0026#34;, \u0026#34;.jpeg\u0026#34;: ctx.SetContentType(\u0026#34;image/jpeg\u0026#34;) case \u0026#34;.ico\u0026#34;: ctx.SetContentType(\u0026#34;image/x-icon\u0026#34;) default: ctx.SetContentType(\u0026#34;application/octet-stream\u0026#34;) } // Copy file content to response ctx.Response.SetBodyStream(file, -1) return } } // SPA fallback - serve index.html for all routes file, err := distFiles.Open(\u0026#34;dist/index.html\u0026#34;) if err != nil { ctx.SetStatusCode(404) ctx.SetBodyString(\u0026#34;Not Found\u0026#34;) return } defer file.Close() ctx.SetContentType(\u0026#34;text/html; charset=utf-8\u0026#34;) ctx.Response.SetBodyStream(file, -1) } // healthcheck if len(os.Args) \u0026gt; 1 \u0026amp;\u0026amp; os.Args[1] == \u0026#34;-health\u0026#34; { _, _, err := fasthttp.Get(nil, \u0026#34;http://127.0.0.1:\u0026#34;+port) if err != nil { os.Exit(1) } os.Exit(0) } log.Printf(\u0026#34;FastHTTP server on :%s\u0026#34;, port) log.Fatal(fasthttp.ListenAndServe(\u0026#34;:\u0026#34;+port, handler)) } GOSRC # Download dependencies và build với tối ưu extreme RUN go mod tidy RUN CGO_ENABLED=0 GOOS=linux go build -ldflags=\u0026#34;-s -w\u0026#34; -trimpath -o server main.go RUN strip server RUN upx --ultra-brute --lzma server # Stage 3: Ultra-minimal runtime (scratch) FROM scratch # Copy binary (chứa cả static files) COPY --from=go-builder /app/server /server EXPOSE 3000 # Thêm USER 1000 USER 1000 HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \\ CMD [\u0026#34;/server\u0026#34;, \u0026#34;-health\u0026#34;] CMD [\u0026#34;/server\u0026#34;] 5. Dockerfile TOP (React) – Self-compiled Nginx on Alpine Technique Explanation Reference Compiled Nginx Build Nginx 1.27.3 from source, enable modules needed for SPA (gzip_static, ssl). Pre-compression gzip -k -9 for assets, serve via gzip_static on. Inline Security Headers Add X-Frame-Options, X-Content-Type-Options, X-XSS-Protection. Healthcheck Use wget to check HTTP 200 on port 3000. Dockerfile TOP (React)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 # ========================= # Giai đoạn 1: Node base # ========================= # Môi trường build cho Vite/React: chỉ cài những thứ tối thiểu để biên dịch FROM alpine:3.20@sha256:765942a4039992336de8dd5db680586e1a206607dd06170ff0a37267a9e01958 AS node-base RUN apk add --no-cache nodejs npm git python3 g++ make \u0026amp;\u0026amp; \\ npm install -g pnpm@9.12.2 \u0026amp;\u0026amp; npm cache clean --force # ========================= # Giai đoạn 2: Builder # ========================= # Dùng cache mount cho pnpm để tăng tốc build lại; xóa source map; nén gzip sẵn FROM node-base AS builder WORKDIR /app COPY package.json pnpm-lock.yaml ./ RUN --mount=type=cache,id=pnpm-custom,target=/root/.local/share/pnpm/store \\ pnpm install --frozen-lockfile --prefer-offline # Sao chép cấu hình \u0026amp; mã nguồn COPY index.html ./ COPY vite.config.ts tsconfig.json tsconfig.node.json ./ COPY postcss.config.js tailwind.config.ts ./ COPY public ./public COPY src ./src # Build \u0026amp; tối ưu artefact tĩnh RUN pnpm run build \u0026amp;\u0026amp; \\ find /app/dist -name \u0026#34;*.map\u0026#34; -type f -delete || true \u0026amp;\u0026amp; \\ find /app/dist -type f \\( -name \u0026#39;*.html\u0026#39; -o -name \u0026#39;*.js\u0026#39; -o -name \u0026#39;*.css\u0026#39; -o -name \u0026#39;*.svg\u0026#39; -o -name \u0026#39;*.json\u0026#39; \\) \\ -exec gzip -k -9 {} \\; # ====================================== # Giai đoạn 3: Nginx builder (tự biên dịch) # ====================================== # Biên dịch nginx 1.27.3 với module tối thiểu cho SPA + nén tĩnh FROM alpine:3.20@sha256:765942a4039992336de8dd5db680586e1a206607dd06170ff0a37267a9e01958 AS nginx-builder RUN apk add --no-cache --virtual .build-deps gcc libc-dev make pcre2-dev zlib-dev openssl-dev linux-headers \u0026amp;\u0026amp; \\ wget -O /tmp/nginx.tar.gz https://nginx.org/download/nginx-1.27.3.tar.gz \u0026amp;\u0026amp; \\ tar -xzf /tmp/nginx.tar.gz -C /tmp \u0026amp;\u0026amp; cd /tmp/nginx-1.27.3 \u0026amp;\u0026amp; \\ ./configure \\ --prefix=/etc/nginx \\ --sbin-path=/usr/sbin/nginx \\ --modules-path=/usr/lib/nginx/modules \\ --conf-path=/etc/nginx/nginx.conf \\ --error-log-path=/var/log/nginx/error.log \\ --http-log-path=/var/log/nginx/access.log \\ --pid-path=/var/run/nginx.pid \\ --lock-path=/var/run/nginx.lock \\ --http-client-body-temp-path=/var/cache/nginx/client_temp \\ --http-proxy-temp-path=/var/cache/nginx/proxy_temp \\ --user=nginx --group=nginx \\ --with-http_ssl_module --with-http_v2_module \\ --with-http_gzip_static_module --with-http_stub_status_module \\ --with-threads --with-file-aio \\ --without-http_autoindex_module --without-http_browser_module \\ --without-http_geo_module --without-http_map_module \\ --without-http_memcached_module --without-http_userid_module \\ --without-mail_pop3_module --without-mail_imap_module \\ --without-mail_smtp_module --without-http_split_clients_module \\ --without-http_uwsgi_module --without-http_scgi_module \\ --without-http_grpc_module \u0026amp;\u0026amp; \\ make -j$(nproc) \u0026amp;\u0026amp; make install \u0026amp;\u0026amp; strip /usr/sbin/nginx \u0026amp;\u0026amp; \\ rm -rf /tmp/nginx* \u0026amp;\u0026amp; apk del .build-deps # ====================================== # Giai đoạn 4: Runtime base tối giản # ====================================== # Chỉ giữ runtime deps; tạo user non-root; chuẩn bị thư mục và quyền FROM alpine:3.20@sha256:765942a4039992336de8dd5db680586e1a206607dd06170ff0a37267a9e01958 AS custom-runtime-base RUN apk add --no-cache pcre2 zlib openssl tzdata \u0026amp;\u0026amp; \\ addgroup -g 101 -S nginx \u0026amp;\u0026amp; \\ adduser -S -D -H -u 101 -h /var/cache/nginx -s /sbin/nologin -G nginx -g nginx nginx \u0026amp;\u0026amp; \\ mkdir -p /var/cache/nginx /var/log/nginx /etc/nginx/conf.d /usr/share/nginx/html \u0026amp;\u0026amp; \\ chown -R nginx:nginx /var/cache/nginx /var/log/nginx /usr/share/nginx/html COPY --from=nginx-builder /usr/sbin/nginx /usr/sbin/nginx COPY --from=nginx-builder /etc/nginx /etc/nginx # ====================================== # Giai đoạn 5: Runtime cuối # ====================================== FROM custom-runtime-base AS runtime LABEL org.opencontainers.image.title=\u0026#34;SvnFrs-Dockerfile_Contest_2025\u0026#34; \\ org.opencontainers.image.description=\u0026#34;SPA production trên Alpine với Nginx tự biên dịch, tối ưu và bảo mật\u0026#34; \\ org.opencontainers.image.version=\u0026#34;1.0.0\u0026#34; \\ org.opencontainers.image.licenses=\u0026#34;MIT\u0026#34; \\ org.opencontainers.image.created=\u0026#34;2025-10-28\u0026#34; \\ org.opencontainers.image.base.name=\u0026#34;alpine:3.20\u0026#34; # Ứng dụng tĩnh đã build COPY --from=builder --chown=nginx:nginx /app/dist /usr/share/nginx/html # Cấu hình Nginx tối thiểu (inline) bật gzip_static để phục vụ file .gz RUN echo \u0026#39;server { \\ listen 3000; server_name localhost; \\ root /usr/share/nginx/html; index index.html; \\ gzip_static on; gzip_vary on; \\ location / { try_files $uri $uri/ /index.html; expires 1y; add_header Cache-Control \u0026#34;public, immutable\u0026#34;; } \\ location = /index.html { expires -1; add_header Cache-Control \u0026#34;no-cache\u0026#34;; } \\ add_header X-Frame-Options \u0026#34;SAMEORIGIN\u0026#34; always; \\ add_header X-Content-Type-Options \u0026#34;nosniff\u0026#34; always; \\ add_header X-XSS-Protection \u0026#34;1; mode=block\u0026#34; always; \\ }\u0026#39; \u0026gt; /etc/nginx/conf.d/default.conf \u0026amp;\u0026amp; \\ echo \u0026#39;worker_processes auto; error_log /var/log/nginx/error.log warn; pid /var/run/nginx.pid; \\ events { worker_connections 1024; } http { include /etc/nginx/mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; include /etc/nginx/conf.d/*.conf; }\u0026#39; \u0026gt; /etc/nginx/nginx.conf \u0026amp;\u0026amp; \\ echo \u0026#39;types { \\ text/html html htm shtml; text/css css; text/javascript js; application/json json; \\ image/svg+xml svg svgz; image/x-icon ico; image/png png; image/jpeg jpeg jpg; \\ font/woff2 woff2; application/wasm wasm; \\ }\u0026#39; \u0026gt; /etc/nginx/mime.types # Thư mục tạm/ngầm của Nginx + phân quyền trước khi chuyển USER RUN mkdir -p /var/cache/nginx/client_temp \\ /var/cache/nginx/proxy_temp \\ /etc/nginx/fastcgi_temp \\ /etc/nginx/proxy_temp \\ /etc/nginx/client_body_temp \\ /etc/nginx/uwsgi_temp \\ /etc/nginx/scgi_temp \u0026amp;\u0026amp; \\ chown -R nginx:nginx /var/cache/nginx \\ /var/log/nginx \\ /usr/share/nginx/html \\ /etc/nginx/fastcgi_temp \\ /etc/nginx/proxy_temp \\ /etc/nginx/client_body_temp \\ /etc/nginx/uwsgi_temp \\ /etc/nginx/scgi_temp \u0026amp;\u0026amp; \\ chmod -R 755 /var/cache/nginx \\ /etc/nginx/fastcgi_temp \\ /etc/nginx/proxy_temp \\ /etc/nginx/client_body_temp \\ /etc/nginx/uwsgi_temp \\ /etc/nginx/scgi_temp \u0026amp;\u0026amp; \\ touch /var/run/nginx.pid \u0026amp;\u0026amp; \\ chown nginx:nginx /var/run/nginx.pid # Healthcheck rẻ: HTTP 200 ở cổng 3000 RUN apk add --no-cache wget HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \\ CMD wget --quiet --tries=1 --spider http://localhost:3000/ || exit 1 EXPOSE 3000 STOPSIGNAL SIGQUIT USER nginx CMD [\u0026#34;nginx\u0026#34;, \u0026#34;-g\u0026#34;, \u0026#34;daemon off;\u0026#34;] Deployment Notes The Dockerfiles above are architectural references only; when applying them to your Node.js/React project, keep the core principles: multi-stage build, pre-compress, non-root, pin SHA. ","date":"2025-11-18T00:00:00Z","image":"https://tech.nguuyen.io.vn/images/docker-optimization/docker-optimization-react.webp","permalink":"https://tech.nguuyen.io.vn/en/posts/docker-optimization/docker-opt-nodejs/","title":"Optimize Docker for Node.js"},{"content":"📌 Introduction Nginx is an open-source web server that can serve static and dynamic web applications. It can run as a web server, load balancer, reverse proxy, or HTTP cache, and integrates with existing applications to build complete systems or distribute web apps via IP or domain.\nThis guide shows how to install Nginx on Ubuntu 24.04 and set up a sample web app on your server.\n✅ Prerequisites Before you begin, make sure you:\n🚀 Provision an Ubuntu 24.04 server. 🌍 Create an A record pointing your domain or subdomain to the server IP (e.g., app.example.com). 🔐 Access the server via SSH and create a non-root sudo user. 🔄 Update the system packages. ⚙️ Install NGINX on Ubuntu 24.04 The latest NGINX package is available in Ubuntu 24.04\u0026rsquo;s default APT repository. Follow these steps to update the system and install NGINX.\n🔄 Update package lists\n1 sudo apt update 📦 Install NGINX\n1 sudo apt install nginx -y 🔍 Check installed NGINX version\n1 sudo nginx -version Sample output:\n1 nginx version: nginx/1.24.0 (Ubuntu) ⚙️ Manage the NGINX service NGINX uses the nginx systemd service to manage runtime and processes. Use the commands below to enable and manage the service.\n🚀 Enable NGINX to start on boot\n1 sudo systemctl enable nginx Result:\n1 2 Synchronizing state of nginx.service with SysV service script with /usr/lib/systemd/systemd-sysv-install. Executing: /usr/lib/systemd/systemd-sysv-install enable nginx ▶️ Start NGINX\n1 sudo systemctl start nginx ⏹️ Stop NGINX\n1 sudo systemctl stop nginx 🔄 Restart NGINX\n1 sudo systemctl restart nginx 🔍 Check NGINX status\n1 sudo systemctl status nginx Sample output:\n1 2 3 4 5 6 7 8 9 10 11 12 13 ● nginx.service - A high performance web server and a reverse proxy server Loaded: loaded (/usr/lib/systemd/system/nginx.service; enabled; preset: enabled) Active: active (running) since Wed 2024-06-26 10:55:50 UTC; 1min 0s ago Docs: man:nginx(8) Process: 2397 ExecStartPre=/usr/sbin/nginx -t -q -g daemon on; master_process on; (code=exited, status=0/SUCCESS) Process: 2399 ExecStart=/usr/sbin/nginx -g daemon on; master_process on; (code=exited, status=0/SUCCESS) Main PID: 2400 (nginx) Tasks: 2 (limit: 1068) Memory: 1.7M (peak: 2.4M) CPU: 13ms CGroup: /system.slice/nginx.service ├─2400 \u0026#34;nginx: master process /usr/sbin/nginx -g daemon on; master_process on;\u0026#34; └─2401 \u0026#34;nginx: worker process\u0026#34; If Active: active (running) appears, NGINX is running. If you see Active: active (failed), stop any process using HTTP port 80, then restart NGINX. 🌐 Create an Nginx virtual host An Nginx virtual host serves web app files from a specific directory using a domain name. Follow these steps to set up a sample virtual host securely.\n📂 Create a new virtual host config file\nIn /etc/nginx/sites-available, create a new config file, for example: app.example.com.conf. 1 $ sudo nano /etc/nginx/sites-available/app.example.com.conf Add the following configuration: 1 2 3 4 5 6 7 8 9 10 11 12 13 server { listen 80; listen [::]:80; server_name app.example.com; root /var/www/app.example.com; index index.html; location / { try_files $uri $uri/ =404; } } Save and close the file. ✅ Test Nginx configuration\nCheck the configuration for errors: 1 $ sudo nginx -t Result: 1 2 nginx: the configuration file /etc/nginx/nginx.conf syntax is ok nginx: configuration file /etc/nginx/nginx.conf test is successful 🔗 Enable the virtual host\nLink the config file into /etc/nginx/sites-enabled: 1 $ sudo ln -s /etc/nginx/sites-available/app.example.com.conf /etc/nginx/sites-enabled/ 📁 Create the web root directory\n1 $ sudo mkdir -p /var/www/app.example.com 📝 Create a sample HTML file\nCreate index.html in the web root: 1 $ sudo nano /var/www/app.example.com/index.html Add the following content: 1 2 3 4 5 6 \u0026lt;html\u0026gt; \u0026lt;head\u0026gt;\u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Hello Hoang Duong\u0026lt;/h1\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Save and close the file. 🔄 Restart Nginx\n1 $ sudo systemctl restart nginx 🌍 Verify the virtual host\nUse curl to test: 1 $ curl http://app.example.com Result: 1 Hello Hoang Duong 🔒 Secure the Nginx web server SSL certificates encrypt communication between the browser and your server over HTTPS. By default, Nginx listens on insecure HTTP port 80. Follow these steps to obtain a trusted SSL certificate from Let\u0026rsquo;s Encrypt and enable HTTPS.\nInstall Certbot – Let\u0026rsquo;s Encrypt client 🔧\nInstall Certbot using Snap: 1 $ sudo snap install --classic certbot Verify the Certbot version: 1 $ sudo certbot --version 📌 Sample output:\n1 certbot 2.11.0 🛡️ Generate an SSL certificate for Nginx Create a new SSL certificate for your domain. Replace app.example.com with your real domain: 1 $ sudo certbot --nginx -d app.example.com --agree-tos This will:\n✅ Generate a valid SSL certificate ✅ Automatically configure Nginx to use SSL ✅ Enable HTTPS on your web server Test automatic renewal 🔄\nLet\u0026rsquo;s Encrypt certificates last 90 days. Check auto-renewal with: 1 $ sudo certbot renew --dry-run If no errors appear, the certificate will renew automatically.\n🌐 Verify the site with HTTPS Open the browser and visit: 1 https://app.example.com If you see the lock icon 🔒, SSL is installed successfully! 🚀\n🔥 Configure UFW firewall rules Uncomplicated Firewall (UFW) is installed and enabled by default on Ubuntu 24.04. Follow these steps to allow HTTP and HTTPS traffic.\nAllow HTTP (Port 80) 🔌\nRun: 1 $ sudo ufw allow 80/tcp Allow HTTPS (Port 443) 🔐\nRun: 1 $ sudo ufw allow 443/tcp Check firewall status 🔢\nRun: 1 $ sudo ufw status 👉 Sample output:\n1 2 3 4 5 6 7 8 9 10 Status: active To Action From -- ------ ---- 22/tcp ALLOW Anywhere 80/tcp ALLOW Anywhere 443/tcp ALLOW Anywhere 22/tcp (v6) ALLOW Anywhere (v6) 80/tcp (v6) ALLOW Anywhere (v6) 443/tcp (v6) ALLOW Anywhere (v6) 🛡️ After setup, the server only allows HTTP and HTTPS connections.\n🎯 Conclusion Congratulations on installing Nginx on Ubuntu 24.04 and configuring a web server for your applications. Nginx supports multiple virtual hosts to deploy applications securely. You can also integrate Nginx with MySQL and PHP to build dynamic web apps. For more configuration options, visit the official Nginx documentation. 🚀😊\n","date":"2025-03-10T00:00:00Z","image":"https://tech.nguuyen.io.vn/images/website/how-to-install-nginx.webp","permalink":"https://tech.nguuyen.io.vn/en/posts/installation-guides/how-to-install-nginx/","title":"How to Install Nginx on Ubuntu 24.04 ⚙️"},{"content":"🔀 What is Nginx? NGINX is a reliable open-source web server\nNginx is an open-source web server that uses an asynchronous, event-driven architecture. It was originally built for HTTP caching, later expanded to support reverse proxy, HTTP load balancing, and mail protocols such as IMAP4, POP3, and SMTP.\nReleased in October 2014, Nginx is used by large companies like Google, Adobe, Netflix, and WordPress because it can handle thousands of concurrent connections.\n⚙️ How Nginx works NGINX works similarly to other servers\nNginx follows an asynchronous processing model, unlike the sequential processing used by traditional web servers.\nEach process has multiple worker connections to handle requests. Worker connections pass requests to worker processes, which forward them to the master process. With this model, one worker connection can handle up to 1024 requests at once, enabling Nginx to serve thousands of requests efficiently. 🔥 Nginx features NGINX offers powerful features for web development\nNginx provides many standout features:\n⚡ Handles over 10,000 concurrent connections with low memory usage. 📂 Serves static files and file indexing. 🔄 Load balancing and reverse proxy with caching. 🚀 Supports FastCGI, uWSGI, SCGI, and Memcached. 🛠️ Modular architecture with automatic gzip compression. 🔐 SSL/TLS encryption support. 🔀 URL rewrites using regular expressions. 🌐 WebSocket support and connection limits. 📡 IPv6 compatibility. ⚖️ Nginx vs Apache Compared to Apache, NGINX has many advantages\n🖥️ Apache server: Processes requests using a forked threaded model or keep-alive. Handles both static and dynamic content. 🌍 Nginx server: Uses a non-blocking event loop. Serves static content more efficiently than Apache. Faster request handling and better resource usage. Requires a separate processor for dynamic content. 🔎 How to check if a website uses Nginx You can use available tools to check if a website runs Nginx\nYou can check if a website runs Nginx by inspecting HTTP headers:\nOpen the website in Chrome. Press Ctrl + Shift + I or F12 to open DevTools. Switch to the Network tab. Select any request and inspect Headers. You can also use tools like Pingdom or GTmetrix to check.\n🎯 Conclusion Nginx has become one of the most popular web servers thanks to high performance, the ability to handle thousands of concurrent connections, and powerful features. Whether you need to serve static content, balance load, or run a reverse proxy, Nginx is a strong choice. Hopefully this guide helps you understand Nginx and how it works. If you are considering an optimized web server for your project, try Nginx and explore its benefits! 🚀😊\n","date":"2025-03-10T00:00:00Z","image":"https://tech.nguuyen.io.vn/images/website/nginx-introduction-guide.webp","permalink":"https://tech.nguuyen.io.vn/en/posts/website/nginx-introduction-guide/","title":"Introduction to Nginx 🚀"},{"content":"🌐 Why point a domain to hosting? Domain and hosting are the two essentials that keep a website online.\nDomain is the website address. Hosting stores the website data. Why you need to point a domain Connect the domain to hosting so the site appears on the internet. Let people access the site using a memorable address. How it works 🔧 Get nameserver or IP information from your hosting provider. Configure it in your domain management dashboard. Three simplest ways to point a domain to hosting 🚀 Point using nameservers Steps:\nLog in to your domain management panel. Select the domain you want to point. Select the domain to point to hosting\nReplace the default nameservers with the hosting provider\u0026rsquo;s nameservers (found in the hosting welcome email). Replace default nameservers with your hosting nameservers\nWait for DNS propagation (minutes to hours, up to 24h). Verify by visiting the domain. Point using IP (A record) Steps:\nLog in to your domain management panel. Select the domain and open DNS settings\nOpen the DNS settings. Add two A records: @ → Hosting IP (from the provider email). www → Hosting IP. Add two A records\nSave changes and wait for DNS propagation (minutes to hours). Point using an intermediate nameserver To point a domain via an intermediate nameserver such as Cloudflare, Namecheap FreeDNS, or Incapsula, follow these steps:\nSteps:\nCreate an account with the intermediate nameserver and confirm required DNS records. In your domain registrar, change the nameservers to the intermediate provider. Wait for propagation. Common issues when pointing a domain to hosting ⚠️ You may run into issues that prevent your site from working correctly. Understanding these will help you troubleshoot faster.\n🚫 Wrong hosting domain When registering hosting, the provider asks for your domain. If it is incorrect or mismatched, the website may not work.\n🚫 Using both methods at once If you already pointed a domain via nameservers, do not switch to IP (A record) at the same time. It can cause DNS conflicts.\n🚫 Incorrect DNS record type Common DNS record types include:\nA record: Points the domain to the hosting IP. CNAME: Points one domain to another. MX record: Configures email. Entering the wrong record type can break connectivity.\n🚫 Wrong hosting IP Hosting IPs are long and easy to mistype. Copy directly from the provider email to avoid confusion with a personal IP.\nIf errors occur, double-check your settings and wait for DNS updates to finish.\nVideo guide 🎥 For a visual walkthrough, watch the video below. It covers steps from getting nameserver details to updating settings in your domain manager.\n― Source, F8 Official ✔️\nConclusion 🌟 Pointing a domain to hosting is essential for your website to work online. Whether you use nameservers, A records, or an intermediate nameserver, double-check your settings and allow time for DNS propagation. If you encounter issues, review the configuration and try again 🔄. Good luck, and feel free to leave a comment if you need help! 💬😊\n","date":"2025-02-27T00:00:00Z","image":"https://tech.nguuyen.io.vn/images/website/domain-to-hosting-setup.webp","permalink":"https://tech.nguuyen.io.vn/en/posts/website/domain-to-hosting-setup/","title":"Point a Domain to Hosting 🌐"},{"content":"How to upload a website to hosting via File Manager and cPanel from A to Z Uploading a website to hosting is an essential step to bring your site online. It is not just about transferring files but also ensuring everything runs smoothly and securely. This guide walks you through the steps, from preparing files and databases to using the right upload tools.\nKey points to remember Before you start uploading, keep these essentials in mind:\n✅ What to prepare before uploading: Website files, database, control panel access, and an FTP client like FileZilla.\n✅ Ways to upload: Choose a reliable host, then use File Manager, FTP, WordPress plugins, SSH, or provider support.\n✅ Why upload to hosting: Makes your site public, improves security, boosts speed, and simplifies management.\n✅ Four steps to upload a website:\nUpload website files to public_html via File Manager or FTP. Verify files are in the correct location. Create a database in cPanel, import via phpMyAdmin, and configure the connection. Visit the website to confirm everything works. What do you need before uploading a website? Prepare the following:\nWebsite files: These can be the contents of your old public_html folder, source code, or the latest backup. 📌 Note for React/Vue.js deployments\nWhen building a React or Vue.js app, you do not upload the full source code, only the static build output.\n🔹 Build and zip steps:\nRun the build command: React: 1 npm run build Vue.js: 1 npm run build The output folder is usually build/ (React) or dist/ (Vue.js). Zip the build or dist folder before uploading. 📤 Uploading to the server:\nWith cPanel, extract directly in public_html. With SSH/SFTP, upload and unzip using: 1 unzip build.zip -d /var/www/html/ Ensure your Apache/Nginx config points to the folder containing index.html. Database file (if applicable). Control panel access for the new hosting account. FTP client such as FileZilla. To upload, access your hosting control panel via an FTP client like FileZilla. If your site already exists, use the CMS or cPanel backup feature to download it and upload to the new host.\nChoose a reliable hosting provider Quality hosting affects both speed and long-term performance. Choose carefully based on:\nLive support: 24/7 customer support for quick issue resolution. Account control: Full access to cPanel or equivalent tools. Scalability: Ability to upgrade storage, bandwidth, or domains. Clear refund policy: Flexible refunds for testing services. Free add-ons: SSL, scheduled backups, or free migrations. ICANN-accredited domains: Protect your brand with trusted domain registration. Note: Review provider reputation and user feedback on forums or review sites before deciding.\nChoose a website upload method There are five main ways to upload a website depending on your tools and needs:\nUse File Manager File Manager is a web-based file tool built into cPanel or other hosting dashboards.\nPros:\nFree. Easy to use. Cons:\nUpload size limits (usually 256 MB). Can only extract archives within that size limit. Note: For larger files, use FTP and unzip via SSH.\nFigure 1: Access File Manager\nUse FTP FTP (File Transfer Protocol) lets you upload files using an FTP client like FileZilla. This is efficient and has no upload size limit.\nPros:\nNo upload size limits. Faster for large files. Cons:\nRequires an FTP client. Requires FTP credentials from your host. Lower security, use SFTP for better protection. Figure 2: FTP details\nUse WordPress migration plugins If you use WordPress, plugins like All in One WP Migration can move your entire site automatically.\nPros:\nEasy to use with minimal technical knowledge. Simple drag and drop workflow. Cons:\nUpload size limits (usually 256 MB). For larger files, use FTP + SSH. Note: After upload, move files out of subfolders into public_html so the site loads correctly.\nFigure 3: Use WordPress Migration Plugin\nUse SSH (Secure Shell) SSH lets you upload and manage files via command line, ideal for large files or high speed.\nPros:\nFast with no size limits. Extract archives directly on the server. Cons:\nRequires basic command-line skills and SSH access. Figure 4: Upload via SSH\nUse Import Site (automatic importer) Some hosting providers offer an Import Site tool that uploads and extracts a website into public_html.\nPros:\nQuick and easy upload. Minimal technical effort. Cons:\nDepends on whether your host supports the tool. Tip: Ask your hosting provider if they support site import.\nFigure 5: Import Site tool\nAsk hosting support Most providers offer migration help, especially if you are switching hosts.\nPros:\nSaves time and avoids technical mistakes. Done by experts. Cons:\nMay be a paid service. Depends on support availability. Tip: Check the provider\u0026rsquo;s migration policy before requesting support.\nFigure 6: Ask hosting support\n📌 Notes after uploading\nMove all data from subfolders into public_html so the site loads correctly. Check file structure and database connections to avoid runtime errors. Choose the best method based on file size, platform, and your skills. Why you should upload a website to hosting Uploading to hosting is required to make your site public, but it also brings major benefits in security, performance, and reliability:\nPublish your website globally When hosted, your website becomes accessible worldwide via your domain name. Otherwise, only your local machine can access it.\nSecurity and data safety Professional hosting provides strong security measures such as SSL certificates, firewalls, and data protection tools. This keeps your site and brand data safe from attacks and breaches.\nSpeed and performance Hosting servers are optimized for high traffic and fast internet speeds, improving page load times and user experience while reducing lag.\nFigure 7: Speed and performance\nStability and reliability Hosting providers manage the hardware and network, ensuring high uptime so your website stays available without long downtime.\nEasy management Hosting dashboards make it easy to back up data, upgrade software, and optimize performance.\nScalability and long-term growth Hosting services scale with your traffic and resource needs, unlike personal machines that struggle with growth.\nFigure 8: Scalability and growth\nIn short, hosting your website improves security, performance, stability, and manageability. It is the foundation for sustainable growth and easy global access.\nStep 1: Choose how to upload website files Method 1: Upload via File Manager in cPanel Method 2: Upload via FTP client After you choose a method, follow the guide below.\n\u0026mdash;Method 1: Upload via File Manager in cPanel\u0026mdash; (Easiest)\nAccess cPanel and follow these steps:\nStep 1: Click File Manager under the Files section.\nChoose File Manager Illustration: Choose File Manager\nChoose File Manager\nStep 2: In File Manager, open the public_html folder.\nOpen public_html Illustration: Open public_html\nOpen public_html\nStep 3: Click Upload inside public_html.\nClick Upload Illustration: Click Upload\nClick Upload\nStep 4: Use Select File to pick files or drag and drop into the upload area.\nSelect file Illustration: Select file\nSelect file\nStep 5: In this example, drag and drop wordpress.zip.\nSelect wordpress zip Illustration: Select wordpress zip (or dist.zip/build.zip)\nSelect wordpress zip\nStep 6: After upload, return to File Manager. The archive appears in public_html. Right-click and choose Extract.\nExtract file Illustration: Extract file\nExtract file\nStep 7: Choose the extraction location. In this example, use /public_html.\nChoose extraction location Illustration: Choose extraction location\nChoose extraction location\nStep 8: After extraction, you will see the files in public_html. This is the website root.\nReturn to the original folder Illustration: Return to the original folder\nReturn to the original folder\nStep 9: The website is now uploaded. Enter your URL in a browser to visit.\nLanguage selection during WordPress setup (If applicable)\n\u0026mdash;Method 2: Upload via FTP client\u0026mdash;\nSome users prefer FTP clients like FileZilla, SmartFTP, CoreFTP, or similar tools. In this guide, we use FileZilla.\nNotes before uploading via FTP:\nYour skills: If you are new to hosting, File Manager is easier. If you are experienced, FTP is faster. Website size: Large websites are easier with FTP because there are no upload limits. Special requirements: If you need special server configuration, check with your hosting provider. Step 1: Get FTP information via FTP Access. If you forgot the password, reset it under Change account password.\nGet FTP Access details Illustration: Get FTP Access details\nGet FTP Access details\nStep 2: Open FileZilla, enter FTP details, and click Quickconnect.\nClick Quickconnect Illustration: Click Quickconnect\nClick Quickconnect\nStep 3: After connecting, locate your website files and drag them from the left pane to the right pane, targeting public_html. Unzip archives before upload because FTP cannot extract files.\nDrag and drop files Illustration: Drag and drop files\nDrag and drop files\nStep 4: You can also upload archives via FTP, then extract using File Manager.\nUpload archives via FTP Illustration: Upload archives via FTP\nUpload archives via FTP\nStep 5: After upload, enter the website URL in a browser to confirm installation or customize it.\nStep 2: Verify files are inside public_html After uploading, check that all files are in public_html. If you extracted a backup into a subfolder, users would need to visit example.com/something instead of example.com. Fix this by moving files:\nOpen the folder with the website files. Select all files, right-click, and choose Move. Select public_html and click Proceed. If your site has been running already, upload the database as well.\nAfter confirming file placement, open your domain in a browser. If DNS is not updated, you can:\nEdit the hosts file to simulate DNS changes. Use online tools to check DNS status. Install browser plugins for virtual hosts. If you need to move a site from a subfolder into public_html, use File Manager or FTP. Remember to upload the database if needed.\nStep 3: Upload the database to hosting Perform this step only if your website uses a database.\nCreate a database in cPanel Create a new database in MySQL Databases and record:\nMySQL Database MySQL User MySQL Host MySQL Password Create a new database\nOpen phpMyAdmin In phpMyAdmin, import your MySQL database. If importing into an existing database, clear it first to avoid errors.\nOpen phpMyAdmin\nImport the database Go to the Import tab and upload your SQL file (.sql, .sql.zip, or .sql.gz). Click Choose File, then Go to start. When phpMyAdmin shows Import has been successfully finished, 302 queries executed, the upload is done.\nImport database\nUpdate configuration for database connection After upload, open your PHP configuration file and fill in host, database name, username, and password. The file name and location depend on your software. For WordPress, the file is wp-config.php in public_html.\n⚠ Notes:\nLarge databases should be split into smaller files to speed up uploads. If your database contains special characters, convert them to ASCII before upload. If import errors occur, check for corruption. Create a fresh database and try again. After uploading, test your website to ensure everything works. Update configuration for database connection After uploading, open the PHP config file to enter the host, database name, username, and password.\nThe file name and location depend on your software. For example, WordPress uses wp-config.php in public_html.\nStep 4: Check if the website is stable To ensure stability after upload and domain mapping, perform the following checks:\n🔍 Check website access Access via domain or IP: If it loads, the site is live. Wait for DNS propagation: New DNS changes can take 24 hours. Check immediately using: 🖥️ Hosts file: Edit the hosts file to simulate DNS changes. 🌐 Online tools: Use online DNS checkers. 🔌 Browser plugins: Use plugins for virtual hosts. 🛠️ Test website functions Visit multiple pages and verify links. Ensure features work as expected. ⚡ Check load speed Use Google PageSpeed Insights or GTmetrix. 🚨 Check for errors 404: Page not found. 500: Hosting server error. 503: Server maintenance. 🌍 Test across browsers and devices Try Chrome, Firefox, Safari, Edge on desktop, mobile, tablet. ⏳ Test at different times Performance can vary throughout the day. 👉 If issues appear and you are unsure how to fix them, contact your hosting provider. 🚀\n📌 Conclusion Uploading a website to hosting is a key step for stable online operation. By preparing correctly and following the steps above, you ensure your website is ready to serve users effectively.\n💡 If you have questions or need help, leave a comment below. I will respond soon.\nThanks for reading! 🚀\n","date":"2025-02-26T00:00:00Z","image":"https://tech.nguuyen.io.vn/images/website/upload-website-on-hosting.webp","permalink":"https://tech.nguuyen.io.vn/en/posts/website/how-to-deploy-a-website/","title":"Deploy a Website to Hosting the Simplest Way (HTML/CSS or React/Vue.js) 🔥"},{"content":"📊 Application Monitoring Application monitoring is the continuous tracking and analysis of software applications to ensure they run optimally, detect incidents, and provide deep insights into system performance. Monitoring covers critical metrics such as:\n⏳ Response time ❌ Error rate 🖥️ Resource usage (CPU, RAM, Disk) 🔄 Transaction performance Application monitoring tools collect and analyze data to detect anomalies, alert on potential issues, and provide a complete view of application behavior. This allows teams to proactively resolve incidents, optimize performance, and improve user experience.\n🔍 Jaeger - Distributed tracing tool Jaeger is an open-source distributed tracing system developed by Uber, designed to track and troubleshoot complex microservices architectures.\n🔹 Key features: 🌐 Distributed request tracing 🔎 Service dependency analysis 📊 Root cause identification 🛠️ OpenTracing support for easy integration 🔹 Example: Running Jaeger with Docker 1 2 3 4 5 6 7 8 9 10 11 docker run -d --name jaeger \\ -e COLLECTOR_ZIPKIN_HTTP_PORT=9411 \\ -p 5775:5775/udp \\ -p 6831:6831/udp \\ -p 6832:6832/udp \\ -p 5778:5778 \\ -p 16686:16686 \\ -p 14268:14268 \\ -p 14250:14250 \\ -p 9411:9411 \\ jaegertracing/all-in-one:1.37 After installation, open Jaeger UI at http://localhost:16686.\n📌 References:\n📖 Jaeger documentation 🛠️ GitHub - jaegertracing 🌎 New Relic - Application performance monitoring New Relic is a cloud observability platform that provides a comprehensive view of software and infrastructure. It supports real-time performance monitoring, data analysis, and automated alerts.\n🔹 Key features: 📈 Application performance monitoring (APM) 🔎 Log analysis and error tracing 🚀 Web and mobile user experience monitoring 🤖 AI-powered analytics for incident detection 🔹 Example: Installing New Relic Agent in Node.js 1 npm install newrelic --save Then add require('newrelic') at the top of your main file:\n1 2 3 4 5 require(\u0026#39;newrelic\u0026#39;); const express = require(\u0026#39;express\u0026#39;); const app = express(); app.get(\u0026#39;/\u0026#39;, (req, res) =\u0026gt; res.send(\u0026#39;Hello, New Relic!\u0026#39;)); app.listen(3000, () =\u0026gt; console.log(\u0026#39;App running on port 3000\u0026#39;)); 📌 References:\n🌐 New Relic website 🎥 New Relic platform demo 🐶 Datadog - Full-stack monitoring solution Datadog is a monitoring and analytics platform for large-scale applications. It covers infrastructure monitoring, application performance, logs management, and user experience.\n🔹 Key features: 🔗 400+ integrations with DevOps tools 📊 Unified dashboards for full system visibility ⚠️ Intelligent alerting 📡 Cloud-native monitoring support 🔹 Example: Installing Datadog Agent 1 2 DD_AGENT_MAJOR_VERSION=7 DD_API_KEY=\u0026lt;YOUR_API_KEY\u0026gt; \\ DD_SITE=\u0026#34;datadoghq.com\u0026#34; bash -c \u0026#34;$(curl -L https://s3.amazonaws.com/dd-agent/scripts/install_script.sh)\u0026#34; After installation, access the Datadog dashboard to view metrics.\n📌 References:\n🌐 Datadog website 📖 Datadog documentation 🎯 Conclusion Application monitoring is crucial for ensuring system performance and reliability. Tools such as Jaeger, New Relic, and Datadog provide comprehensive solutions to track, analyze, and optimize software systems effectively. Choosing the right tool helps you manage applications better, detect issues early, and improve user experiences.\n🚀 Deploy monitoring to optimize your application performance!\n👉 Next step: Learn about Artifacts in software development. Artifacts are the files or products produced during the development and deployment process.\n","date":"2025-02-25T00:00:00Z","image":"https://tech.nguuyen.io.vn/images/devops/devops-step-eighteen.webp","permalink":"https://tech.nguuyen.io.vn/en/posts/devops/devops-step-eighteen/","title":"Application Monitoring 📊"},{"content":"🏗️ What are Artifacts? Artifacts are products generated throughout the software development lifecycle. They can include:\n📜 Source code: Files that contain application logic. 🏗️ Binaries: Compiled executables or libraries. 📖 Documentation: User guides, API specs. ⚙️ Configuration files: Settings needed to run the application. 🛠️ Test results: Reports from testing pipelines. Managing artifacts helps ensure consistency, traceability, and more efficient software delivery.\n🏢 Popular Artifact Management Tools 🚀 1. Artifactory Artifactory is a DevOps solution for storing, managing, and distributing artifacts. It supports many formats such as Docker, npm, Maven, Python, Go, and more.\n🔧 Install Artifactory with Docker: 1 2 3 4 5 # Pull Artifactory image docker pull releases-docker.jfrog.io/jfrog/artifactory-oss:latest # Run container docker run --name artifactory -d -p 8081:8081 releases-docker.jfrog.io/jfrog/artifactory-oss:latest After installation, you can access the web interface at http://localhost:8081.\n📦 2. Nexus Repository Manager Nexus is one of the most popular tools for binary artifacts, especially in Java environments.\n🏗️ Install Nexus on Linux: 1 2 3 4 wget https://download.sonatype.com/nexus/3/latest-unix.tar.gz tar -xvf latest-unix.tar.gz cd nexus-3.* ./bin/nexus start After it starts, you can access Nexus at http://localhost:8081.\n☁️ 3. Cloudsmith Cloudsmith is a cloud-based artifact management platform that supports many package formats such as Docker, Helm, npm, and pip.\n⚡ Upload packages to Cloudsmith: 1 2 3 4 5 # Install Cloudsmith CLI pip install cloudsmith-cli # Push a package to the repository cloudsmith push python my-org/my-repo my-package-1.0.0.tar.gz Cloudsmith simplifies package distribution and management.\n🎯 Conclusion Artifact management is critical in the software development process. Using tools like Artifactory, Nexus, and Cloudsmith ensures organized, secure, and efficient storage and deployment. Depending on project needs, you can choose the tool that best fits your DevOps workflow.\n🛠️ What tools do you use to manage artifacts? Share your experience!\n👉 Next step: Learn about GitOps - the process of provisioning and configuring resources (servers, networks, storage, accounts) so that systems or applications can operate effectively.\n","date":"2025-02-25T00:00:00Z","image":"https://tech.nguuyen.io.vn/images/devops/devops-step-nineteen.webp","permalink":"https://tech.nguuyen.io.vn/en/posts/devops/devops-step-nineteen/","title":"Artifact Management in Software Development 🏢"},{"content":"Introduction Cloud design patterns are reusable solutions for common problems in cloud architectures. These patterns address scalability, reliability, security, and performance challenges in distributed systems.\nThey provide best practices for designing and deploying cloud applications, including data management, messaging, resiliency, and deployment. Common examples include:\nCircuit Breaker: Prevents cascading failures by temporarily breaking connections after repeated errors. CQRS (Command Query Responsibility Segregation): Separates read and write operations to improve performance. Sidecar Pattern: Splits auxiliary application components into separate processes or containers for more flexibility. 🔹 Availability Availability is the percentage of time a system operates as expected, often called uptime. It can be affected by hardware or software failures, infrastructure issues, cyberattacks, or excessive load.\nCloud providers typically define service level agreements (SLA) that specify guaranteed uptime. For example, a company might promise 99.99% availability.\n🔹 Example: A system that guarantees 99.99% uptime can only be down for 52 minutes per year.\n🔹 Data Management Data management is a key factor in cloud applications and influences most quality attributes. Data is often stored in multiple locations to improve performance, scale, or availability. This introduces challenges such as:\nMaintaining data consistency when synchronizing across servers. Securing data during storage, transit, and access. Scalability to handle growth demands. 🔹 Example: Banking systems must ensure transactions remain consistent across data centers to avoid incorrect account balances.\n🔹 Design and Implementation Good design helps systems stay maintainable, consistent, and reusable across multiple scenarios. Decisions made during design and implementation directly affect cost and overall cloud application quality.\nKey design principles:\nConsistency: Components should follow a defined structure. Scalability: Systems must handle high traffic without performance degradation. Reusability: Components should be usable across different applications. 🔹 Example: An e-commerce system uses microservices architecture to scale individual services independently (payments, cart, product search).\n🔹 Management and Monitoring DevOps management and monitoring covers the lifecycle from planning, development, testing, deployment, to operations. A solid monitoring system tracks the status of applications, services, and infrastructure in production.\n🔹 Key monitoring components:\nReal-time streaming: Observe systems in real time. Historical replay: Store history for incident analysis. Visualization: Present data visually to evaluate system health. 🔹 Example: Using Prometheus + Grafana to monitor Kubernetes container performance.\n🔥 Conclusion Cloud design patterns optimize system architecture by providing solutions for common challenges such as performance, security, and scalability. Choosing the right patterns helps businesses build resilient, flexible, and maintainable systems.\n✅ Wrap-up: You have completed the DevOps journey, combining Development and Operations to speed up software delivery, improve reliability, and optimize workflows. Thank you for following along, and we hope this knowledge supports your next steps! 🚀\n","date":"2025-02-25T00:00:00Z","image":"https://tech.nguuyen.io.vn/images/devops/devops-step-twenty-two.webp","permalink":"https://tech.nguuyen.io.vn/en/posts/devops/devops-step-twenty-two/","title":"Cloud Design Patterns 🌟"},{"content":"Introduction GitOps is a method for managing infrastructure and application deployments by using Git as the single source of truth. It automates deployments, ensures consistency, and improves change tracking.\nGitOps extends DevOps practices by applying version control and CI/CD principles to infrastructure management. Instead of manual changes, every update flows through pull requests and is automatically synchronized with live environments.\n🚀 Benefits of GitOps ✅ Version control: Every change is tracked in Git, making it easy to audit or roll back when needed.\n✅ Automated deployments: Tools like ArgoCD or FluxCD sync environments automatically.\n✅ Improved security: All changes go through Git, preventing direct access to production systems.\n✅ High resilience: Systems can recover quickly by re-applying Git state after incidents.\n✅ Better team collaboration: Changes are reviewed via pull requests, enabling smoother teamwork.\n🛠️ Popular GitOps Tools ArgoCD - Powerful continuous deployment ArgoCD is a continuous deployment (CD) tool for Kubernetes. It automates deployments by watching application state in Git and syncing it with the live cluster.\n🔹 Key features:\nAutomatic sync between Git and environment state. Visual web UI for deployment status. Supports Helm, Kustomize, and other configuration tools. 🔹 Example ArgoCD application\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: my-app namespace: argocd spec: destination: namespace: default server: https://kubernetes.default.svc source: repoURL: https://github.com/my-org/my-repo.git targetRevision: HEAD path: manifests syncPolicy: automated: selfHeal: true prune: true FluxCD - Automated deployment management FluxCD is a GitOps tool that deploys applications to Kubernetes by watching Git repositories and updating clusters automatically.\n🔹 Key features:\nTracks Git repositories and deploys automatically. Supports Helm, Kustomize, and many CI/CD systems. Provides automatic container image updates. 🔹 Example FluxCD configuration:\n1 2 3 4 5 6 7 8 9 10 apiVersion: source.toolkit.fluxcd.io/v1beta1 kind: GitRepository metadata: name: my-app-repo namespace: flux-system spec: interval: 1m0s url: https://github.com/my-org/my-repo.git ref: branch: main 🔥 ArgoCD vs FluxCD Feature ArgoCD FluxCD Visual UI ✅ Yes ❌ No Helm support ✅ Yes ✅ Yes Automatic container image updates ❌ No ✅ Yes Canary/Rollback deployments ✅ Yes ✅ Yes 📌 Conclusion GitOps simplifies application deployments by using Git as the central control layer. With tools like ArgoCD and FluxCD, teams can automate deployments, ensure consistency, and improve system resiliency.\n🔗 References:\n📖 ArgoCD documentation 📖 FluxCD documentation 📘 GitOps guide Have you tried GitOps in your projects? Share your experience! 🚀\n👉 Next step: Learn about Service Mesh - a software infrastructure layer that manages communication between services in microservices systems. It provides load balancing, security, observability, traffic control, and fault handling so services can communicate efficiently without changing application code.\n","date":"2025-02-25T00:00:00Z","image":"https://tech.nguuyen.io.vn/images/devops/devops-step-twenty.webp","permalink":"https://tech.nguuyen.io.vn/en/posts/devops/devops-step-twenty/","title":"GitOps - Automated Deployment with Git 🌟"},{"content":"📌 Introduction Service Mesh is a software infrastructure layer that manages communication between services in microservices systems. It provides load balancing, security, observability, traffic control, and fault handling, enabling services to communicate efficiently without changing application code.\nPopular Service Mesh tools include Istio, Linkerd, and Consul.\n🚀 Benefits of Service Mesh ✅ Automatic service-to-service communication management - No application code changes required. ✅ Enhanced security - TLS encryption, service authentication, RBAC policies. ✅ Performance optimization - Smart load balancing and traffic control. ✅ Comprehensive observability - Logs, metrics, and tracing for monitoring. ✅ High resilience - Automatic fault detection and recovery.\n🛠️ Popular Service Mesh Tools 1️⃣ Istio - The most powerful solution Istio is an open-source Service Mesh platform that provides control, security, and observability for Kubernetes services.\n🔹 Key features:\nmTLS support for secure service communication. Smart load balancing, retries, and timeouts. API gateway management and access policies. 🔹 Istio Gateway example:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: my-gateway spec: selector: istio: ingressgateway servers: - port: number: 80 name: http protocol: HTTP hosts: - \u0026#34;example.com\u0026#34; 📖 Istio documentation\n2️⃣ Linkerd - Lightweight and fast Linkerd is a lightweight Service Mesh optimized for Kubernetes, designed to reduce latency and resource usage.\n🔹 Key features:\nAutomatic TLS (mTLS) for all communications. Built-in tracing and metrics. Quick installation with a single CLI command. 🔹 Linkerd configuration example:\n1 2 3 4 5 6 7 8 9 10 11 apiVersion: linkerd.io/v1alpha2 kind: ServiceProfile metadata: name: my-service.default.svc.cluster.local spec: routes: - name: \u0026#34;GET /health\u0026#34; condition: method: GET pathRegex: \u0026#34;/health\u0026#34; isRetryable: true 📖 Linkerd documentation\n3️⃣ Consul - Service Mesh with service discovery Consul is not only a Service Mesh, but also provides service discovery, configuration management, and secure communication between services.\n🔹 Key features:\nService discovery for Kubernetes and traditional systems. Integration with Envoy Proxy for traffic management. Supports multi-cluster and hybrid cloud environments. 🔹 Consul service registration example:\n1 2 3 4 5 6 7 { \u0026#34;service\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;web\u0026#34;, \u0026#34;port\u0026#34;: 8080, \u0026#34;connect\u0026#34;: { \u0026#34;sidecar_service\u0026#34;: {} } } } 📖 Consul documentation\n🔥 Service Mesh Comparison Feature Istio Linkerd Consul Visual UI ✅ Yes ❌ No ✅ Yes mTLS support ✅ Yes ✅ Yes ✅ Yes High performance ⚠️ Needs tuning ✅ Fast ⚠️ Moderate Service discovery ❌ No ❌ No ✅ Yes 📌 Conclusion Service Mesh simplifies managing communication between microservices, ensuring security, observability, and high performance.\n💡 If you need the most powerful solution, try Istio. 💡 If you prioritize performance and ease of setup, choose Linkerd. 💡 If you need service discovery and multi-cloud, consider Consul.\n📖 References: 🔗 Istio documentation 🔗 Linkerd documentation 🔗 Consul documentation\nHave you tried any Service Mesh deployments? Share your experience! 🚀\n👉 Final step: Learn about Cloud Design Patterns - a set of proven patterns for building efficient, flexible, and scalable cloud applications. They address common challenges like high availability, fault tolerance, elastic scaling, and security in cloud environments.\n","date":"2025-02-25T00:00:00Z","image":"https://tech.nguuyen.io.vn/images/devops/devops-step-twenty-one.webp","permalink":"https://tech.nguuyen.io.vn/en/posts/devops/devops-step-twenty-one/","title":"Service Mesh - Managing Microservices Communication 🌐"},{"content":"🔍 What is CI/CD? CI/CD (Continuous Integration/Continuous Deployment) is a methodology that helps automate the software development process, reducing errors, speeding up deployment, and improving product quality.\nContinuous Integration (CI): Continuous integration, helps detect errors early by automatically testing whenever there are changes in the source code. Continuous Deployment (CD): Continuous deployment, ensures software is released automatically and quickly. 🌟 Popular CI/CD Tools 🔧 Jenkins Jenkins is a popular open-source automation server that helps build, test, and deploy software automatically.\n📌 Key Features:\nSupports many plugins, easy integration with DevOps tools. Intuitive web interface, easy to configure. Supports both Windows and Linux environments. 💡 Example: Configuring a simple pipeline in Jenkinsfile\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 pipeline { agent any stages { stage(\u0026#39;Build\u0026#39;) { steps { echo \u0026#39;Building the application...\u0026#39; } } stage(\u0026#39;Test\u0026#39;) { steps { echo \u0026#39;Running tests...\u0026#39; } } stage(\u0026#39;Deploy\u0026#39;) { steps { echo \u0026#39;Deploying the application...\u0026#39; } } } } 📖 Useful Resources:\n🔗 Jenkins Homepage 🎥 Jenkins Tutorial from A-Z 🔄 CircleCI CircleCI is a powerful CI/CD platform that supports many programming languages and integrates well with GitHub and Bitbucket.\n📌 Key Features:\nSupports parallel builds to increase processing speed. Good integration with Docker and Kubernetes. Available in cloud and self-hosted versions. 💡 Example: Configuring a CircleCI pipeline in .circleci/config.yml\n1 2 3 4 5 6 7 8 9 version: 2.1 jobs: build: docker: - image: circleci/node:14 steps: - checkout - run: npm install - run: npm test 📖 Useful Resources:\n🔗 CircleCI Homepage 🎥 CircleCI Tutorial Video 🛠️ GitLab CI/CD GitLab CI/CD is a built-in system in GitLab that allows automating testing and deployment processes right within the Git repository.\n📌 Key Features:\nSupports full pipeline automation from build, test to deploy. Built into GitLab, no external tools needed. Supports running pipelines on Docker containers. 💡 Example: Configuring GitLab CI/CD pipeline in .gitlab-ci.yml\n1 2 3 4 5 6 7 8 9 10 stages: - build - test - deploy test: stage: test script: - npm install - npm test 📖 Useful Resources:\n🔗 GitLab Homepage 🎥 GitLab CI/CD Tutorial ⚡ GitHub Actions GitHub Actions is a CI/CD tool integrated directly into GitHub, helping automate testing and deployment processes whenever there are changes in the repository.\n📌 Key Features:\nTightly integrated with GitHub, easy workflow setup. Supports many operating systems and programming languages. Can use marketplace with thousands of available actions. 💡 Example: GitHub Actions workflow in .github/workflows/main.yml\n1 2 3 4 5 6 7 8 9 10 11 12 13 name: Node.js CI on: [push, pull_request] jobs: build: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - name: Setup Node.js uses: actions/setup-node@v3 with: node-version: 14 - run: npm install - run: npm test 📖 Useful Resources:\n🔗 GitHub Actions Homepage 🎥 GitHub Actions Tutorial Video 🎯 Conclusion CI/CD helps accelerate the software development process, minimize errors, and improve product quality. Depending on your needs and working environment, you can choose Jenkins, CircleCI, GitLab CI/CD, or GitHub Actions to deploy your CI/CD system. 🚀\n👉 Next step: Learn about Secret Management - the process of storing, managing, and protecting sensitive information such as passwords, API keys, certificates, and access tokens to prevent data leakage and ensure system security. 💡\n","date":"2025-02-24T00:00:00Z","image":"https://tech.nguuyen.io.vn/images/devops/devops-step-thirteen.webp","permalink":"https://tech.nguuyen.io.vn/en/posts/devops/devops-step-thirteen/","title":"CI/CD - Continuous Integration and Continuous Deployment 🚀"},{"content":"🔧 Configuration Management Configuration management is the process of managing and maintaining consistency of components within an information technology system. In the software field, it includes monitoring, tracking, and managing system configuration changes throughout the product lifecycle. Applying configuration management helps increase synchronization, reduce error risks, and ensure compliance with standards in CI/CD (Continuous Integration and Continuous Deployment) processes.\n🌐 Popular Configuration Management Tools 🚀 Ansible Ansible is an open automation tool, primarily used for configuration management, application deployment, and task automation.\n🔄 Features:\nUses YAML (playbooks) to define desired states. Works without requiring agent installation. Suitable for small to large scale environments. 💡 Ansible usage example:\n1 2 3 4 5 6 7 8 9 10 11 12 - name: Install and start Nginx hosts: all become: yes tasks: - name: Install Nginx apt: name: nginx state: present - name: Start Nginx service: name: nginx state: started 📖 Useful free resources:\n📚 Complete Ansible Course for Beginners 🔗 Official Ansible Website 🎥 Ansible in 100 Seconds 🌟 Chef Chef (now part of Progress Chef) is one of the first configuration management tools. It uses Ruby language and emphasizes idempotence (ensuring running n times produces the same result).\n🔄 Features:\nClient/server based. Has Chef-Solo for standalone deployment. Suitable for enterprise environments. 💡 Chef usage example:\n1 2 3 4 5 6 7 package \u0026#39;nginx\u0026#39; do action :install end service \u0026#39;nginx\u0026#39; do action [:enable, :start] end 📖 Useful free resources:\n🔗 Official Chef Website 📚 Chef Tutorial 🎥 Chef Tutorial Video 🏰 Puppet Puppet is a declarative configuration management tool that operates in a client/server model and supports multiple operating systems.\n🔄 Features:\nLarge scale management. Periodic configuration checking and application. Integration with many DevOps tools. 💡 Puppet usage example:\n1 2 3 4 5 6 7 8 package { \u0026#39;nginx\u0026#39;: ensure =\u0026gt; installed, } service { \u0026#39;nginx\u0026#39;: ensure =\u0026gt; running, enable =\u0026gt; true, } 📖 Useful free resources:\n📚 Complete Puppet Course 🔗 Official Puppet Website 🎥 Great Puppet Articles 🎉 Conclusion Configuration management is an important element in the DevOps process, helping ensure consistent system operation, minimize errors, and enhance reliability. Depending on needs and scale, businesses can choose Ansible, Chef, or Puppet to manage infrastructure effectively.\n👉 Next step: Learn about Continuous Integration and Continuous Deployment (CI/CD), which is an automation process in software development that helps integrate, test, and deploy applications continuously.\n","date":"2025-02-24T00:00:00Z","image":"https://tech.nguuyen.io.vn/images/devops/devops-step-twelve.webp","permalink":"https://tech.nguuyen.io.vn/en/posts/devops/devops-step-twelve/","title":"Configuration Management 🔧"},{"content":"🔍 What is Container Orchestration? Container orchestration is the process of managing and automating the container lifecycle, including deployment, scaling, and networking across multiple servers. It is critical for running complex applications in production environments.\nBy using tools like Kubernetes, Docker Swarm, and Apache Mesos, organizations can ensure high availability, scalability, and reliability for their applications. Container orchestration automates operational tasks and provides a strong foundation for microservices, cloud-native development, and DevOps.\n📚 Free resources:\n📦 What is Container Orchestration? 🚀 What is Kubernetes? 🐳 Docker Swarm 🎥 Kubernetes introduction ☸️ Kubernetes Kubernetes is the most popular open-source platform for container management. It allows container deployment across multiple servers, defining availability, deployment logic, and scaling through YAML.\nKubernetes originated from Borg, Google\u0026rsquo;s internal platform, and has become a critical skill for DevOps engineers. Many organizations now have Platform Engineering teams dedicated to supporting Kubernetes for product teams.\n📚 Free resources:\n🗺️ In-depth Kubernetes roadmap 🌐 Official Kubernetes website 📖 Kubernetes overview 🎥 Complete Kubernetes course - beginner to advanced 📌 Example:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion: apps/v1 kind: Deployment metadata: name: my-app spec: replicas: 3 selector: matchLabels: app: my-app template: metadata: labels: app: my-app spec: containers: - name: my-app image: my-app-image:latest ☁️ GKE / EKS / AKS 🔹 GKE - Google Kubernetes Engine Google Kubernetes Engine (GKE) is a managed Kubernetes service by Google Cloud. It helps deploy, manage, and scale container applications with Kubernetes without managing the cluster manually.\n🔹 EKS - Amazon Elastic Kubernetes Service Amazon Elastic Kubernetes Service (EKS) is a Kubernetes service provided by AWS. It automatically manages the Kubernetes control plane and integrates with other AWS services.\n🔹 AKS - Azure Kubernetes Service Azure Kubernetes Service (AKS) is Microsoft Azure’s Kubernetes service. AKS supports monitoring, security, autoscaling, and integration with Azure DevOps.\n📚 Free resources:\n☁️ Google Kubernetes Engine (GKE) 🟧 Amazon Elastic Kubernetes Service (EKS) 🔵 Azure Kubernetes Service (AKS) 🎥 AWS EKS tutorial 🎥 What is Google Kubernetes Engine? 🚀 ECS / Fargate ECS is a container management service that runs on AWS EC2, giving you control over the server infrastructure.\nFargate is a serverless container service that runs containers without managing servers or clusters.\n📚 Free resources:\n📄 AWS Fargate documentation 📄 AWS ECS documentation 🎥 AWS Fargate overview 🎥 AWS ECS tutorial 📌 Example:\n1 2 3 4 5 6 7 8 9 10 11 { \u0026#34;family\u0026#34;: \u0026#34;my-task\u0026#34;, \u0026#34;containerDefinitions\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;my-container\u0026#34;, \u0026#34;image\u0026#34;: \u0026#34;my-container-image:latest\u0026#34;, \u0026#34;memory\u0026#34;: 512, \u0026#34;cpu\u0026#34;: 256 } ] } 🐳 Docker Swarm Docker Swarm is a cluster of Docker nodes (physical or virtual machines). The cluster is managed by a swarm manager, and the participating machines are nodes.\n📚 Free resources:\n📄 Docker Swarm documentation 🔧 Manage Docker Swarm with Portainer 📦 Docker Swarm with GlusterFS storage 🎥 Docker Swarm introduction | step-by-step 📌 Example:\n1 2 3 4 5 # Initialize Swarm $ docker swarm init # Deploy a service on Swarm $ docker service create --name web -p 80:80 nginx ✅ Conclusion Container orchestration plays a vital role in managing containerized applications, helping simplify operations, optimize resources, and improve availability. Tools such as Kubernetes, Docker Swarm, ECS, and Fargate offer flexible solutions for modern enterprises. Choosing the right tool enables organizations to maximize the power of containerization and cloud computing. 🚀\n👉 Next step: Learn about Application Monitoring - tracking, measuring, and analyzing application performance, status, and behavior to detect incidents, optimize performance, and ensure a great user experience. Popular tools include Prometheus, Grafana, Datadog, and New Relic for collecting data from applications, servers, and infrastructure to provide alerts and detailed reports.\n","date":"2025-02-24T00:00:00Z","image":"https://tech.nguuyen.io.vn/images/devops/devops-step-seventeen.webp","permalink":"https://tech.nguuyen.io.vn/en/posts/devops/devops-step-seventeen/","title":"Container Orchestration 🚢"},{"content":"🔍 What is Infrastructure Monitoring? Infrastructure monitoring is the process of tracking the performance and status of systems to detect incidents early and optimize operations. This is a broad field with many tools, each with its own strengths and weaknesses. Understanding these tools helps you choose the right solution for your monitoring goals.\n🚀 Popular Monitoring Tools Grafana 📝 Overview: Grafana is an open-source web application for data analysis and visualization. It connects to many data sources such as time series databases, relational databases, and cloud services.\n🔹 Key features:\nPowerful visualization with many chart types. Supports a wide range of plugins. Real-time alerting system. User authentication and role-based access control. 📌 Example: Use Grafana to monitor CPU and RAM usage to detect and handle overload incidents.\n📚 Useful resources:\n🔗 Grafana homepage 📖 Installation and usage guides Prometheus 📝 Overview: Prometheus is an open-source monitoring and alerting tool, especially well-suited for microservices and containerized systems like Kubernetes.\n🔹 Key features:\nMulti-dimensional data model. Powerful PromQL query language. Pull-based data collection model. Smart alert management with Alertmanager. 📌 Example: Use Prometheus to collect and analyze API request counts to identify peak traffic and optimize performance.\n📚 Useful resources:\n🔗 Prometheus homepage 🎥 Prometheus introduction Zabbix 📝 Overview: Zabbix is an open-source monitoring platform that supports comprehensive tracking for system components such as servers, networks, applications, and services.\n🔹 Key features:\nMultiple data collection methods: Agent, SNMP, IPMI, custom scripts. Real-time alerts and notifications. Detailed dashboards and reporting system. Scales well for large environments. 📌 Example: Use Zabbix to monitor server status, detect downtime, and send alerts immediately.\n📚 Useful resources:\n🔗 Zabbix homepage 📖 Zabbix documentation ✅ Conclusion Each monitoring tool has its own strengths:\nGrafana: Strong data visualization. Prometheus: Great for container and microservices environments. Zabbix: End-to-end monitoring for large systems. Depending on your specific needs, you can combine multiple tools to build an optimal monitoring system. 🚀\n👉 Next step: Learn about Logs Management - the process of collecting, storing, processing, and analyzing logs from systems, applications, and devices to track activity, detect incidents, ensure security, and support faster troubleshooting.\n","date":"2025-02-24T00:00:00Z","image":"https://tech.nguuyen.io.vn/images/devops/devops-step-fifteen.webp","permalink":"https://tech.nguuyen.io.vn/en/posts/devops/devops-step-fifteen/","title":"Infrastructure Monitoring 📊"},{"content":"🔹 What is Provisioning? Provisioning refers to the process of setting up and configuring the necessary IT infrastructure to support an application or service. This includes allocating and preparing resources such as servers, storage, networking, and software environments.\nWhile provisioning can be done manually, in modern DevOps, this process is typically automated using tools like Terraform, Pulumi, CloudFormation. Using Infrastructure-as-Code (IaC) helps define the entire provisioning process in version-controlled script files, ensuring consistency, reducing human errors, and improving scalability and disaster recovery.\n📖 Free resources to learn:\n📄 What is provisioning? - RedHat 📄 What is provisioning? - IBM 🎥 Open Answers: What is provisioning? 🏗️ Terraform - Powerful IaC Solution Terraform is an open-source Infrastructure-as-Code (IaC) tool developed by HashiCorp that helps define, deploy, and manage infrastructure on multi-cloud or on-premises environments using declarative configuration files.\n🌟 Benefits of Using Terraform ✅ Multi-platform support: AWS, Azure, Google Cloud, Kubernetes, etc. ✅ State management: Helps track infrastructure resources. ✅ Scalability and reusability: Easy to modularize configurations. ✅ CI/CD integration: Automates infrastructure deployment.\n🔨 Example: Creating an EC2 instance on AWS 1 2 3 4 5 6 7 8 9 10 11 provider \u0026#34;aws\u0026#34; { region = \u0026#34;us-east-1\u0026#34; } resource \u0026#34;aws_instance\u0026#34; \u0026#34;web\u0026#34; { ami = \u0026#34;ami-12345678\u0026#34; instance_type = \u0026#34;t2.micro\u0026#34; tags = { Name = \u0026#34;Terraform-Instance\u0026#34; } } 📖 Free resources to learn:\n📍 Detailed Terraform Roadmap 🎥 Complete Terraform Course 📄 Official Terraform Documentation 📖 How to Scale Your Terraform Infrastructure 🔥 Explore Top Terraform Articles 🔹 AWS CDK - An Alternative? AWS Cloud Development Kit (AWS CDK) is an open-source framework for provisioning AWS infrastructure using code in languages like TypeScript, Python, Java, C#, Go. AWS CDK uses CloudFormation to deploy resources safely and repeatedly.\n📖 Free resources to learn:\n🎥 AWS CDK Course for Beginners 📄 Official AWS CDK Documentation 📂 AWS CDK Examples 🔥 Explore Top AWS Articles 📌 Conclusion Terraform is the leading tool for Infrastructure-as-Code, providing flexibility and powerful automation capabilities across multiple cloud platforms. If you work extensively with AWS and want to deploy using programming languages, AWS CDK is also a viable option to consider.\nDepending on project requirements, you can choose the appropriate tool to manage infrastructure more effectively. 🚀\n👉 Next step: Learn about Configuration Management - the process of managing, monitoring, and automating system, software, and infrastructure configurations to ensure consistency, stability, and easy control throughout their lifecycle. It helps track and control changes, reduce errors from manual configuration, and support rapid deployment. Popular tools include Ansible, Puppet, Chef, and SaltStack.\n","date":"2025-02-24T00:00:00Z","image":"https://tech.nguuyen.io.vn/images/devops/devops-step-eleven.webp","permalink":"https://tech.nguuyen.io.vn/en/posts/devops/devops-step-eleven/","title":"Infrastructure Provisioning with Terraform 🟪🔳"},{"content":"🔍 What is Logs Management? Logs management is the process of collecting, aggregating, analyzing, storing, and retrieving logs from applications and infrastructure systems. Logs contain critical information about system activity, helping detect incidents, optimize performance, and ensure security compliance.\n🚀 Popular Logs Management Tools Elastic Stack (ELK Stack) 📝 Overview: Elastic Stack, formerly known as the ELK Stack, includes Elasticsearch (search and analytics), Logstash (data processing), Kibana (visualization), and Beats (data collection). It is a popular solution for logs management with high scalability.\n🔹 Key features:\nReal-time log search and analysis. Supports multiple data formats. Scales well for enterprise environments. Provides a visual interface with Kibana. Use Elastic Stack to collect logs from web servers, analyze errors, and create dashboards that track traffic patterns.\n📚 Useful resources:\n🔗 Elastic Stack homepage 🎥 Elastic Stack overview Loki 📝 Overview: Loki is a log aggregation system developed by Grafana Labs, optimized for Kubernetes and containers. Loki stores log labels instead of full-text indexes, saving resources.\n🔹 Key features:\nTight integration with Grafana. Uses LogQL, a query language similar to PromQL. Optimized for Kubernetes and container environments. Resource-efficient compared to other solutions. Use Loki to track logs from containers in Kubernetes to detect application errors quickly.\n📚 Useful resources:\n🔗 Loki homepage 📖 Loki documentation Graylog 📝 Overview: Graylog is an open-source logs management platform that supports real-time log collection, storage, and analysis. It provides a friendly web interface and supports multiple log data types.\n🔹 Key features:\nSupports log collection protocols such as Syslog and GELF. Powerful search interface. Alerting capabilities for incident detection. Real-time log queries. Use Graylog to monitor security system logs and detect intrusion behavior.\n📚 Useful resources:\n🔗 Graylog homepage 🎥 Graylog usage guides 🔥 Example Code Below is an example of sending logs to a log collection system using JavaScript:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 const winston = require(\u0026#39;winston\u0026#39;); const logger = winston.createLogger({ level: \u0026#39;info\u0026#39;, format: winston.format.json(), transports: [ new winston.transports.Console(), new winston.transports.File({ filename: \u0026#39;app.log\u0026#39; }) ] }); // Write logs logger.info(\u0026#39;Application started successfully\u0026#39;); logger.warn(\u0026#39;Warning: Invalid input data\u0026#39;); logger.error(\u0026#39;Error: Unable to connect to the database\u0026#39;); ✅ Conclusion Logs management plays a vital role in monitoring systems and resolving incidents quickly. Tools such as Elastic Stack, Loki, and Graylog provide powerful solutions for log collection, analysis, and visualization. Choosing the right tool will help improve operational efficiency. 🚀\n👉 Next step: Learn about Container Orchestration - the process of automatically managing, deploying, scaling, and coordinating containers in infrastructure environments, ensuring availability, scalability, and optimized resource usage. Common tools for container orchestration include Kubernetes, Docker Swarm, and Apache Mesos.\n","date":"2025-02-24T00:00:00Z","image":"https://tech.nguuyen.io.vn/images/devops/devops-step-sixteen.webp","permalink":"https://tech.nguuyen.io.vn/en/posts/devops/devops-step-sixteen/","title":"Logs Management 📜"},{"content":"🔍 What is Secret Management? Secret Management is the process of protecting, storing, and securely distributing sensitive information such as passwords, API keys, and certificates in an organization\u0026rsquo;s information technology system. It helps prevent unauthorized access while ensuring authorized systems and users can use the information when needed.\nSecret Management typically includes features:\nData encryption during storage and transmission. Access control, only allowing authorized people or systems to access. Key rotation mechanism to change passwords periodically. Integration with DevOps systems to automate security processes. 🔗 Additional References:\n📄 How to manage secrets in web applications 📄 Why DevSecOps needs secret management 🎥 DevOps tips for managing secrets in production environments 🏛️ HashiCorp Vault HashiCorp Vault is a secret management tool that helps protect sensitive data such as passwords, API keys, and encryption keys.\n🔑 Key Features: Centralized management helps easily control secrets. Supports multiple authentication methods such as LDAP, Kubernetes. Provides dynamic secrets helps create temporary secrets when needed. Strong data encryption during storage and transmission. 🔗 Examples:\nStore and manage TLS certificates for a microservices architecture.\nIntegrate with Kubernetes to provide dynamic secrets for Pods each time they start.\nCreate temporary passwords for databases to reduce the risk of information leakage.\n🛠 Practical Examples: 1️⃣ Provision dynamic passwords for PostgreSQL using Vault\nSuppose you have a PostgreSQL database and want to provision a temporary account:\n1 vault write database/creds/postgres-role This command will create a new account with limited lifetime.\n2️⃣ Use Vault to manage API keys in Node.js applications\nYou can retrieve API keys from Vault in your source code:\n1 2 3 const { execSync } = require(\u0026#39;child_process\u0026#39;); const secret = execSync(\u0026#39;vault kv get -field=value secret/my-api-key\u0026#39;).toString(); console.log(`API Key: ${secret}`); 🔗 Useful Resources:\n🌍 HashiCorp Vault Homepage 🛠️ HashiCorp Vault Source Code on GitHub 🎥 HashiCorp Vault Introduction in 180 seconds 🎥 HashiCorp Vault Tutorial for Beginners ☁️ Cloud Secret Management Tools 🔥 AWS Secrets Manager Provides secure secret storage and management service on AWS with automatic password rotation capability and integration with other AWS services.\n🌍 Google Cloud Secret Manager Secret management solution on Google Cloud, allowing automatic password rotation and easy access control.\n🔷 Azure Key Vault Microsoft Azure service that helps securely store and manage keys, passwords, and digital certificates.\n🔗 Examples:\nAWS Secrets Manager: Store API keys of a web application, ensuring API keys are not exposed in source code.\nHashiCorp Vault: Manage database access passwords for PostgreSQL.\nAzure Key Vault: Store and secure application encryption keys.\n🛠 Practical Examples: 1️⃣ Use AWS Secrets Manager to manage API keys\nSuppose you have a web application that needs to access a third-party service through an API key. Instead of storing the API key in source code, you can store it in AWS Secrets Manager and call it when needed:\n1 aws secretsmanager get-secret-value --secret-id my-api-key 2️⃣ Create dynamic secrets with HashiCorp Vault\nHashiCorp Vault can create temporary passwords for databases to enhance security. For example, you can create a temporary account for PostgreSQL:\n1 vault write database/creds/my-role 🔗 Useful Resources:\n📄 AWS Secrets Manager – AWS Secret Management Service 📄 Google Cloud Secret Manager – Google Cloud Secret Management Service 📄 Azure Key Vault – Azure Key and Secret Management Service 🎥 AWS Secrets Manager Demo – AWS Secrets Manager Tutorial 🎥 Google Cloud Secret Manager – Google Cloud Secret Manager Introduction 🎥 Azure Key Vault and How to Use – Azure Key Vault Tutorial 🔐 Sealed Secrets Sealed Secrets is a tool for Kubernetes that helps encrypt sensitive data into SealedSecrets, which can be safely stored even in public environments like GitHub.\n🛠️ How it works: Encryption: User creates a SealedSecret from a Kubernetes Secret. Storage: SealedSecret can be committed to Git. Decryption: Only the Kubernetes Controller in the cluster can decrypt and restore it to a regular Kubernetes Secret. 💡 Highlights:\nUses asymmetric encryption ensuring only the controller can decrypt data. Supports GitOps helps manage secrets safely in Git repository. Easy integration with Kubernetes to protect sensitive data. 🔗 Useful Resources:\n🛠️ Sealed Secrets on GitHub 📄 Sealed Secrets Documentation 🔄 Integrating Secret Management into CI/CD CI/CD tools like Azure DevOps, Travis CI, and AWS CodePipeline support secret management by integrating with Secret Management systems to protect sensitive information during deployment.\n🏗️ Azure DevOps Azure DevOps provides Azure Key Vault to securely store and manage secrets. Pipelines in Azure DevOps can retrieve secrets from Key Vault for use during deployment.\n🚀 Travis CI Travis CI supports environment variable encryption, helping protect API keys and sensitive information during builds.\n🌩️ AWS CodePipeline AWS CodePipeline can integrate with AWS Secrets Manager to retrieve secrets during application deployment.\n🎯 Conclusion Secret management plays an important role in modern system security. Using tools such as HashiCorp Vault, AWS Secrets Manager, Google Cloud Secret Manager, Azure Key Vault, Sealed Secrets, Azure DevOps, Travis CI, and AWS CodePipeline helps ensure the safety of sensitive information, supports secure DevOps deployment, and complies with security standards.\n👉 Next step: Monitor infrastructure Infrastructure Monitoring - the process of tracking and analyzing the performance, availability, and status of information technology infrastructure components such as servers, networks, storage, databases, and cloud computing services, helping to detect incidents early, optimize resources, and ensure stable system operation. 🚀\n","date":"2025-02-24T00:00:00Z","image":"https://tech.nguuyen.io.vn/images/devops/devops-step-fourteen.webp","permalink":"https://tech.nguuyen.io.vn/en/posts/devops/devops-step-fourteen/","title":"Secret Management 🔒"},{"content":"🚀 What is Serverless? Serverless is a cloud computing model where service providers fully manage the infrastructure, allowing developers to focus solely on writing code. The system automatically allocates resources based on demand and charges only for actual resource usage. Serverless architecture is commonly applied for microservices applications, event processing and helps minimize operational costs.\n📖 Free resources to learn:\n📄 What is Serverless? 🎥 Introduction to Serverless 🌍 Great articles about Serverless ⚡ AWS Lambda AWS Lambda is a serverless service from AWS that allows running code without managing servers. Lambda automatically scales based on demand, supports multiple programming languages, and easily integrates with other AWS services. It\u0026rsquo;s suitable for data processing, task automation, building microservices.\n🖥️ Example: Deploying a function on AWS Lambda\n1 2 3 4 5 6 import json def lambda_handler(event, context): return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps(\u0026#39;Hello from AWS Lambda!\u0026#39;) } 📖 Free resources to learn:\n🔗 Introduction to AWS Lambda 🎥 AWS Lambda Tutorial from A-Z 🌍 Great articles about AWS Lambda 🌍 Cloudflare Cloudflare is a company that provides CDN, security, performance optimization services for websites. Cloudflare acts as a reverse proxy, helping to speed up page loading and protect websites from attacks. The company was founded in 2009 and went public in 2019.\n📖 Free resources to learn:\n🔗 Cloudflare Homepage 🎥 Introduction to Cloudflare 🌍 Great articles about Cloudflare 🌐 Vercel Vercel is a frontend deployment platform that helps deploy web applications to the cloud quickly. It supports React, Next.js, Vue, Angular, integrates with GitHub, and allows deployment with just a push command.\n📖 Free resources to learn:\n🔗 Vercel Homepage 📖 Official Vercel Documentation 🎥 Vercel Usage Guide 🌍 Great articles about Vercel 📌 Conclusion Serverless helps automate deployment, reduce costs, and easily scale. Popular platforms:\nAWS Lambda: Event processing without servers. Cloudflare: CDN and website security. Vercel: Fast frontend deployment. Platform selection depends on your needs, technology, and budget. 🚀\n👉 Next step: Learn about Provisioning - the process of providing and configuring resources (servers, networks, storage, accounts) so that systems or applications can operate effectively.\n","date":"2025-02-24T00:00:00Z","image":"https://tech.nguuyen.io.vn/images/devops/devops-step-ten.webp","permalink":"https://tech.nguuyen.io.vn/en/posts/devops/devops-step-ten/","title":"Serverless and Related Platforms ☁️"},{"content":"Setting up important network components This article will help you understand important network components:\n🔹 Forward Proxy\n🔹 Reverse Proxy\n🔹 Load Balancer\n🔹 Firewall\n🔹 Caching Server\n🔹 Web Server\n⚖️ Load Balancer Load Balancer works like a \u0026ldquo;traffic cop\u0026rdquo; standing in front of servers and directing client requests to appropriate servers. This helps optimize speed, efficiently utilize resources, and avoid overload situations.\n🔹 If a server fails, the Load Balancer will redirect traffic to the remaining servers.\n🔹 Can be deployed with algorithms like Round Robin, Least Connections, IP Hash\u0026hellip;\n🔍 Example Load Balancer configuration with Nginx: 1 2 3 4 5 6 7 8 9 10 11 upstream backend_servers { server backend1.example.com; server backend2.example.com; } server { listen 80; location / { proxy_pass http://backend_servers; } } 📚 Further reading:\n📄 What is Load Balancing?\n📄 Load Balancing Algorithms\n📄 Nginx Reverse Proxy \u0026amp; Load Balancing\n🎥 Video: How does Load Balancer work?\n🔁 Forward Proxy Forward Proxy is an intermediary server standing between client and internet, forwarding requests from client to destination server. It helps with anonymity, security, access control, and content caching.\n🔹 Commonly used in enterprise networks to monitor and control access.\n🔹 Supports bypassing censorship and geographical restrictions.\n🔍 Example Forward Proxy configuration with Squid: 1 2 3 4 5 6 7 8 9 10 11 apt update \u0026amp;\u0026amp; apt install squid -y # Edit configuration file nano /etc/squid/squid.conf # Add simple configuration http_access allow all http_port 3128 # Restart service systemctl restart squid 📚 Further reading:\n📄 What is Forward Proxy?\n📄 Forward Proxy vs Reverse Proxy comparison\n🎥 Video: How does Proxy work?\n🔄 Reverse Proxy Reverse Proxy is an intermediary server that receives requests from clients and forwards them to appropriate backend servers. It helps with load balancing, caching, security, and SSL termination.\n🔹 Helps hide backend server information to enhance security.\n🔹 Supports traffic distribution and application performance optimization.\n🔍 Example Reverse Proxy configuration with Nginx: 1 2 3 4 5 6 7 8 9 10 server { listen 80; server_name example.com; location / { proxy_pass http://backend_server; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; } } 📚 Further reading:\n📄 What is Reverse Proxy?\n📄 Nginx Reverse Proxy Guide\n🎥 Video: Reverse Proxy and practical applications\n🔥 Firewall Firewall is a network security device that monitors and filters incoming/outgoing traffic based on organization\u0026rsquo;s security policies.\n🔹 Prevents unauthorized access to internal systems.\n🔹 Supports data traffic control rules.\n🔍 Example Firewall configuration with UFW (Uncomplicated Firewall): 1 2 3 4 5 6 7 8 9 10 11 # Install UFW apt install ufw -y # Open SSH port ufw allow 22/tcp # Block all other connections ufw default deny incoming # Enable UFW ufw enable 📚 Further reading:\n📄 What is Firewall?\n📄 Common Firewall types\n🎥 Video: Introduction to Firewall\n🌐 Nginx Nginx is an open-source web server, widely used for its ability to handle many concurrent connections with high performance.\n🔹 Supports web server, reverse proxy, load balancing, caching.\n🔹 Suitable for microservices systems and containers.\n🔍 Example simple Nginx configuration: 1 2 3 4 5 6 server { listen 80; server_name example.com; root /var/www/html; index index.html; } 📚 Further reading:\n📄 Nginx installation guide on Ubuntu\n🎥 Video: Nginx in 100 seconds\n🏛️ Apache Apache is one of the most popular web servers, supporting many extension modules and compatible with many operating systems.\n🔹 Easy to configure with .conf files.\n🔹 Supports SSL/TLS, user authentication, URL rewriting\u0026hellip;\n🔍 Example simple Apache configuration: 1 2 3 4 5 6 7 8 \u0026lt;VirtualHost *:80\u0026gt; ServerName example.com DocumentRoot /var/www/html \u0026lt;Directory /var/www/html\u0026gt; AllowOverride All Require all granted \u0026lt;/Directory\u0026gt; \u0026lt;/VirtualHost\u0026gt; 📚 Further reading:\n📄 Apache homepage\n🎥 Video: Installing Apache on Ubuntu\n✅ Conclusion 🔹 Load Balancer helps distribute traffic efficiently, reducing server load.\n🔹 Forward Proxy supports anonymity, caching, and access control from client.\n🔹 Reverse Proxy helps enhance security, caching, and optimize backend systems.\n🔹 Firewall protects systems from unauthorized access.\n🔹 Nginx \u0026amp; Apache are two popular web servers, serving web content and applications.\nBy implementing these components, you can build a powerful, secure, and efficient network system. 🚀\n👉 Next step: Learn about Networking Protocols - a set of rules and standards that define how devices in a network communicate with each other. They ensure data is transmitted accurately, securely, and efficiently between different systems.\n","date":"2025-02-23T00:00:00Z","image":"https://tech.nguuyen.io.vn/images/devops/devops-step-seven.webp","permalink":"https://tech.nguuyen.io.vn/en/posts/devops/devops-step-seven/","title":"Application Gateway 🌐"},{"content":"🌐 Cloud Providers Cloud service providers offer an API layer to abstract infrastructure, enabling resource deployment based on security standards and billing models. While cloud services actually run on servers in data centers, abstraction layers create the feeling of interacting with a single platform. The ability to quickly deploy, configure, and secure resources has made cloud a critical factor in the success and complexity of modern DevOps.\n📖 Free resources to learn:\n📄 Cloud Service Provider 📄 What are Cloud Providers? 🌍 Great articles about Cloud ☁️ AWS (Amazon Web Services) AWS has been the leading cloud computing platform since 2011, far ahead of Azure and Google Cloud. AWS provides over 200 services, operating on a global scale. AWS delivers flexible and cost-effective computing solutions, including: computing power, data storage, content distribution, etc.\n🖥️ Example: Create an EC2 instance using AWS CLI\n1 aws ec2 run-instances --image-id ami-12345678 --count 1 --instance-type t2.micro --key-name MyKeyPair --security-groups MySecurityGroup 📖 Free resources to learn:\n🎥 100 hours AWS course - 2024 🔗 AWS Homepage 📄 Guide to creating AWS account 🌍 Great articles about AWS 💠 Microsoft Azure Azure is Microsoft\u0026rsquo;s cloud computing platform, providing IaaS, PaaS, SaaS along with many services like analytics, AI, machine learning, security. Azure supports multiple tools and programming languages, helping businesses develop rapidly.\n🖥️ Example: Deploy application on Azure App Service\n1 az webapp create --resource-group MyResourceGroup --plan MyAppServicePlan --name MyUniqueApp --runtime \u0026#34;PYTHON:3.8\u0026#34; 📖 Free resources to learn:\n🔗 Azure Homepage 📖 Microsoft Azure Guide 🎥 Azure Fundamentals Certification (AZ-900) 🌍 Great articles about Azure ☁️ Google Cloud Platform (GCP) Google Cloud provides over 150 services, running on the same infrastructure as Google products like Search, Gmail, YouTube. Services include: VMs, databases, AI/ML, Kubernetes, etc.\n🖥️ Example: Create a VM on Google Cloud\n1 gcloud compute instances create my-instance --machine-type=e2-medium --image-project=debian-cloud --image-family=debian-11 📖 Free resources to learn:\n🔗 Google Cloud Homepage 📖 5 tips to become Google Cloud Certified 🎥 Google Cloud Platform Course - 2023 🌍 Great articles about Google Cloud 🌊 DigitalOcean DigitalOcean is a cloud infrastructure provider focused on simplicity, low cost, ease of use. DigitalOcean provides services like virtual machines (Droplets), databases, Kubernetes, object storage, suitable for startups and developers.\n🖥️ Example: Create a Droplet on DigitalOcean using API\n1 curl -X POST -H \u0026#34;Content-Type: application/json\u0026#34; -H \u0026#34;Authorization: Bearer YOUR_TOKEN\u0026#34; -d \u0026#39;{\u0026#34;name\u0026#34;:\u0026#34;example-droplet\u0026#34;,\u0026#34;region\u0026#34;:\u0026#34;nyc3\u0026#34;,\u0026#34;size\u0026#34;:\u0026#34;s-1vcpu-1gb\u0026#34;,\u0026#34;image\u0026#34;:\u0026#34;ubuntu-20-04-x64\u0026#34;}\u0026#39; \u0026#34;https://api.digitalocean.com/v2/droplets\u0026#34; 📖 Free resources to learn:\n🔗 DigitalOcean Homepage 📄 DigitalOcean\u0026rsquo;s Hacktoberfest 🎥 Kubernetes on DigitalOcean Tutorial 🌍 Great articles about DigitalOcean 📌 Conclusion Cloud service providers like AWS, Azure, GCP, DigitalOcean provide flexible solutions for all server, storage, AI, DevOps needs. Each platform has its own advantages:\nAWS: Comprehensive, most services available. Azure: Good integration with Microsoft ecosystem. GCP: Optimized for AI, big data. DigitalOcean: Simple, suitable for startups. Choosing the right platform depends on your goals, budget, technical requirements. 🚀\n👉 Next step: Learn about Serverless - a cloud computing model that allows running applications without managing servers. Cloud providers automatically allocate resources, scale, and charge based on actual resource usage, helping optimize costs and simplify deployment.\n","date":"2025-02-23T00:00:00Z","image":"https://tech.nguuyen.io.vn/images/devops/devops-step-nine.webp","permalink":"https://tech.nguuyen.io.vn/en/posts/devops/devops-step-nine/","title":"Cloud Services 🌩️"},{"content":"📦 Containers, Docker and LXC Containers are lightweight, portable, and isolated environments that help package applications along with all their dependencies, ensuring consistent deployment across different environments. Container technology simplifies application deployment processes, supports microservices architecture models, and optimizes system resources.\n🏗️ What are Containers? Containers are an operating system-level virtualization method that allows running multiple isolated applications on the same kernel. Unlike virtual machines (VMs) that require separate operating systems for each environment, containers only use the host operating system\u0026rsquo;s kernel, helping reduce resource costs and increase performance.\n🎯 Key Characteristics of Containers 🏋️ Lightweight: Share kernel with host operating system, reducing resource consumption. 🚀 Portable: Run consistently across multiple platforms from personal computers to cloud. 🔒 Isolated: Applications and libraries are packaged separately. 📈 High Performance: No need to boot separate operating systems like virtual machines. 🐳 Docker - The Most Popular Container Platform Docker is an open-source platform that helps automate application deployment using container technology. Docker helps package applications with all necessary libraries and configurations to run across different environments.\n✨ Notable Features of Docker 📦 Docker Engine: Tool for creating and running containers. 🔄 Docker Compose: Manage multiple containers in one application. 🏗️ Docker Hub: Repository for storing and sharing container images. 🔍 Docker Usage Example: 1 docker run -d -p 80:80 nginx The above command will run an Nginx container on port 80.\n📚 Useful Resources:\n📖 Docker Documentation 🎥 Docker in 5 Minutes 🖥️ LXC - Linux Containers LXC (Linux Containers) is an operating system-level virtualization method that allows running multiple isolated Linux systems on the same kernel.\n🛠️ LXC Characteristics: 🏗️ Creates environments similar to virtual machines but with higher performance. ⚡ Faster startup compared to traditional VMs. 🔍 Uses Linux technologies like cgroups and namespaces. 📌 Example of Creating an LXC Container: 1 2 lxc-create -n my-container -t ubuntu lxc-start -n my-container -d 📚 Useful Resources:\n📖 LXC Homepage 🎥 LXC Usage Guide 🎯 Conclusion Containers help deploy applications quickly, efficiently, and save resources. Docker is a popular choice for application development, while LXC is more suitable for full operating system simulation. Choose the tool that fits your needs! 🚀\n👉 Next Step: Learn about Application Gateway - an application-layer traffic management service that helps optimize, secure, and control access flow between clients and backends. It can act as a reverse proxy, protecting the system and ensuring requests are processed correctly.\n","date":"2025-02-23T00:00:00Z","image":"https://tech.nguuyen.io.vn/images/devops/devops-step-six.webp","permalink":"https://tech.nguuyen.io.vn/en/posts/devops/devops-step-six/","title":"Containers, Docker and LXC 🏗️"},{"content":"🖧 Networking Protocols Networking protocols are a set of standardized rules that help data be transmitted, received, and understood correctly across computer networks. They define the format, timing, sequence, and error control during data transmission. Some important protocols include:\nTCP/IP: The foundational protocol suite for Internet communication. HTTP/HTTPS: Hypertext transfer protocol used for the web. FTP/SFTP: File transfer protocols. SMTP/POP3/IMAP: Email transmission protocols. DNS: Domain name resolution protocol. DHCP: Automatic IP address allocation protocol. SSL/TLS: Data security protocols. UDP: Connectionless, fast transmission protocol. 🌍 Domain Name System (DNS) DNS (Domain Name System) is a domain name resolution system that helps convert memorable domain names (e.g., www.example.com) into IP addresses (192.168.1.1) that computers can understand.\n🔹 Example DNS configuration in Linux: 1 2 3 4 5 6 7 8 9 # Check DNS for a domain name nslookup example.com dig example.com # Edit hosts file to map domain names sudo nano /etc/hosts # Add the following line: 192.168.1.100 mycustomdomain.com 🔗 Reference resources:\n📄 How DNS works 🎥 DNS explanation video 🌐 HTTP Protocol HTTP (Hypertext Transfer Protocol) is a data transmission protocol on the web following a request-response model.\n🔹 Example sending HTTP requests with cURL: 1 2 3 4 5 6 7 # Send GET request curl -X GET https://jsonplaceholder.typicode.com/posts/1 # Send POST request curl -X POST https://jsonplaceholder.typicode.com/posts \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;title\u0026#34;: \u0026#34;Hello\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;World\u0026#34;}\u0026#39; 🔗 Reference resources:\n📄 Learn about HTTP 🎥 HTTP tutorial video 🔒 HTTPS and Security (SSL/TLS) HTTPS is the secure version of HTTP, using SSL/TLS to encrypt data, ensuring safety during transmission over the Internet.\n🔹 Example setting up HTTPS with Nginx: 1 2 3 4 5 6 server { listen 443 ssl; server_name example.com; ssl_certificate /etc/letsencrypt/live/example.com/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/example.com/privkey.pem; } 🔗 Reference resources:\n📄 What is HTTPS? 🎥 How HTTPS works video 🔑 SSH - Secure Connection SSH (Secure Shell) is a protocol that enables secure connection to remote servers.\n🔹 Example using SSH for remote connection: 1 2 3 4 5 # Connect to remote server ssh user@example.com # Copy files from server to local machine scp user@example.com:/path/to/file ./localfile 🔗 Reference resources:\n📄 SSH tutorial 🎥 How SSH works video 🎯 Conclusion Networking protocols are the foundation of all online systems, from web browsing to sending emails. Understanding and knowing how to use them helps improve system security and performance. Try applying the commands above to check and configure your system! 🚀\n👉 Next step: Learn about Cloud Providers - companies that provide cloud computing services, allowing individuals and businesses to access resources like servers, storage, databases, AI, and other services over the internet without investing in hardware infrastructure.\n","date":"2025-02-23T00:00:00Z","image":"https://tech.nguuyen.io.vn/images/devops/devops-step-eight.webp","permalink":"https://tech.nguuyen.io.vn/en/posts/devops/devops-step-eight/","title":"Networking Protocols 🖧"},{"content":"🌍 Translation in Progress We are gradually translating the entire website to English to serve our international readers better.\nCurrently Available in English: About page DevOps series (in progress) Selected technical articles Coming Soon: Complete DevOps tutorial series Docker optimization guides Nginx installation guides All technical content Thank you for your patience as we work to make all content accessible in English! 🚀\n","date":"2025-02-23T00:00:00Z","permalink":"https://tech.nguuyen.io.vn/en/posts/devops/notice/","title":"Notice"},{"content":"🌍 Source Code Hosting Services (Repo Hosting Services) When working in teams, you need a remote place to store source code so everyone can access it, create their own branches, and create or review pull requests. These services typically include issue tracking, code review, and continuous integration (CI/CD). Popular options include GitHub, GitLab, Bitbucket, and AWS CodeCommit.\n📚 Free resources to learn more:\n🔗 GitHub 🔗 GitLab 🔗 BitBucket 🎥 GitHub vs GitLab vs Bitbucket - Which one to choose? 🐙 GitHub GitHub is a Git-based source code management platform that provides cloud-based code repository hosting services. It supports features like bug tracking, task management, and project wikis. GitHub enables code review through pull requests, issue tracking, and supports collaborative programming with features like fork and star.\nGitHub supports both public and private repositories, making it a popular choice for both open-source projects and personal development. The GitHub ecosystem includes:\n🚀 GitHub Actions: Workflow automation. 📦 GitHub Packages: Software package management. 🌐 GitHub Pages: Free static website hosting. 📚 Free resources to learn more:\n🗺️ Git \u0026amp; GitHub Roadmap 🔗 GitHub Homepage 📖 How to use Git in professional development teams 🎥 What is GitHub? 📰 Great articles about GitHub 🦊 GitLab GitLab is a comprehensive DevOps tool that provides Git repository management along with wikis, issue tracking, and built-in CI/CD features. It\u0026rsquo;s a complete DevOps platform covering all stages from planning, development, testing to deployment and monitoring.\nGitLab supports both cloud and self-hosted versions, suitable for organizations with high security requirements. Some notable GitLab features include:\n🔄 Built-in CI/CD: Supports automated testing and deployment. 📦 Container \u0026amp; Package Registry: Package management and storage. 🔎 Source code security scanning: Detects vulnerabilities in code. 📚 Free resources to learn more:\n🔗 GitLab Homepage 📖 Official GitLab Documentation 🎥 What is GitLab and why use it? 📰 Great articles about GitLab 🏗️ Bitbucket Bitbucket is Atlassian\u0026rsquo;s source code repository hosting service that supports both Git and Mercurial. It integrates tightly with other Atlassian tools like Jira and Trello, making project management easier. Bitbucket offers both cloud and self-hosted versions.\nSome notable Bitbucket features:\n🔍 Code Review \u0026amp; Pull Requests: Supports code review. 🔄 Bitbucket Pipelines: Built-in CI/CD. 📖 Wiki \u0026amp; Issue Tracking: Documentation management and issue tracking. 🔐 Free private repository support: Suitable for small teams. 📚 Free resources to learn more:\n🔗 Bitbucket Homepage 📖 Bitbucket Overview 📚 Introduction to Git and Bitbucket 🎥 Bitbucket Cloud Tutorial 📰 Great articles about Bitbucket 📌 Conclusion Choosing a source code repository hosting service depends on your development team\u0026rsquo;s needs. If you need a popular platform with a vast ecosystem, GitHub is a strong choice. If you want a fully integrated DevOps solution, GitLab would be more suitable. If you\u0026rsquo;re already using the Atlassian ecosystem, Bitbucket would be the best choice.\nConsider your project needs and desired level of integration to make the right decision! 🚀\n👉 Next step: Learn about Containers that help package applications along with all libraries, configurations, and dependencies to run consistently across different environments.\n","date":"2025-02-23T00:00:00Z","image":"https://tech.nguuyen.io.vn/images/devops/devops-step-five.webp","permalink":"https://tech.nguuyen.io.vn/en/posts/devops/devops-step-five/","title":"Source Code Hosting Services 🐱"},{"content":"🖥️ Terminal Knowledge Terminal is a text interface that allows users to interact with computer systems through CLI (Command Line Interface). It is an essential tool for system management, command execution, and task automation.\n📚 Free Resources 📄 Article: What is CLI? 🔍 Google Search ▶️ YouTube Search 🔹 Example:\n1 2 ls -l # List files in current directory pwd # Display current directory path 📊 Process Monitoring Process monitoring is the continuous observation and analysis of processes in IT systems to ensure performance, efficiency, and compliance. It helps track important metrics such as resource usage and behavior of individual processes or applications running in the system.\n🏆 Recommended Tools 🟣 lsof - Lists information about files opened by processes. 📚 Free Resources 📄 Lsof Cheat Sheet 📖 lsof Documentation ▶️ Video: Linux Crash Course - lsof Command 📝 Great Articles on Monitoring 🔹 Example:\n1 2 lsof -i :80 # List processes using port 80 ps aux # Display all running processes 🚀 Performance Monitoring Performance monitoring helps collect, analyze, and report key performance metrics from applications, networks, servers, and databases.\n🏆 Recommended Tools 🟣 vmstat - Virtual memory and system performance monitoring tool. 📚 Free Resources 📖 Linux Commands: Exploring Virtual Memory with vmstat 📄 vmstat Documentation ▶️ vmstat Tutorial 📝 Great Articles on Monitoring 🔹 Example:\n1 vmstat 5 10 # Update system status every 5 seconds for 10 times 🌐 Networking Tools Networking tools support monitoring, analyzing, troubleshooting, and managing network systems.\n🏆 Recommended Tools 🟣 Wireshark - Deep packet analysis. 🟣 Nmap - Network scanning and security testing. Ping - Basic connectivity testing. Traceroute - Determine packet path through network. Netstat - Display network connections. Tcpdump - Command-line packet capture and analysis. Iperf - Network performance testing. Netcat - Perform various network tasks. Nslookup/Dig - DNS queries. PuTTY - Remote connection via SSH or Telnet. 🔹 Example:\n1 2 ping google.com # Test connection to Google nmap -sS 192.168.1.1 # Scan internal server ports ✂️ Text Manipulation Tools that support editing, processing, and converting text data.\n🏆 Recommended Tools 🟣 sed - Stream editor for data manipulation. 🟣 awk - Pattern scanning and data extraction. 🟣 grep - Text search using regular expressions. cut, sort, tr, uniq - Supporting commands for text data processing. 🔹 Example:\n1 2 grep \u0026#34;error\u0026#34; logfile.txt # Find \u0026#34;error\u0026#34; in logfile.txt awk \u0026#39;{print $1}\u0026#39; data.txt # Get first column from data.txt ⚡ Bash Scripts Bash is a powerful shell on Unix/Linux that helps execute commands and automate tasks.\n🔹 Example:\n1 2 #!/bin/bash echo \u0026#34;Hello, World!\u0026#34; ✍️ Editors Text editors are essential tools for editing and managing text files.\n🏆 Recommended Tools 🟣 Vim - Powerful, highly customizable, suitable for programmers. 🟣 Emacs - Flexible with many supporting plugins. Sublime Text - High speed, user-friendly interface. Visual Studio Code - Open source, supports debugging, extensions, integrated development tools. 🔹 Example:\n1 2 vim myfile.txt # Open file with Vim nano myfile.txt # Open file with Nano 🔚 Conclusion Understanding and mastering these tools helps you work more efficiently in Linux and DevOps environments. Tools 🟣 marked are the most popular and powerful ones, recommended by many experts. You can learn more through the accompanying free resources. If there\u0026rsquo;s anything that needs clarification or addition, please provide feedback so I can update accordingly!\n👉 Next step: Advance your knowledge of Version Control Systems to effectively track, manage, and collaborate on source code.\n","date":"2025-02-23T00:00:00Z","image":"https://tech.nguuyen.io.vn/images/devops/devops-step-three.webp","permalink":"https://tech.nguuyen.io.vn/en/posts/devops/devops-step-three/","title":"Terminal Knowledge 📟"},{"content":"🔄 Version Control Systems Version control systems (VCS) are tools that help track changes to source code and files over time. They support team collaboration, manage change history, and maintain multiple versions of source code. There are two main types of VCS:\nCentralized VCS (CVCS): Uses a central repository, examples include Subversion (SVN), CVS. Distributed VCS (DVCS): Each user has a complete copy of the repository, including the entire history. The most popular example is Git. Git is a powerful distributed version control system that allows offline work, supports fast branching and merging operations, enhancing collaboration capabilities.\n🛠️ Git - The Most Popular Version Control Tool 🔹 Installing Git If you haven\u0026rsquo;t installed Git yet, you can download it from git-scm.com or use the following commands:\n1 2 3 sudo apt install git # Ubuntu/Debian yum install git # CentOS/RHEL brew install git # macOS Verify Git installation:\n1 2 3 4 git --version # output: # git version 2.47.1.windows.1 🚀 Basic Git Commands Below are common Git commands, organized from basic to advanced:\nInitialization \u0026amp; Configuration 1 git init # Initialize Git repository 1 2 git config --global user.name \u0026#34;Your Name\u0026#34; # Configure name git config --global user.email \u0026#34;email@example.com\u0026#34; # Configure email Working With Repository 1 git clone \u0026lt;repo_url\u0026gt; # Clone a remote repository to local machine 1 git status # Check file status Adding \u0026amp; Saving Changes 1 git add \u0026lt;file\u0026gt; # Add file to staging area 1 git commit -m \u0026#34;Change description\u0026#34; # Save changes to history Working With Remote Repository 1 git remote add origin \u0026lt;repo_url\u0026gt; # Link remote repository 1 git push -u origin main # Push changes to main branch 1 git pull origin main # Update latest changes from remote repository Working With Branches 1 git branch new-feature # Create new branch 1 git checkout new-feature # Switch to new branch 1 git merge new-feature # Merge branch into current branch Tracking History 1 git log # View commit history 1 git diff # Compare changes between versions 📚 Free Git Learning Resources 📖 Official Git Documentation 📄 Git Cheat Sheet ▶️ Git Tutorial Video for Beginners 📝 Article: What is Version Control System? 🔚 Conclusion Using Git makes source code management easier, supports effective team collaboration, and protects important project data. Understanding and mastering Git is an essential skill for every programmer.\n👉 Next step: Learn about GitHub \u0026amp; GitLab to manage Git repositories on cloud platforms.\n","date":"2025-02-23T00:00:00Z","image":"https://tech.nguuyen.io.vn/images/devops/devops-step-four.webp","permalink":"https://tech.nguuyen.io.vn/en/posts/devops/devops-step-four/","title":"Version Control Systems 🛠️"},{"content":"📌 Why is Linux important in DevOps? Linux is the foundation of most server systems, containers (Docker, Kubernetes), and cloud platforms. DevOps professionals need to master Linux to:\n✅ Manage servers effectively. ✅ Write automation scripts. ✅ Handle files, users, and processes. ✅ Optimize systems and security. ⚙️ What is an Operating System? An operating system (OS) is software that manages computer hardware and software resources, providing common services for programs. It acts as an intermediary between applications and hardware, handling tasks such as:\n🔹 Memory management. 🔹 Process scheduling. 🔹 File system management. 🔹 Device control. 🌍 Popular operating systems: 💻 Personal computers: Windows, macOS, Linux (Ubuntu, Fedora,\u0026hellip;) 📱 Mobile devices: iOS, Android 🖥️ Servers: Ubuntu Server, Red Hat Enterprise Linux, Windows Server Each operating system has different characteristics, interfaces, and compatibility capabilities. They play crucial roles in system security, performance optimization, and providing consistent user experiences.\n🛠️ Basic Linux Commands Here are some important Linux commands:\n🔍 System Information 1 2 3 uname -a # Display operating system information uptime # System uptime free -m # Check RAM memory 📂 File \u0026amp; Directory Management 1 2 3 ls -l # List files with detailed information mkdir mydir # Create new directory rm -rf mydir # Delete directory and its contents 🚀 Process Management 1 2 3 top # Display running processes ps aux # List all processes kill -9 PID # Stop process by PID 👤 User Management 1 2 3 whoami # View current user sudo useradd devops # Create new user sudo passwd devops # Set password for user 📜 Bash Script for System Resource Monitoring 1 2 3 4 5 6 7 8 9 10 #!/bin/bash echo \u0026#34;==== System Information ====\u0026#34; uname -a echo \u0026#34;==== System Uptime ====\u0026#34; uptime echo \u0026#34;==== RAM Memory ====\u0026#34; free -m ▶️ How to run the script: 1 2 chmod +x system_check.sh # Grant execute permission to script ./system_check.sh # Run script in terminal 📚 Learning Resources Here are some free resources to learn more about operating systems:\n📖 Operating Systems - Wiki 📖 All you need to know about OS 📖 Learn Operating Systems 🎥 What are Operating Systems? 🎥 Operating Systems 🎯 Conclusion ✅ Linux is a mandatory skill in DevOps. ✅ Learn to use terminal \u0026amp; Bash scripting. 👉 Next step: Learn more about terminal and CLI usage to work effectively with systems.\n","date":"2025-02-22T00:00:00Z","image":"https://tech.nguuyen.io.vn/images/devops/devops-step-two.webp","permalink":"https://tech.nguuyen.io.vn/en/posts/devops/devops-step-two/","title":"Learning Linux \u0026 Operating Systems 🖥️"},{"content":"📌 Why do you need to choose a programming language? In DevOps, you will need to use programming languages to:\n✅ Write automation scripts. ✅ Manage servers and cloud. ✅ Create tools to support CI/CD. ✅ Build and deploy Infrastructure as Code (IaC). Choosing the right language helps you work more efficiently with systems, automate many processes, and improve software development speed.\n🔥 Suitable languages for DevOps 🐍 Python (Main recommendation) 🔹 Reasons to choose Python: Easy-to-read syntax, easy to learn. Rich libraries supporting automation like fabric, paramiko, boto3 (AWS SDK), pyinfra. Strong support in Cloud management (AWS, GCP, Azure). 🔹 Practical applications: Write automated code deployment scripts. Create server management bots. Build system management APIs. 📝 Example: Automated SSH deployment script with Paramiko 1 2 3 4 5 6 7 8 9 10 11 import paramiko def deploy_code(host, user, password, command): client = paramiko.SSHClient() client.set_missing_host_key_policy(paramiko.AutoAddPolicy()) client.connect(hostname=host, username=user, password=password) stdin, stdout, stderr = client.exec_command(command) print(stdout.read().decode()) client.close() deploy_code(\u0026#39;192.168.1.100\u0026#39;, \u0026#39;ubuntu\u0026#39;, \u0026#39;yourpassword\u0026#39;, \u0026#39;git pull origin main \u0026amp;\u0026amp; systemctl restart app\u0026#39;) 🖥️ Bash (Need to know basics) 🔹 Reasons to choose Bash: The most popular shell script on Linux. Helps you work quickly with the system. Optimized for server management and small task automation. 🔹 Practical applications: Write automated server update scripts. Create periodic cron jobs. Manage users and permissions on Linux. 📝 Example: Automated server update script 1 2 #!/bin/bash sudo apt update \u0026amp;\u0026amp; sudo apt upgrade -y 🚀 Go (Golang) (If working with Kubernetes) 🔹 Reasons to choose Go: High performance, easy to compile into compact binaries. Kubernetes and many DevOps tools like Terraform are written in Go. 🔹 Practical applications: Write container management tools. Create plugins for Kubernetes. Build custom DevOps tools. 📝 Example: Display system information with Go 1 2 3 4 5 6 7 8 9 10 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; ) func main() { hostname, _ := os.Hostname() fmt.Println(\u0026#34;Hostname:\u0026#34;, hostname) } ⚙️ Groovy (If working with Jenkins) 🔹 Reasons to choose Groovy: The main language for writing pipelines in Jenkins. Flexible syntax, easy to extend and integrate with Java. 🔹 Practical applications: Write CI/CD pipelines for Jenkins. Create system management scripts. Automate build, test, deploy steps. 📝 Example: Basic pipeline in Jenkinsfile 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 pipeline { agent any stages { stage(\u0026#39;Build\u0026#39;) { steps { echo \u0026#39;Building the project...\u0026#39; sh \u0026#39;mvn clean package\u0026#39; } } stage(\u0026#39;Test\u0026#39;) { steps { echo \u0026#39;Running tests...\u0026#39; sh \u0026#39;mvn test\u0026#39; } } stage(\u0026#39;Deploy\u0026#39;) { steps { echo \u0026#39;Deploying application...\u0026#39; sh \u0026#39;./deploy.sh\u0026#39; } } } } 🎯 Conclusion ✅ Python + Bash is the best choice to start with DevOps. ✅ If working with Kubernetes, learn Go as well. ✅ If working with Jenkins, learn Groovy to write pipelines. 👉 Next step: Learn the basics of Linux \u0026amp; operating systems.\n","date":"2025-02-21T00:00:00Z","image":"https://tech.nguuyen.io.vn/images/devops/devops-step-one.webp","permalink":"https://tech.nguuyen.io.vn/en/posts/devops/devops-step-one/","title":"Choosing a Programming Language 🤗"},{"content":"Understanding DevOps DevOps is the combination of software development and system operations, aimed at enhancing collaboration and automation in the software development and deployment process.\nLearn a Programming Language Mastering at least one programming language is essential for effective automation and system management.\nPopular Languages\nPython Go Ruby Master Operating System Knowledge Linux: Popular operating system in server environments. Windows: Important in enterprises using Microsoft infrastructure. Learn About Computer Networks and Security Key topics to focus on:\nNetwork Protocols: HTTP, HTTPS, FTP, TCP/IP. Network Security: Firewalls, VPN, SSL/TLS. Use Source Code Management Tools Effective source code management is a crucial element in DevOps.\nGit: Popular version control system. Understand Configuration Management and Infrastructure as Code Configuration automation helps maintain consistency and efficiency.\nAnsible: Automation tool. Terraform: Infrastructure management by defining it in source code. Master Containerization and Orchestration Docker: Popular container platform. Kubernetes: Powerful container orchestration system. Set Up and Manage CI/CD Jenkins: Open-source automation server. GitLab CI/CD: Effective CI/CD support. Monitoring and Logging Prometheus: Monitoring system. ELK stack: Log analysis toolkit. Learn About Cloud Services Popular providers:\nAWS Google Cloud Microsoft Azure Conclusion Becoming a DevOps engineer requires broad knowledge and deep practical skills. Keep learning and practicing continuously to achieve your goals.\nNote: This roadmap is compiled from multiple sources and real-world experience, aiming to provide you with the most comprehensive and detailed view of the path to becoming a DevOps engineer. 🎯🚀\nDevOps Roadmap 2025 ","date":"2025-02-20T00:00:00Z","image":"https://tech.nguuyen.io.vn/images/devops/devops-roadmap.webp","permalink":"https://tech.nguuyen.io.vn/en/posts/devops/devops-roadmap/","title":"DevOps Engineer Roadmap 😎"},{"content":"Introduction to DevOps DevOps is a methodology that combines software development (Development - Dev) and system operations (Operations - Ops) to optimize the process of developing, deploying, and operating applications. DevOps helps development and operations teams work together more effectively through tools, automated processes, and collaborative culture.\nWhy is DevOps Important? Accelerated Development Speed DevOps helps automate processes like testing, deployment, and monitoring, reducing time-to-market for products.\nImproved Product Quality Integration of automated testing and CI/CD helps detect bugs early, minimizing risks during software deployment.\nEnhanced Reliability Monitoring and logging tools help quickly identify issues, reduce downtime, and ensure systems operate stably.\nBetter Team Collaboration DevOps breaks down barriers between development and operations teams, creating a more effective collaborative work environment.\nKey Components of DevOps CI/CD (Continuous Integration \u0026amp; Continuous Deployment) CI/CD automates the process of source code integration, testing, and deployment, minimizing errors when releasing products to production environments.\nInfrastructure as Code (IaC) IaC allows infrastructure management as code, making it easy to deploy and scale systems.\nMonitoring and Logging Tools like Prometheus, Grafana, ELK Stack help monitor and analyze logs to quickly handle incidents.\nContainerization and Orchestration Docker and Kubernetes help package, manage, and scale applications flexibly.\nPopular DevOps Tools CI/CD: Jenkins, GitHub Actions, GitLab CI/CD IaC: Terraform, Ansible, CloudFormation Monitoring: Prometheus, Grafana, ELK Stack Container \u0026amp; Orchestration: Docker, Kubernetes Source Code Management: Git, GitHub, GitLab Conclusion DevOps is an important methodology that helps improve development speed, product quality, and optimize system operations. In the upcoming articles of this series, we will explore each aspect of DevOps in depth, from CI/CD, Infrastructure as Code to system monitoring.\nDocker Example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Use Node.js as the base FROM node:18 # Set working directory in container WORKDIR /app # Copy package.json and install dependencies COPY package.json . RUN npm install # Copy entire source code into container COPY . . # Expose port 3000 for the application EXPOSE 3000 # Command to run the application CMD [\u0026#34;npm\u0026#34;, \u0026#34;start\u0026#34;] 💡 Follow the blog to stay updated with the latest DevOps articles!\n","date":"2025-02-19T00:00:00Z","image":"https://tech.nguuyen.io.vn/images/devops/devops-intro.webp","permalink":"https://tech.nguuyen.io.vn/en/posts/devops/devops-intro/","title":"What is DevOps?"}]