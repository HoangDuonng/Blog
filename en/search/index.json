[{"content":"üê≥ Introduction to Docker Docker is a popular platform in DevOps for deploying applications using containers. Containers allow you to package applications together with all their dependencies and necessary configurations, ensuring consistent execution across all environments from development to production.\nBenefits of Docker:\n‚ö° Consistency: Applications run identically across all environments üöÄ Speed: Containers start faster than virtual machines üì¶ Packaging: Easy to share and deploy applications üîÑ Scalability: Easy to scale and manage multiple containers This guide walks you through installing Docker on Ubuntu 20.04 and 22.04 in a straightforward and efficient manner, using Docker\u0026rsquo;s official repository to ensure you always have the latest stable version.\n‚úÖ Prerequisites Before you begin, make sure you have:\nüñ•Ô∏è Ubuntu 20.04 or 22.04 server installed üîê Sudo privileges to execute administrative commands üåê Internet connection to download packages ‚è±Ô∏è Time: Installation takes approximately 5-10 minutes 1. Update the System Before installing Docker, ensure your system is fully updated. This guarantees you have the latest security patches and necessary dependencies.\n1 sudo apt update \u0026amp;\u0026amp; sudo apt upgrade -y üí° Notes:\nThe apt update command updates the list of available packages from repositories The apt upgrade -y command automatically installs updates without prompting This process may take several minutes depending on the number of packages to update 2. Install Docker 2.1. Install Required Dependencies These packages are necessary for Docker to download and authenticate packages from the official repository:\napt-transport-https: Allows APT to use HTTPS protocol ca-certificates: Provides SSL/TLS certificates for secure connection authentication curl: Tool to download files from the internet software-properties-common: Provides utilities for managing repositories 1 sudo apt install -y apt-transport-https ca-certificates curl software-properties-common 2.2. Add the Official Docker Repository To install Docker from the official source, we need to add Docker\u0026rsquo;s GPG key and repository to the system. This ensures you receive official and security updates from Docker.\nStep 1: Add Docker\u0026rsquo;s GPG Key\nThe GPG key is used to authenticate the integrity of Docker packages:\n1 curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg Step 2: Add Docker Repository\nAdd Docker repository to APT\u0026rsquo;s sources list. This command automatically detects your Ubuntu version ($(lsb_release -cs)) and adds the appropriate repository:\n1 echo \u0026#34;deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\u0026#34; | sudo tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null üí° Explanation:\narch=amd64: Specifies 64-bit architecture signed-by: Specifies the GPG key for authentication $(lsb_release -cs): Automatically retrieves Ubuntu\u0026rsquo;s code name (e.g., jammy for Ubuntu 22.04) stable: Uses the stable channel (you can use test or nightly for experimental versions) 2.3. Install Docker Engine After adding the repository, update the package list and install Docker Engine:\n1 2 sudo apt update sudo apt install -y docker-ce docker-ce-cli containerd.io üì¶ Packages Being Installed:\ndocker-ce: Docker Community Edition - the open-source version of Docker docker-ce-cli: Docker Command Line Interface - command-line tool to interact with Docker containerd.io: Container runtime - low-level tool for managing container lifecycle ‚è±Ô∏è Installation Time: This process typically takes 2-5 minutes depending on your internet speed.\n3. Verify Docker After installation, verify that Docker is running properly:\n1 sudo systemctl status docker Expected Result: You should see the status active (running) if Docker started successfully.\nIf Docker isn\u0026rsquo;t running, you can start it and enable auto-start on boot:\n1 2 sudo systemctl start docker sudo systemctl enable docker üí° Command Explanations:\nsystemctl start docker: Starts the Docker service immediately systemctl enable docker: Configures Docker to auto-start on system boot 4. Test Docker To confirm Docker is working correctly, run the sample hello-world container:\n1 sudo docker run hello-world Expected Result: If you see the \u0026ldquo;Hello from Docker!\u0026rdquo; message along with Docker information, congratulations, installation successful! üéâ\nüîç What does the hello-world container do?\nDownloads the hello-world image from Docker Hub (if not already present) Runs the container and displays a welcome message Automatically stops after completion 5. Configure User Permissions By default, Docker requires root privileges to run commands. To run Docker without sudo, add your current user to the docker group:\n1 2 sudo usermod -aG docker $USER newgrp docker üí° Explanation:\nusermod -aG docker $USER: Adds the current user to the docker group (which has Docker daemon access privileges) newgrp docker: Applies group changes immediately without needing to log out ‚ö†Ô∏è Important Notes:\nAfter executing this command, you may need to log out and log back in for changes to take effect Alternatively, you can open a new terminal window to apply changes Adding a user to the docker group grants equivalent root privileges on the Docker daemon, so only add trusted users Verify New Permissions:\nAfter configuration, try running Docker without sudo:\n1 docker run hello-world If the command succeeds without sudo, you\u0026rsquo;ve configured it correctly! ‚úÖ\n6. Uninstall Docker (if needed) If you want to completely remove Docker from your system, follow these steps:\nStep 1: Remove Docker Packages\n1 sudo apt remove -y docker-ce docker-ce-cli containerd.io Step 2: Delete Docker Data\nDocker data (images, containers, volumes, networks) is stored in /var/lib/docker. To remove completely:\n1 sudo rm -rf /var/lib/docker ‚ö†Ô∏è Warning: This command will delete ALL Docker data including:\nAll downloaded images All containers (running and stopped) All volumes and networks All data within containers Step 3: Remove Docker Configuration (optional)\nTo completely remove all traces of Docker:\n1 2 sudo rm -rf /var/lib/containerd sudo rm -rf /etc/docker üìö Basic Docker Commands After installation, here are some basic commands you should know:\nCommand Description docker --version Check Docker version docker ps List running containers docker ps -a List all containers (including stopped) docker images List all images docker pull \u0026lt;image\u0026gt; Download image from Docker Hub docker run \u0026lt;image\u0026gt; Run container from image docker stop \u0026lt;container\u0026gt; Stop running container docker rm \u0026lt;container\u0026gt; Remove container docker rmi \u0026lt;image\u0026gt; Remove image üîß Troubleshooting Issue: Docker daemon won\u0026rsquo;t start Cause: May be due to conflicts with other services or misconfiguration.\nSolution:\n1 2 3 4 5 # Check Docker logs sudo journalctl -u docker # Restart Docker sudo systemctl restart docker Issue: Permission denied when running Docker Cause: User hasn\u0026rsquo;t been added to the docker group.\nSolution:\n1 2 3 4 5 # Add user to docker group sudo usermod -aG docker $USER # Log out and log back in, or run: newgrp docker Issue: Cannot connect to Docker daemon Cause: Docker daemon hasn\u0026rsquo;t been started.\nSolution:\n1 2 3 4 5 # Check status sudo systemctl status docker # Start Docker sudo systemctl start docker üéØ Conclusion Congratulations on completing Docker installation on Ubuntu! üéâ Now you can:\nüê≥ Create and run containers for your applications üì¶ Use Docker images available from Docker Hub üöÄ Deploy applications consistently and easily üîÑ Manage development and production environments more efficiently Docker is a powerful tool in the DevOps world. Continue exploring Docker features like Docker Compose, Docker Swarm, and CI/CD integration to optimize your development workflow!\nüìñ References:\nOfficial Docker Documentation Docker Hub - The largest image repository Docker Best Practices Good luck with Docker! üöÄüòä\n","date":"2025-11-30T00:00:00Z","image":"https://tech.nguuyen.io.vn/images/installation-guides/docker-installation-ubuntu.webp","permalink":"https://tech.nguuyen.io.vn/en/posts/installation-guides/setup-docker-on-ubuntu/","title":"How to Install Docker on Ubuntu üê≥"},{"content":"Dockerfile Contest 2025 ‚Äì Extreme Java Optimization Dockerfile Contest 2025 encourages the Vietnamese DevOps community to rethink how Dockerfiles are written to achieve security, optimization, and clarity. This article focuses on the Java Spring Boot category, where authors optimize the JRE, reduce image size, and standardize build workflows.\nI. JAVA Category (Spring Boot Service) The Java category focuses on:\nJRE optimization: use jlink and jdeps to build a custom JRE that only includes required modules. Security \u0026amp; dependency optimization: automatically update dependencies to safer versions and reduce CVEs. Multi-stage build: separate build and runtime stages, use distroless or optimized base images. Clear healthchecks: use wget or native Java healthchecks to monitor containers. 1. Dockerfile TOP 1 (Java) ‚Äì Spring Boot Template + Distroless Technique Author\u0026rsquo;s explanation Reference Custom JRE with jlink Use jdeps to analyze the fat JAR for modules, then jlink builds a JRE with only needed modules, significantly reducing runtime size. Clear multi-stage build Build stage uses eclipse-temurin:21-jdk for Gradle; runtime uses gcr.io/distroless/base-debian12 to run only the JRE. Separate Healthcheck Stage Create a healthcheck stage with BusyBox, copy wget into runtime to avoid installing curl/wget via a package manager. Distroless Runtime Use distroless base to reduce attack surface (no shell, no package manager), smaller and safer image. Pin source \u0026amp; license Add LABELs org.opencontainers.image.source, version, licenses for traceability and license compliance. Dockerfile TOP 1 (JAVA)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 # Build stage FROM eclipse-temurin:21-jdk AS build WORKDIR /app # Copy gradle wrapper and properties first for better caching COPY gradlew gradlew.bat build.gradle ./ COPY gradle/ gradle/ # Download Gradle distribution (cached) RUN --mount=type=cache,target=/root/.gradle ./gradlew --version # Copy source code COPY src/ src/ # Build the application RUN --mount=type=cache,target=/root/.gradle ./gradlew --no-daemon clean bootJar \\ -Dspring-framework.version=6.2.11 \\ -Dtomcat.version=10.1.47 # Extract the application dependencies RUN jar xf build/libs/spring-boot-template.jar # Analyze the dependencies contained into the fat jar RUN jdeps --ignore-missing-deps -q \\ --recursive \\ --multi-release 21 \\ --print-module-deps \\ --class-path \u0026#39;BOOT-INF/lib/*\u0026#39; \\ build/libs/spring-boot-template.jar \u0026gt; deps.info # Create the custom JRE RUN jlink \\ --verbose \\ --add-modules $(cat deps.info) \\ --compress zip-9 \\ --no-header-files \\ --no-man-pages \\ --output /custom_jre # Healthcheck stage FROM busybox:1.36.0-musl AS healthcheck # Runtime stage FROM gcr.io/distroless/base-debian12 ENV JAVA_HOME=/opt/java/openjdk ENV PATH=\u0026#34;$JAVA_HOME/bin:$PATH\u0026#34; COPY --from=build /custom_jre $JAVA_HOME # Copy wget for healthcheck COPY --from=healthcheck /bin/wget /usr/bin/wget WORKDIR /app # Copy application insights config COPY lib/applicationinsights.json ./ # Copy the built JAR COPY --from=build /app/build/libs/spring-boot-template.jar /app.jar # Add labels LABEL org.opencontainers.image.source=\u0026#34;https://github.com/hmcts/spring-boot-template\u0026#34; \\ org.opencontainers.image.version=\u0026#34;0.0.1\u0026#34; \\ org.opencontainers.image.licenses=\u0026#34;MIT\u0026#34; EXPOSE 8080 HEALTHCHECK --interval=30s --timeout=3s --start-period=10s --retries=3 \\ CMD [\u0026#34;/usr/bin/wget\u0026#34;, \u0026#34;--quiet\u0026#34;, \u0026#34;--output-document=/dev/null\u0026#34;, \u0026#34;http://localhost:8080/health\u0026#34;] CMD [\u0026#34;java\u0026#34;, \u0026#34;-jar\u0026#34;, \u0026#34;/app.jar\u0026#34;] 2. Dockerfile TOP 2 (Java) ‚Äì Gradle Auto Dependency Update + Custom Healthcheck Technique Author\u0026rsquo;s explanation Reference Gradle Dependency Updates Use the dependencyUpdates plugin to generate a report, then parse it to auto-update plugin/ext/dependency versions in build.gradle. Force Dependency Upgrade DEPENDENCIES_FORCE_UPDATE lets you specify group:name:version to force upgrades of critical dependencies, usually CVE-related libs. Native Java Healthcheck HealthCheck.java uses HttpURLConnection to call /health, avoiding curl/wget dependencies. Distroless Base Java Runtime uses hmctspublic.azurecr.io/base/java:21-distroless, optimized Java 21 image for production. Gradle Cache Mount cache /root/.gradle to speed up ./gradlew in the build stage. Dockerfile TOP 2 (JAVA)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 # Stage 1 ‚Äî Build application using Gradle FROM gradle:8.14.3-jdk21-alpine AS builder WORKDIR /app # Caching wrapper and build configuration before build COPY gradlew ./ COPY gradle gradle COPY build.gradle build.gradle # Caching gradle/download RUN ./gradlew --no-daemon help # Copy src code COPY src/main src/main # Check dependency need to update RUN ./gradlew --no-daemon dependencyUpdates -Drevision=release # Auto update for auto vulnerability fixing RUN REPORT_FILE=\u0026#34;build/dependencyUpdates/report.txt\u0026#34; \u0026amp;\u0026amp; \\ echo \u0026#34;=== Parsing $REPORT_FILE ===\u0026#34; \u0026amp;\u0026amp; \\ \\ PLUGINS_TO_UPGRADE=${PLUGINS_TO_UPGRADE:-\u0026#34;org.springframework.boot org.sonarqube com.github.ben-manes.versions uk.gov.hmcts.java\u0026#34;} \u0026amp;\u0026amp; \\ EXTS_TO_UPGRADE=${EXTS_TO_UPGRADE:-\u0026#34;org.apache.logging.log4j ch.qos.logback\u0026#34;} \u0026amp;\u0026amp; \\ DEPENDENCIES_FORCE_UPDATE=${DEPENDENCIES_FORCE_UPDATE:-\u0026#34;org.apache.commons:commons-lang3:3.19.0\u0026#34;} \u0026amp;\u0026amp; \\ \\ escape_sed() { printf \u0026#39;%s\\n\u0026#39; \u0026#34;$1\u0026#34; | sed \u0026#39;s/[.[\\*^$/\u0026amp;]/\\\\\u0026amp;/g\u0026#39;; } \u0026amp;\u0026amp; \\ \\ # --- Plugin updates --- for plugin in $PLUGINS_TO_UPGRADE; do \\ LINE=$(grep -A1 \u0026#34;$plugin\u0026#34; \u0026#34;$REPORT_FILE\u0026#34; | grep \u0026#39;\\[\u0026#39; | head -1 || true); \\ OLD_VERSION=$(echo \u0026#34;$LINE\u0026#34; | sed -E \u0026#39;s/.*\\[(.*) -\u0026gt; .*\\].*/\\1/\u0026#39; || true); \\ NEW_VERSION=$(echo \u0026#34;$LINE\u0026#34; | sed -E \u0026#39;s/.*\\[.* -\u0026gt; (.*)\\].*/\\1/\u0026#39; || true); \\ if [ -n \u0026#34;$NEW_VERSION\u0026#34; ] \u0026amp;\u0026amp; [ \u0026#34;$NEW_VERSION\u0026#34; != \u0026#34;$OLD_VERSION\u0026#34; ]; then \\ echo \u0026#34;===== Upgrading plugin $plugin: $OLD_VERSION ‚Üí $NEW_VERSION\u0026#34;; \\ ESC_OLD=$(escape_sed \u0026#34;$OLD_VERSION\u0026#34;); \\ ESC_NEW=$(escape_sed \u0026#34;$NEW_VERSION\u0026#34;); \\ sed -i \u0026#34;s#id \u0026#39;$plugin\u0026#39; version \u0026#39;$ESC_OLD\u0026#39;#id \u0026#39;$plugin\u0026#39; version \u0026#39;$ESC_NEW\u0026#39;#g\u0026#34; build.gradle; \\ fi; \\ done \u0026amp;\u0026amp; \\ \\ # --- ext{} version updates --- for prefix in $EXTS_TO_UPGRADE; do \\ LINE=$(grep -A1 \u0026#34;$prefix\u0026#34; \u0026#34;$REPORT_FILE\u0026#34; | grep \u0026#39;\\[\u0026#39; | head -1 || true); \\ OLD_VERSION=$(echo \u0026#34;$LINE\u0026#34; | sed -E \u0026#39;s/.*\\[(.*) -\u0026gt; .*\\].*/\\1/\u0026#39; || true); \\ NEW_VERSION=$(echo \u0026#34;$LINE\u0026#34; | sed -E \u0026#39;s/.*\\[.* -\u0026gt; (.*)\\].*/\\1/\u0026#39; || true); \\ if [ -n \u0026#34;$NEW_VERSION\u0026#34; ] \u0026amp;\u0026amp; [ \u0026#34;$NEW_VERSION\u0026#34; != \u0026#34;$OLD_VERSION\u0026#34; ]; then \\ echo \u0026#34;===== Upgrading prefix $prefix: $OLD_VERSION ‚Üí $NEW_VERSION\u0026#34;; \\ ESC_OLD=$(escape_sed \u0026#34;$OLD_VERSION\u0026#34;); \\ ESC_NEW=$(escape_sed \u0026#34;$NEW_VERSION\u0026#34;); \\ sed -i \u0026#34;s/\\\u0026#34;$ESC_OLD\\\u0026#34;/\\\u0026#34;$ESC_NEW\\\u0026#34;/g\u0026#34; build.gradle; \\ fi; \\ done \u0026amp;\u0026amp; \\ \\ # --- Force dependency updates with explicit GAV --- for dep in $DEPENDENCIES_FORCE_UPDATE; do \\ GROUP=$(echo \u0026#34;$dep\u0026#34; | cut -d\u0026#39;:\u0026#39; -f1); \\ NAME=$(echo \u0026#34;$dep\u0026#34; | cut -d\u0026#39;:\u0026#39; -f2); \\ NEW_VERSION=$(echo \u0026#34;$dep\u0026#34; | cut -d\u0026#39;:\u0026#39; -f3); \\ if [ -z \u0026#34;$GROUP\u0026#34; ] || [ -z \u0026#34;$NAME\u0026#34; ] || [ -z \u0026#34;$NEW_VERSION\u0026#34; ]; then \\ echo \u0026#34;====== Invalid DEPENDENCIES_FORCE_UPDATE format for $dep, expected group:name:version\u0026#34;; \\ continue; \\ fi; \\ echo \u0026#34;====== Forcing dependency update: $GROUP:$NAME ‚Üí $NEW_VERSION\u0026#34;; \\ ESC_GROUP=$(escape_sed \u0026#34;$GROUP\u0026#34;); \\ ESC_NAME=$(escape_sed \u0026#34;$NAME\u0026#34;); \\ ESC_NEW=$(escape_sed \u0026#34;$NEW_VERSION\u0026#34;); \\ if grep -q \u0026#34;$ESC_GROUP\u0026#34; build.gradle | grep -q \u0026#34;$ESC_NAME\u0026#34;; then \\ # Replace existing dependency version sed -i \u0026#34;s#group: \u0026#39;$ESC_GROUP\u0026#39;, name: \u0026#39;$ESC_NAME\u0026#39;, version: \u0026#39;[^\u0026#39;]*\u0026#39;#group: \u0026#39;$ESC_GROUP\u0026#39;, name: \u0026#39;$ESC_NAME\u0026#39;, version: \u0026#39;$ESC_NEW\u0026#39;#g\u0026#34; build.gradle; \\ else \\ # Insert new dependency inside dependencies { } echo \u0026#34;====== Adding new dependency $GROUP:$NAME:$NEW_VERSION\u0026#34;; \\ sed -i \u0026#34;/dependencies\\s*{/a\\ implementation group: \u0026#39;$GROUP\u0026#39;, name: \u0026#39;$NAME\u0026#39;, version: \u0026#39;$NEW_VERSION\u0026#39;\u0026#34; build.gradle; \\ fi; \\ done \u0026amp;\u0026amp; \\ \\ echo \u0026#34;Version upgrade complete!\u0026#34; \u0026amp;\u0026amp; \\ cat build.gradle # Build the application JAR after dependency check RUN ./gradlew --no-daemon bootJar # Generate java healthcheck class RUN mkdir -p /app/health \u0026amp;\u0026amp; cat \u0026gt; /app/health/HealthCheck.java \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; import java.net.HttpURLConnection; import java.net.URL; import java.time.Instant; public class HealthCheck { public static void main(String[] args) { String healthUrl = \u0026#34;http://localhost:8080/health\u0026#34;; try { URL url = new URL(healthUrl); HttpURLConnection conn = (HttpURLConnection) url.openConnection(); conn.setConnectTimeout(2000); conn.setReadTimeout(2000); conn.setRequestMethod(\u0026#34;GET\u0026#34;); int code = conn.getResponseCode(); if (code == 200) { System.out.println(Instant.now() + \u0026#34;Healthcheck OK (\u0026#34; + code + \u0026#34;)\u0026#34;); System.exit(0); } else { System.err.println(Instant.now() + \u0026#34;Healthcheck failed (\u0026#34; + code + \u0026#34;)\u0026#34;); System.exit(1); } } catch (Exception e) { System.err.println(Instant.now() + \u0026#34; Healthcheck error: \u0026#34; + e.getMessage()); System.exit(1); } } } EOF # Compile HealthCheck.java file RUN javac /app/health/HealthCheck.java # Stage 2 ‚Äî Runtime image (auto-updated base) FROM hmctspublic.azurecr.io/base/java:21-distroless WORKDIR /app # Copy compiled app COPY --from=builder /app/build/libs/*.jar app.jar # Copy complied healthcheck class COPY --from=builder /app/health/HealthCheck.class /app/HealthCheck.class EXPOSE 8080 ENTRYPOINT [\u0026#34;java\u0026#34;, \u0026#34;-jar\u0026#34;, \u0026#34;app.jar\u0026#34;] HEALTHCHECK --interval=15s --timeout=5s --start-period=10s --retries=3 \\ CMD [\u0026#34;java\u0026#34;, \u0026#34;HealthCheck\u0026#34;] 3. Dockerfile TOP (Java) ‚Äì Dung Cao (Optimized Alpine JDK/JRE) Technique Author\u0026rsquo;s explanation Reference Separate JDK/JRE via ARG Use ARG BUILD_JDK_IMAGE and RUNTIME_IMAGE to switch base images (JDK for build, JRE for runtime) while keeping the Dockerfile simple. Gradle cache with BuildKit Use --mount=type=cache,target=/cache/.gradle for ./gradlew to speed up repeat builds. Skip tests in build image bootJar -x test speeds builds in CI/CD and contest context. Non-root user + chown Create user/group app, use --chown=app:app when copying JAR to keep runtime secure and CIS-compliant. Healthcheck with curl Install curl and use curl -fsS to /health for clear, debuggable healthchecks. JVM tuning for containers JAVA_OPTS enables UseContainerSupport, MaxRAMPercentage=75, G1GC, ExitOnOutOfMemoryError, heap dump path, helping JVM respect container limits and fail fast on OOM. Dockerfile TOP (JAVA) ‚Äì Dung Cao\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 # Dockerfile.txt # === Alpine build optimized for Dockerfile Contest 2025 === # Focus: security, lightweight, clear, maintainable # Build-time arguments ARG BUILD_JDK_IMAGE=eclipse-temurin:21-jdk-alpine-3.22 ARG RUNTIME_IMAGE=eclipse-temurin:21-jre-alpine-3.22 # ---------- Stage: builder ---------- FROM ${BUILD_JDK_IMAGE} AS builder # Set non-interactive environment \u0026amp; reproducible timezone ENV TZ=UTC \\ LANG=C.UTF-8 \\ LC_ALL=C.UTF-8 \\ GRADLE_USER_HOME=/cache/.gradle WORKDIR /workspace # Copy Gradle wrapper and descriptors COPY gradlew . COPY gradle/ gradle/ COPY build.gradle ./ RUN chmod +x ./gradlew # Resolve dependencies using BuildKit cache mount RUN --mount=type=cache,target=/cache/.gradle \\ ./gradlew --no-daemon dependencies || true # Copy application source COPY src/ src/ RUN --mount=type=cache,target=/cache/.gradle \\ ./gradlew --no-daemon clean bootJar -x test \\ -Dspring-framework.version=6.2.11 \\ -Dcommons-lang3.version=3.18.0 \\ -Dtomcat.version=10.1.47 # ---------- Stage: runtime ---------- FROM ${RUNTIME_IMAGE} AS runtime ARG VERSION ARG VCS_REF ARG BUILD_DATE ARG LICENSE=\u0026#34;MIT License\u0026#34; ARG SOURCE=\u0026#34;contest-submission\u0026#34; # OCI Labels (metadata) LABEL org.opencontainers.image.title=\u0026#34;spring-boot-template\u0026#34; \\ org.opencontainers.image.description=\u0026#34;Spring Boot Java application built for Dockerfile Contest 2025\u0026#34; \\ org.opencontainers.image.url=\u0026#34;${SOURCE}\u0026#34; \\ org.opencontainers.image.source=\u0026#34;${SOURCE}\u0026#34; \\ org.opencontainers.image.version=\u0026#34;${VERSION}\u0026#34; \\ org.opencontainers.image.revision=\u0026#34;${VCS_REF}\u0026#34; \\ org.opencontainers.image.licenses=\u0026#34;${LICENSE}\u0026#34; \\ org.opencontainers.image.created=\u0026#34;${BUILD_DATE}\u0026#34; \\ org.opencontainers.image.authors=\u0026#34;Dung Cao\u0026#34; # Create non-root user RUN addgroup -S app \u0026amp;\u0026amp; adduser -S app -G app WORKDIR /app # Copy application jar COPY lib/applicationinsights.json applicationinsights.json COPY --from=builder --chown=app:app /workspace/build/libs/*.jar app.jar # Install curl for healthcheck RUN apk add --no-cache curl \\ \u0026amp;\u0026amp; rm -rf /var/cache/apk/* # Expose HTTP port EXPOSE 8080 # Healthcheck HEALTHCHECK --interval=10s --timeout=3s --start-period=10s --retries=3 \\ CMD curl -fsS http://127.0.0.1:8080/health || exit 1 # JVM optimization ENV JAVA_OPTS=\u0026#34;\\ -XX:+UseContainerSupport \\ -XX:MaxRAMPercentage=75.0 \\ -Djava.security.egd=file:/dev/./urandom \\ -Dserver.shutdown=graceful \\ -Dspring.lifecycle.timeout-per-shutdown-phase=10s \\ -Dfile.encoding=UTF-8 \\ -XX:+ExitOnOutOfMemoryError \\ -XX:+UseG1GC \\ -XX:+HeapDumpOnOutOfMemoryError \\ -XX:HeapDumpPath=/tmp \\ \u0026#34; # Graceful termination signal STOPSIGNAL SIGTERM # Switch to non-root user USER app # --- Entry point --- ENTRYPOINT [\u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;exec java ${JAVA_OPTS} -jar /app/app.jar\u0026#34;] 4. Dockerfile TOP (Reference ‚Äì Java) ‚Äì HMCTS Spring Boot Template (Layered + jlink) Technique Author\u0026rsquo;s explanation Reference Gradle cache + layer split Use BuildKit cache id=gradle-cache for Gradle, then jarmode=layertools to extract layers dependencies, spring-boot-loader, snapshot-dependencies, application for maximum layer caching. Custom JRE with jlink Run jlink with selected Java modules to create a minimal JRE (/jre-minimal), reducing \u0026gt;100MB compared to full JDK. Minimal Alpine runtime Runtime base alpine:3.21 installs only ca-certificates, tzdata, tini, curl, then runs as non-root appuser. JAVA_TOOL_OPTIONS for containers Optimize JVM: UseContainerSupport, MaxRAMPercentage, UseG1GC, UseStringDeduplication, ExitOnOutOfMemoryError, \u0026hellip; to optimize memory and GC in containers. Very complete OCI labels Include vendor, authors, source, version, revision, base.name, base.digest, com.hmcts.* for traceability. Healthcheck using curl Healthcheck /health via curl -f, with reasonable retries for Spring Boot startup. Dockerfile TOP (Reference ‚Äì JAVA) ‚Äì HMCTS Spring Boot Template\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 # syntax=docker/dockerfile:1.7 # ============================================================================ # STAGE 1: Application builder with optimized caching # ============================================================================ FROM eclipse-temurin:21-jdk-alpine@sha256:89517925fa675c6c4b770bee7c44d38a7763212741b0d6fca5a5103caab21a97 AS builder # Install build dependencies (minimal) RUN apk add --no-cache binutils \u0026amp;\u0026amp; \\ rm -rf /var/cache/apk/* WORKDIR /build # Copy Gradle wrapper and dependency definition files first # This layer will be cached until these files change COPY gradle/ gradle/ COPY gradlew build.gradle ./ # Download dependencies with BuildKit cache mount for faster subsequent builds RUN --mount=type=cache,id=gradle-cache,target=/root/.gradle,sharing=locked \\ chmod +x gradlew \u0026amp;\u0026amp; \\ ./gradlew dependencies --no-daemon --parallel --console=plain # Copy only production source code (exclude tests, docs, etc.) COPY src/main/ src/main/ # Build optimized JAR with cache mount RUN --mount=type=cache,id=gradle-cache,target=/root/.gradle,sharing=locked \\ ./gradlew bootJar --no-daemon --parallel --console=plain -x test \u0026amp;\u0026amp; \\ mkdir -p /app \u0026amp;\u0026amp; \\ mv build/libs/spring-boot-template.jar /app/app.jar # Extract Spring Boot layers for optimal Docker layer caching WORKDIR /app RUN java -Djarmode=layertools -jar app.jar extract --destination /app/extracted # Create minimal custom JRE with jlink (reduces size by \u0026gt;100MB) # Only include Java modules actually needed by Spring Boot RUN $JAVA_HOME/bin/jlink \\ --add-modules java.base,java.compiler,java.desktop,java.instrument,java.management,java.management.rmi,java.naming,java.net.http,java.prefs,java.rmi,java.scripting,java.security.jgss,java.security.sasl,java.sql,jdk.httpserver,jdk.jfr,jdk.unsupported \\ --strip-debug \\ --no-man-pages \\ --no-header-files \\ --compress=zip-9 \\ --output /jre-minimal # ============================================================================ # STAGE 3: Minimal runtime image # ============================================================================ FROM alpine:3.21@sha256:5405e8f36ce1878720f71217d664aa3dea32e5e5df11acbf07fc78ef5661465b # Install only critical runtime dependencies # ca-certificates: for HTTPS connections # tini: proper init system for PID 1 # tzdata: timezone support # curl: for healthcheck RUN apk upgrade --no-cache \u0026amp;\u0026amp; \\ apk add --no-cache \\ ca-certificates \\ tzdata \\ tini \\ curl \u0026amp;\u0026amp; \\ rm -rf /var/cache/apk/* /tmp/* # Create non-root user for security (CIS Docker Benchmark compliance) RUN addgroup -g 1654 -S appgroup \u0026amp;\u0026amp; \\ adduser -u 1654 -S appuser -G appgroup # Copy minimal custom JRE from builder COPY --from=builder --chown=1654:1654 /jre-minimal /opt/java # Set up application directory with proper ownership WORKDIR /app # Copy Spring Boot layers in optimal order (least to most frequently changed) # This maximizes Docker layer cache efficiency COPY --from=builder --chown=1654:1654 /app/extracted/dependencies/ ./ COPY --from=builder --chown=1654:1654 /app/extracted/spring-boot-loader/ ./ COPY --from=builder --chown=1654:1654 /app/extracted/snapshot-dependencies/ ./ COPY --from=builder --chown=1654:1654 /app/extracted/application/ ./ # Switch to non-root user (security best practice) USER 1654:1654 # Set JAVA_HOME and PATH ENV JAVA_HOME=/opt/java \\ PATH=\u0026#34;/opt/java/bin:${PATH}\u0026#34; # Optimal JVM flags for containerized Spring Boot applications # - UseContainerSupport: respect container memory limits # - MaxRAMPercentage: use max 75% of container memory for heap # - UseG1GC: best GC for containers with predictable pause times # - UseStringDeduplication: reduce memory footprint # - ExitOnOutOfMemoryError: fail fast on OOM # - TieredCompilation with level 1: faster startup, good for short-lived containers ENV JAVA_TOOL_OPTIONS=\u0026#34;-XX:+UseContainerSupport \\ -XX:MaxRAMPercentage=75.0 \\ -XX:InitialRAMPercentage=50.0 \\ -XX:+UseG1GC \\ -XX:MaxGCPauseMillis=100 \\ -XX:+UseStringDeduplication \\ -XX:+ParallelRefProcEnabled \\ -XX:+DisableExplicitGC \\ -XX:+ExitOnOutOfMemoryError \\ -Djava.security.egd=file:/dev/./urandom \\ -Djava.awt.headless=true\u0026#34; # Application server port EXPOSE 8080 # Comprehensive OCI labels for traceability and compliance LABEL org.opencontainers.image.title=\u0026#34;Spring Boot Template\u0026#34; \\ org.opencontainers.image.description=\u0026#34;HMCTS Spring Boot Template - Optimized for Contest 2025\u0026#34; \\ org.opencontainers.image.vendor=\u0026#34;HMCTS Reform Programme\u0026#34; \\ org.opencontainers.image.authors=\u0026#34;HMCTS \u0026lt;hmcts@justice.gov.uk\u0026gt;\u0026#34; \\ org.opencontainers.image.source=\u0026#34;https://github.com/hmcts/spring-boot-template\u0026#34; \\ org.opencontainers.image.version=\u0026#34;0.0.1\u0026#34; \\ org.opencontainers.image.revision=\u0026#34;contest-2025\u0026#34; \\ org.opencontainers.image.licenses=\u0026#34;MIT\u0026#34; \\ org.opencontainers.image.base.name=\u0026#34;docker.io/library/alpine:3.21\u0026#34; \\ org.opencontainers.image.base.digest=\u0026#34;sha256:5405e8f36ce1878720f71217d664aa3dea32e5e5df11acbf07fc78ef5661465b\u0026#34; \\ maintainer=\u0026#34;HMCTS Reform Team\u0026#34; \\ com.hmcts.app.name=\u0026#34;spring-boot-template\u0026#34; \\ com.hmcts.build.date=\u0026#34;2025-10-27\u0026#34; # Health check using Spring Boot Actuator /health endpoint # Using curl for lightweight health checks HEALTHCHECK --interval=30s --timeout=5s --start-period=60s --retries=3 \\ CMD curl -f http://localhost:8080/health || exit 1 # Use tini as init system for proper signal handling # Ensures graceful shutdown and zombie process reaping ENTRYPOINT [\u0026#34;/sbin/tini\u0026#34;, \u0026#34;--\u0026#34;] # Run Spring Boot application # Using exec form to ensure proper signal propagation CMD [\u0026#34;java\u0026#34;, \u0026#34;org.springframework.boot.loader.launch.JarLauncher\u0026#34;] Deployment Notes When applying this to your Java project, keep the core principles shown above: Clear multi-stage build (builder + runtime). JRE optimization (jdeps + jlink, distroless or JRE-only base images). Clear healthchecks (native Java or wget/curl). Dependency optimization (automatic or manual) to reduce CVEs. When copying the template to your own project, you should: Update LABEL org.opencontainers.image.source to your repository. Adjust the healthcheck endpoint (/health, /actuator/health, \u0026hellip;) to match your app. Build and scan the image with tools like Trivy to verify security after optimization. ","date":"2025-11-29T00:00:00Z","image":"https://tech.nguuyen.io.vn/images/docker-optimization/docker-optimization-java.webp","permalink":"https://tech.nguuyen.io.vn/en/posts/docker-optimization/docker-opt-java/","title":"Optimize Docker for Java"},{"content":"Dockerfile Contest 2025 ‚Äì Extreme Python Optimization Dockerfile Contest 2025 encourages the Vietnamese DevOps community to rethink how Dockerfiles are written to achieve security, optimization, and clarity. Below is a dedicated summary for the Python category (FastAPI backend services).\nI. PYTHON Category (Optimization for Backend Services) The Python category focuses on image size optimization, security (CVE patching), and runtime performance for FastAPI apps. Solutions range from distroless, minimal Alpine, to wheel-based builds.\n1. Dockerfile TOP 1 (Python) ‚Äì Thanh Nguyen The Technique Author\u0026rsquo;s explanation Reference UV Package Manager Use uv instead of pip to speed up dependency installs and manage virtual environments more efficiently. Distroless Base Image Use gcr.io/distroless/base-debian12:nonroot to reduce attack surface, without shell, package manager, or unnecessary tools. Multi-arch Support Support amd64 and arm64 by copying shared libraries per architecture. Shared Libraries Copy Copy required libraries (libc, libm, libz, libgcc_s) from the builder stage to run in distroless. Security Patching Upgrade starlette to 0.49.1 to fix CVE-2025-62727 and CVE-2025-54121 without editing pyproject.toml. LD_LIBRARY_PATH Set an env var so the system can find shared libraries in a custom folder. Dockerfile TOP 1 (Python)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 FROM ghcr.io/astral-sh/uv:python3.13-bookworm-slim@sha256:6b8ac7bb76766ffe9f6cc20f56789755d539e8d0e605d8983131227c5c8b87a1 AS builder ENV UV_LINK_MODE=copy ARG TARGETARCH # Copy shared libraries ƒë·ªß ƒë·ªÉ ch·∫°y ·ª©ng d·ª•ng trong m√¥i tr∆∞·ªùng distroless # Ki·ªÉm tra shared libraries c·∫ßn thi·∫øt v·ªõi l·ªánh: # ldd $(which python3) v√† c√°c th∆∞ vi·ªán kh√°c trong virtual environment sau khi c√†i ƒë·∫∑t c√°c package c·∫ßn thi·∫øt (ki·ªÉm tra tr∆∞·ªõc khi build) # M·ªói ki·∫øn tr√∫c s·∫Ω ƒë·∫∑t th∆∞ vi·ªán trong c√°c th∆∞ m·ª•c kh√°c nhau, v√≠ d·ª•: /lib/x86_64-linux-gnu/ cho amd64, /lib/aarch64-linux-gnu/ cho arm64 # do ƒë√≥ c·∫ßn x√°c ƒë·ªãnh ki·∫øn tr√∫c v√† copy t·ª´ th∆∞ m·ª•c t∆∞∆°ng ·ª©ng. # TARGETARCH l√† built-in arg c·ªßa docker buildx, t·ª± ƒë·ªông nh·∫≠n gi√° tr·ªã (amd64 ho·∫∑c arm64) khi build multi-arch # Shared libraries copy ·ªü l·ªánh ph√≠a d∆∞·ªõi l√† ch∆∞a ƒë·ªß ƒë·ªÉ ch·∫°y ·ª©ng d·ª•ng, tuy nhi√™n gcr.io/distroless/base-debian12 (image s·ª≠ d·ª•ng l√†m base image cho runtime t·∫°i runtime state) ƒë√£ c√≥ s·∫µn m·ªôt s·ªë shared libraries n√™n ch·ªâ c·∫ßn copy nh·ªØng th∆∞ vi·ªán c√≤n thi·∫øu. # gcr.io/distroless/base-debian12:nonroot kh√¥ng c√≥ shell, ki·ªÉm tra shared libraries b·∫±ng c√°ch s·ª≠ d·ª•ng gcr.io/distroless/base-debian12:debug # gcr.io/distroless/base-debian12:debug t∆∞∆°ng t·ª± gcr.io/distroless/base-debian12:nonroot nh∆∞ng c√≥ th√™m shell ƒë·ªÉ ph·ª•c v·ª• debug. RUN if [ \u0026#34;$TARGETARCH\u0026#34; = \u0026#34;amd64\u0026#34; ]; then \\ LIBARCH=\u0026#34;x86_64\u0026#34;; \\ elif [ \u0026#34;$TARGETARCH\u0026#34; = \u0026#34;arm64\u0026#34; ]; then \\ LIBARCH=\u0026#34;aarch64\u0026#34;; \\ else \\ LIBARCH=\u0026#34;unknown\u0026#34;; \\ fi \u0026amp;\u0026amp; \\ mkdir -p /lib/multi-arch \u0026amp;\u0026amp; \\ cp /lib/${LIBARCH}-linux-gnu/libc.so.6 /lib/multi-arch/ \u0026amp;\u0026amp; \\ cp /lib/${LIBARCH}-linux-gnu/libm.so.6 /lib/multi-arch/ \u0026amp;\u0026amp; \\ cp /lib/${LIBARCH}-linux-gnu/libz.so.1 /lib/multi-arch/ \u0026amp;\u0026amp; \\ cp /lib/${LIBARCH}-linux-gnu/libgcc_s.so.1 /lib/multi-arch/ WORKDIR /build # S·ª≠ d·ª•ng cache ƒë·ªÉ tƒÉng t·ªëc ƒë·ªô build # C√†i ƒë·∫∑t dependencies trong uv virtual environment # S·ª≠ d·ª•ng mount type=bind ƒë·ªÉ bind c√°c file uv.lock v√† pyproject.toml t·ª´ host v√†o container mount thay v√¨ copy. # --frozen ƒë·ªÉ ƒë·∫£m b·∫£o ch·ªâ c√†i ƒë·∫∑t ƒë√∫ng phi√™n b·∫£n dependencies trong uv.lock, kh√¥ng update uv.lock # --no-install-project ƒë·ªÉ kh√¥ng c√†i ƒë·∫∑t project hi·ªán t·∫°i (ch·ªâ c√†i ƒë·∫∑t dependencies) # --no-dev ƒë·ªÉ kh√¥ng c√†i ƒë·∫∑t dev dependencies # --no-editable ƒë·ªÉ kh√¥ng c√†i ƒë·∫∑t editable mode # starlette 0.46.2 d√≠nh CVE-2025-62727 CVE-2025-54121, n√¢ng c·∫•p ƒë·ªÉ v√° l·ªói b·∫£o m·∫≠t (do thay ƒë·ªïi pyproject.toml v√† file uv.lock s·∫Ω vi ph·∫°m n·ªôi quy n√™n ch·∫°y l·ªánh install ri√™ng) RUN --mount=type=cache,target=/root/.cache/uv \\ --mount=type=bind,source=uv.lock,target=uv.lock \\ --mount=type=bind,source=pyproject.toml,target=pyproject.toml \\ uv sync --frozen --no-install-project --no-dev --no-editable \u0026amp;\u0026amp; \\ uv pip install \u0026#34;starlette==0.49.1\u0026#34; --no-deps # S·ª≠ d·ª•ng distroless l√†m base image cho runtime ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh b·∫£o m·∫≠t v√† t·ªëi ∆∞u k√≠ch th∆∞·ªõc image # Ch·ªçn distroless thay v√¨ alpine v√¨ alpine s·ª≠ d·ª•ng musl libc, trong khi python v√† nhi·ªÅu th∆∞ vi·ªán ph·ªï bi·∫øn trong python ƒë∆∞·ª£c bi√™n d·ªãch v·ªõi glibc, d·∫´n ƒë·∫øn c√°c v·∫•n ƒë·ªÅ t∆∞∆°ng th√≠ch. # distroless gi√∫p ·ª©ng d·ª•ng ch·∫°y ·ªïn ƒë·ªãnh h∆°n v√† c≈©ng r·∫•t nh·∫π. # cc-debian12 c√≥ nhi·ªÅu shared libraries c·∫ßn thi·∫øt cho python v√† c√°c package ph·ªï bi·∫øn h∆°n so v·ªõi base-debian12 # tuy nhi√™n khi ƒë√£ ki·ªÉm tra k·ªπ c√°c shared libraries c·∫ßn thi·∫øt (v·ªõi l·ªánh ldd) v√† copy ƒë·∫ßy ƒë·ªß t·ª´ builder stage th√¨ base-debian12 s·∫Ω gi√∫p t·ªëi ∆∞u k√≠ch th∆∞·ªõc image h∆°n m√† v·∫´n ƒë·∫£m b·∫£o ·ª©ng d·ª•ng ch·∫°y ·ªïn ƒë·ªãnh. FROM gcr.io/distroless/base-debian12:nonroot@sha256:10136f394cbc891efa9f20974a48843f21a6b3cbde55b1778582195d6726fa85 AS runtime LABEL maintainer=\u0026#34;Thanh Nguyen The\u0026#34; LABEL maintainer.email=\u0026#34;thanhnt.devops@gmail.com\u0026#34; LABEL maintainer.company=\u0026#34;VIETNAM NATIONAL CYBER SECURITY TECHNOLOGY CORPORATION\u0026#34; LABEL maintainer.youtube=\u0026#34;DevOps Mentor\u0026#34; LABEL image.description=\u0026#34;Secure, minimal Python app using UV and Distroless\u0026#34; WORKDIR /app # Copy c√°c th∆∞ vi·ªán v√† python t·ª´ builder stage COPY --from=builder /lib/multi-arch/ /lib/multi-arch/ COPY --from=builder /usr/local/lib/libpython3.13.so.1.0 /usr/local/lib/libpython3.13.so.1.0 COPY --from=builder /usr/local/lib/python3.13/ /usr/local/lib/python3.13/ COPY --from=builder /usr/local/bin/python /usr/local/bin/python3 # Copy virtual environment t·ª´ builder COPY --from=builder --chown=nonroot:nonroot /build/.venv/ /app/.venv/ # Copy source code - ch·ªâ copy nh·ªØng g√¨ c·∫ßn thi·∫øt COPY --chown=nonroot:nonroot src/ ./src/ # Thi·∫øt l·∫≠p environment variables # Do runtime limit l√† 1 vCPU, 512MB RAM n√™n thi·∫øt l·∫≠p WORKERS=2 thay v√¨ 3 (nguy c∆° OOM). C√¥ng th·ª©c worker = (2 x s·ªë l∆∞·ª£ng vCPU + 1) ch·ªâ √°p d·ª•ng trong tr∆∞·ªùng h·ª£p \u0026gt; 1GB RAM # LD_LIBRARY_PATH ƒë·ªÉ h·ªá th·ªëng c√≥ th·ªÉ t√¨m th·∫•y c√°c shared libraries c·∫ßn thi·∫øt t·∫°i th∆∞ m·ª•c m·ªõi thay v√¨ th∆∞ m·ª•c m·∫∑c ƒëinh (/lib/x86_64-linux-gnu ho·∫∑c /lib/aarch64-linux-gnu) # shared libraries kh√¥ng c√≥ trong /lib/multi-arch s·∫Ω ti·∫øp t·ª•c ƒë∆∞·ª£c load t·ª´ th∆∞ m·ª•c m·∫∑c ƒë·ªãnh c·ªßa h·ªá th·ªëng ENV PATH=\u0026#34;/app/.venv/bin/:$PATH\u0026#34; \\ PYTHONPATH=\u0026#34;/app/src/\u0026#34; \\ LANG=C.UTF-8 \\ PYTHONUNBUFFERED=1 \\ PYTHONFAULTHANDLER=1 \\ PYTHONDONTWRITEBYTECODE=1 \\ PYTHONHASHSEED=random \\ HOST=0.0.0.0 \\ PORT=8080 \\ WORKERS=2 \\ LOGGING__LEVEL=INFO \\ LOGGING__FORMAT=PLAIN \\ COFFEE_API__HOST=\u0026#34;https://api.sampleapis.com/coffee/\u0026#34; \\ APP_VERSION=0.1.0 \\ GIT_COMMIT_SHA=sha \\ LD_LIBRARY_PATH=/lib/multi-arch # nonroot user m·∫∑c ƒë·ªãnh ƒë√£ ƒë∆∞·ª£c s·ª≠ d·ª•ng trong distroless base-debian12:nonroot n√™n kh√¥ng c·∫ßn thi·∫øt ph·∫£i th√™m l·ªánh ph√≠a d∆∞·ªõi # USER nonroot:nonroot # Expose port m·∫∑c ƒë·ªãnh EXPOSE 8080 # Command ƒë·ªÉ ch·∫°y ·ª©ng d·ª•ng ENTRYPOINT [\u0026#34;python\u0026#34;, \u0026#34;src/python_service_template/app.py\u0026#34;] 2. Dockerfile TOP 2 (Python) ‚Äì newnol Technique Author\u0026rsquo;s explanation Reference Alpine Base Image Use python:3.13-alpine for a smaller image than Debian-based images. Security Patches Update packages with CVEs: starlette, fastapi, aiohttp, pydantic, structlog, uvloop, uvicorn to safe versions. Ultra Aggressive Optimization Strip all .so files, remove __pycache__, tests, docs, examples, typing stubs, license files to reduce size. Python Stdlib Cleanup Remove modules not needed like pip, setuptools, wheel, tkinter, distutils, lib2to3, idlelib, test, unittest. Non-root User Create UID 10001 user with minimum privileges for security. Healthcheck Use wget to check /health/ endpoint with short timeout. Dockerfile TOP 2 (Python)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 # syntax=docker/dockerfile:1.7 # ============================================================================= # DOCKERFILE ULTRA OPTIMIZED + SECURITY PATCHED # M·ª•c ti√™u: Nh·∫π (\u0026lt;110MB) + B·∫£o m·∫≠t cao (0 CVEs) # ============================================================================= # ----------------------------------------------------------------------------- # Stage 1: Dependencies Builder # ----------------------------------------------------------------------------- FROM python:3.13-alpine@sha256:e5fa639e49b85986c4481e28faa2564b45aa8021413f31026c3856e5911618b1 AS deps ENV PIP_NO_CACHE_DIR=1 \\ PIP_DISABLE_PIP_VERSION_CHECK=1 \\ PYTHONDONTWRITEBYTECODE=1 \\ PYTHONUNBUFFERED=1 RUN --mount=type=cache,target=/var/cache/apk \\ apk add --no-cache --virtual .build-deps \\ build-base \\ python3-dev \\ cargo # Install dependencies v·ªõi PATCHED versions ƒë·ªÉ fix CVEs # Note: FastAPI 0.116+ required for starlette 0.49.1+ support RUN --mount=type=cache,target=/root/.cache/pip \\ python -m pip install --no-cache-dir --prefix=/install \\ \u0026#34;aiohttp\u0026gt;=3.12.14,\u0026lt;4.0.0\u0026#34; \\ \u0026#34;asgi-correlation-id\u0026gt;=4.3.4,\u0026lt;5.0.0\u0026#34; \\ \u0026#34;fastapi\u0026gt;=0.116.0\u0026#34; \\ \u0026#34;prometheus-fastapi-instrumentator\u0026gt;=7.0.0,\u0026lt;8.0.0\u0026#34; \\ \u0026#34;pydantic\u0026gt;=2.11.0,\u0026lt;3.0.0\u0026#34; \\ \u0026#34;pydantic-settings\u0026gt;=2.9.1,\u0026lt;3.0.0\u0026#34; \\ \u0026#34;structlog\u0026gt;=25.3.0,\u0026lt;26.0.0\u0026#34; \\ \u0026#34;uvloop\u0026gt;=0.21.0,\u0026lt;0.22.0\u0026#34; \\ \u0026#34;uvicorn[standard]\u0026gt;=0.30.0,\u0026lt;0.31.0\u0026#34; # ULTRA AGGRESSIVE optimization RUN apk add --no-cache binutils \\ # Strip ALL .so files aggressively \u0026amp;\u0026amp; find /install -type f \\( -name \u0026#39;*.so*\u0026#39; -o -name \u0026#39;*.a\u0026#39; \\) -exec strip --strip-all {} + 2\u0026gt;/dev/null || true \\ # Remove all bytecode \u0026amp;\u0026amp; find /install \\( -type d -name __pycache__ -o -type f -name \u0026#39;*.py[co]\u0026#39; \\) -delete 2\u0026gt;/dev/null || true \\ # Remove test/doc/examples \u0026amp;\u0026amp; find /install -type d \\( -name tests -o -name testing -o -name test -o -name doc -o -name docs -o -name example -o -name examples \\) -prune -exec rm -rf {} + 2\u0026gt;/dev/null || true \\ # Minimize .dist-info \u0026amp;\u0026amp; find /install -name \u0026#39;*.dist-info\u0026#39; -type d -exec sh -c \u0026#39;cd \u0026#34;$1\u0026#34; \u0026amp;\u0026amp; find . -type f ! -name \u0026#34;METADATA\u0026#34; ! -name \u0026#34;top_level.txt\u0026#34; ! -name \u0026#34;RECORD\u0026#34; -delete\u0026#39; _ {} \\; 2\u0026gt;/dev/null || true \\ # Remove typing stubs, headers, C files \u0026amp;\u0026amp; find /install -type f \\( -name \u0026#39;*.pyi\u0026#39; -o -name \u0026#39;*.c\u0026#39; -o -name \u0026#39;*.h\u0026#39; -o -name \u0026#39;*.cpp\u0026#39; -o -name \u0026#39;*.cc\u0026#39; \\) -delete \\ # Remove license files \u0026amp;\u0026amp; find /install -type f \\( -name \u0026#39;LICENSE*\u0026#39; -o -name \u0026#39;COPYING*\u0026#39; -o -name \u0026#39;NOTICE*\u0026#39; -o -name \u0026#39;AUTHORS*\u0026#39; -o -name \u0026#39;CHANGELOG*\u0026#39; -o -name \u0026#39;README*\u0026#39; \\) -delete 2\u0026gt;/dev/null || true \\ \u0026amp;\u0026amp; apk del binutils .build-deps # ----------------------------------------------------------------------------- # Stage 2: Runtime (ULTRA MINIMAL + SECURE) # ----------------------------------------------------------------------------- FROM python:3.13-alpine@sha256:e5fa639e49b85986c4481e28faa2564b45aa8021413f31026c3856e5911618b1 AS runtime LABEL org.opencontainers.image.title=\u0026#34;Python Service Template\u0026#34; \\ org.opencontainers.image.description=\u0026#34;Production-ready FastAPI service - Optimized \u0026amp; Secured\u0026#34; \\ org.opencontainers.image.version=\u0026#34;0.1.0\u0026#34; \\ org.opencontainers.image.authors=\u0026#34;newnol \u0026lt;contact@newnol.io.vn\u0026gt;\u0026#34; \\ maintainer=\u0026#34;newnol\u0026#34; \\ security.scan=\u0026#34;trivy-passed\u0026#34; ENV PYTHONDONTWRITEBYTECODE=1 \\ PYTHONUNBUFFERED=1 \\ PIP_NO_CACHE_DIR=1 \\ HOST=0.0.0.0 \\ PORT=5000 \\ WORKERS=1 \\ PYTHONPATH=/app/src \\ TZ=UTC # Install ONLY wget for healthcheck RUN --mount=type=cache,target=/var/cache/apk \\ apk add --no-cache wget WORKDIR /app # Create non-root user RUN addgroup -g 10001 -S app \\ \u0026amp;\u0026amp; adduser -u 10001 -S -G app -h /app -s /sbin/nologin app # Copy dependencies COPY --from=deps --chown=app:app /install /usr/local # Copy source (minimal) COPY --chown=app:app src/ ./src/ # Permissions RUN chmod -R 550 /app # EXTREME Python stdlib cleanup RUN rm -rf \\ /usr/local/lib/python3.13/ensurepip \\ /usr/local/lib/python3.13/site-packages/pip* \\ /usr/local/lib/python3.13/site-packages/setuptools* \\ /usr/local/lib/python3.13/site-packages/wheel* \\ /usr/local/lib/python3.13/distutils \\ /usr/local/lib/python3.13/lib2to3 \\ /usr/local/lib/python3.13/idlelib \\ /usr/local/lib/python3.13/tkinter \\ /usr/local/lib/python3.13/turtledemo \\ /usr/local/lib/python3.13/test \\ /usr/local/lib/python3.13/unittest/test \\ /usr/local/bin/pip* \\ /usr/local/bin/2to3* \\ /usr/local/bin/idle* \\ 2\u0026gt;/dev/null || true # Clean up more unused stdlib modules RUN cd /usr/local/lib/python3.13 \u0026amp;\u0026amp; rm -rf \\ turtle.py \\ pydoc_data \\ 2\u0026gt;/dev/null || true USER app EXPOSE 5000 # Heathcheck HEALTHCHECK --interval=15s --timeout=3s --start-period=10s --retries=2 \\ CMD wget --no-verbose --tries=1 -O /dev/null http://127.0.0.1:${PORT}/health/ || exit 1 STOPSIGNAL SIGTERM CMD [\u0026#34;python\u0026#34;, \u0026#34;src/python_service_template/app.py\u0026#34;] 3. Dockerfile TOP (Python) ‚Äì Wheel-based Build Technique Explanation Reference Wheel-based Installation Build all dependencies into wheels, then install offline to speed up builds and ensure reproducibility. Dynamic Security Patching Use a Python script to parse pyproject.toml and upgrade fastapi and starlette to safe versions without editing the original file. PIP_ONLY_BINARY Use wheel files only, avoid building from source, faster builds and fewer compile errors. Offline Installation Install from local wheels without internet in the runtime stage. Native Healthcheck Use Python http.client instead of external tools like curl/wget, reducing dependencies. Non-root User Create UID 10001 user with a dedicated home directory for security. Dockerfile TOP (Python)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 # syntax=docker/dockerfile:1.7 # ############################## # builder ############################## FROM python:3.13-slim AS builder ENV PIP_DISABLE_PIP_VERSION_CHECK=1 \\ PIP_NO_CACHE_DIR=1 \\ PIP_ONLY_BINARY=:all: \\ PYTHONDONTWRITEBYTECODE=1 \\ PYTHONUNBUFFERED=1 WORKDIR /app # Pre-cache manifests COPY pyproject.toml README.md LICENSE* ./ # Fix HIGH vulnerable issue: CVE-2025-62727 by upgrading starlette and fastapi. RUN --mount=type=cache,target=/root/.cache/pip python - \u0026lt;\u0026lt;\u0026#39;PY\u0026#39; import tomllib, pathlib, re def parse_req(s:str): m = re.match(r\u0026#39;^\\s*([A-Za-z0-9_.-]+)(\\[[^\\]]+\\])?\\s*(.*)$\u0026#39;, s) if m: name, extras, rest = m.group(1), (m.group(2) or \u0026#39;\u0026#39;), (m.group(3) or \u0026#39;\u0026#39;) return name, extras, rest name = re.split(r\u0026#39;[\u0026gt;\u0026lt;=~!; ]\u0026#39;, s, 1)[0] return name, \u0026#39;\u0026#39;, s[len(name):] data = tomllib.loads(pathlib.Path(\u0026#39;pyproject.toml\u0026#39;).read_text()) deps = data.get(\u0026#39;project\u0026#39;, {}).get(\u0026#39;dependencies\u0026#39;, []) safe = [] present = set() for d in deps: name, extras, rest = parse_req(d) norm = name.lower().replace(\u0026#39;_\u0026#39;,\u0026#39;-\u0026#39;) if norm == \u0026#39;fastapi\u0026#39;: safe.append(f\u0026#39;fastapi{extras}\u0026gt;=0.118,\u0026lt;0.121\u0026#39;) else: safe.append(d) present.add(norm) if \u0026#39;starlette\u0026#39; not in present: safe.append(\u0026#39;starlette\u0026gt;=0.49.1,\u0026lt;0.50\u0026#39;) pathlib.Path(\u0026#39;/requirements.safe.txt\u0026#39;).write_text(\u0026#39;\\n\u0026#39;.join(safe) + \u0026#39;\\n\u0026#39;) print(\u0026#39;Resolved safe deps:\u0026#39;, *safe, sep=\u0026#39;\\n- \u0026#39;) PY # Wheel ALL dependencies from the safe list RUN --mount=type=cache,target=/root/.cache/pip \\ pip wheel --wheel-dir /wheels -r /requirements.safe.txt # Build wheel of the project itself COPY src/ ./src/ RUN --mount=type=cache,target=/root/.cache/pip \\ pip wheel --wheel-dir /wheels . ############################## # runtime ############################## FROM python:3.13-slim AS runtime ARG VERSION=0.1.0 ARG VCS_REF=sha ARG BUILD_DATE LABEL org.opencontainers.image.title=\u0026#34;python-service-template\u0026#34; \\ org.opencontainers.image.description=\u0026#34;Dockerfile contest build\u0026#34; \\ org.opencontainers.image.version=$VERSION \\ org.opencontainers.image.revision=$VCS_REF \\ org.opencontainers.image.created=$BUILD_DATE \\ org.opencontainers.image.licenses=\u0026#34;Apache-2.0\u0026#34; ENV PYTHONDONTWRITEBYTECODE=1 \\ PYTHONUNBUFFERED=1 \\ PYTHONOPTIMIZE=2 \\ HOST=0.0.0.0 \\ PORT=5000 \\ WORKERS=1 \\ APP_VERSION=$VERSION \\ GIT_COMMIT_SHA=$VCS_REF # Non-root RUN useradd --create-home --uid 10001 --shell /usr/sbin/nologin appuser WORKDIR /home/appuser # Install offline: install ALL safe deps, then the app wheel with --no-deps COPY --from=builder /wheels /wheels COPY --from=builder /requirements.safe.txt /requirements.safe.txt RUN pip install --no-index --find-links=/wheels -r /requirements.safe.txt \\ \u0026amp;\u0026amp; pip install --no-index --find-links=/wheels --no-deps \\ python-service-template --no-compile \\ \u0026amp;\u0026amp; rm -rf /wheels /requirements.safe.txt EXPOSE 5000 HEALTHCHECK --interval=30s --timeout=2s --start-period=10s --retries=3 \\ CMD python -c \u0026#34;import sys, http.client; c=http.client.HTTPConnection(\u0026#39;127.0.0.1\u0026#39;, int(__import__(\u0026#39;os\u0026#39;).environ.get(\u0026#39;PORT\u0026#39;,\u0026#39;5000\u0026#39;)), timeout=1); c.request(\u0026#39;GET\u0026#39;,\u0026#39;/health\u0026#39;); r=c.getresponse(); sys.exit(0 if r.status==200 else 1)\u0026#34; || exit 1 USER 10001:10001 CMD [\u0026#34;python\u0026#34;, \u0026#34;-m\u0026#34;, \u0026#34;python_service_template.app\u0026#34;] 4. Dockerfile TOP (Python) ‚Äì Khiem Doan Technique Explanation Reference UV Package Manager Use uv to manage dependencies faster than pip, with cache mounts for quicker rebuilds. Alpine + Tini Use lightweight Alpine Linux and tini as init system for proper signal handling. Security Patching Upgrade starlette to 0.50.0 to fix CVEs without editing pyproject.toml. Non-root User Create nonroot user with UID/GID 14406, an uncommon UID to avoid conflicts. Healthcheck with curl Use curl to check the health endpoint and grep to verify JSON response. OCI Labels Add full OCI labels with metadata: version, build date, revision, source. Dockerfile TOP (Python)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 # syntax=docker/dockerfile:1.19 FROM python:3.13-alpine3.22 AS deps RUN --mount=type=cache,target=/root/.cache/pip \\ pip install --no-compile uv==0.9.2 WORKDIR /app COPY pyproject.toml uv.lock ./ RUN --mount=type=cache,target=/root/.cache/uv \\ uv sync --frozen --no-dev --no-install-project \\ \u0026amp;\u0026amp; uv pip install starlette==0.50.0 FROM python:3.13-alpine3.22 AS final RUN pip install -U pip RUN --mount=type=cache,target=/var/cache/apk \\ apk add --no-cache \\ curl=8.14.1-r2 \\ tini=0.19.0-r3 ARG VERSION=\u0026#34;0.1.0\u0026#34; ARG BUILD_DATE=\u0026#34;2025-11-10T00:00:00Z\u0026#34; ARG REVISION=\u0026#34;unknown\u0026#34; ARG GIT_COMMIT_SHA=\u0026#34;unknown\u0026#34; LABEL org.opencontainers.image.title=\u0026#34;python-service-template\u0026#34; \\ org.opencontainers.image.description=\u0026#34;A batteries-included template for building robust, production-ready Python backend services with FastAPI\u0026#34; \\ org.opencontainers.image.authors=\u0026#34;Khiem Doan\u0026#34; \\ org.opencontainers.image.version=$VERSION \\ org.opencontainers.image.created=$BUILD_DATE \\ org.opencontainers.image.revision=$REVISION \\ org.opencontainers.image.source=\u0026#34;https://github.com/khiemdoan/\u0026#34; \\ org.opencontainers.image.licenses=\u0026#34;MIT\u0026#34; ENV USER=nonroot \\ GROUP=nonroot \\ UID=14406 \\ GID=14406 RUN addgroup -g \u0026#34;$GID\u0026#34; \u0026#34;$GROUP\u0026#34; \\ \u0026amp;\u0026amp; adduser -D -u \u0026#34;$UID\u0026#34; -G \u0026#34;$GROUP\u0026#34; \u0026#34;$USER\u0026#34; USER $USER WORKDIR /app COPY --chown=$USER:$GROUP src/ src/ COPY --from=deps --chown=$USER:$GROUP /app/.venv /app/.venv ENV PATH=\u0026#34;/app/.venv/bin:$PATH\u0026#34; \\ PYTHONPATH=\u0026#34;/app/src\u0026#34; \\ PYTHONUNBUFFERED=1 \\ PYTHONDONTWRITEBYTECODE=1 \\ HOST=0.0.0.0 \\ PORT=3000 \\ WORKERS=1 \\ LOGGING__LEVEL=INFO \\ LOGGING__FORMAT=PLAIN \\ COFFEE_API__HOST=https://api.sampleapis.com/coffee/ \\ APP_VERSION=$VERSION \\ GIT_COMMIT_SHA=$GIT_COMMIT_SHA EXPOSE $PORT HEALTHCHECK --timeout=1s \\ CMD curl -f \u0026#34;http://localhost:${PORT}/health/\u0026#34; | grep \u0026#39;\u0026#34;heartbeat\u0026#34;:\u0026#34;HEALTHY\u0026#34;\u0026#39; || exit 1 ENTRYPOINT [\u0026#34;/sbin/tini\u0026#34;, \u0026#34;--\u0026#34;] CMD [\u0026#34;python\u0026#34;, \u0026#34;src/python_service_template/app.py\u0026#34;] Deployment Notes The Dockerfiles above are architectural references only; when applying them to your Python/FastAPI project, keep the core principles: multi-stage build, security patching, non-root user, pin SHA256 for base images. Distroless vs Alpine: Distroless is more secure (no shell, package manager) but requires manual shared library copying. Alpine is lighter and easier to debug but may have compatibility issues with some Python packages. UV vs PIP: UV is significantly faster than pip (10-100x) and manages virtual environments better, but requires extra setup. Security: Always update dependencies to patch CVEs, especially common packages like starlette, fastapi, uvicorn. Healthcheck: Prefer native Python http.client or curl/wget depending on the base image and security requirements. ","date":"2025-11-23T00:00:00Z","image":"https://tech.nguuyen.io.vn/images/docker-optimization/docker-optimization-python.webp","permalink":"https://tech.nguuyen.io.vn/en/posts/docker-optimization/docker-opt-python/","title":"Optimize Docker for Python"},{"content":"Dockerfile Contest 2025 ‚Äì Extreme React/Node Optimization Dockerfile Contest 2025 encourages the Vietnamese DevOps community to rethink how Dockerfiles are written to achieve security, optimization, and clarity. Below is a dedicated summary for the React category (Node.js apps built into static assets).\nI. REACT Category (Optimization for Static Web Serving) The React category focuses on reducing image size and speeding up static file delivery. Some teams compile their own HTTP server or run with FROM scratch to achieve the smallest possible footprint.\n1. Lightest Docker Image Award (Top Slim) ‚Äì Nguy·ªÖn Ph√∫c B·∫£o L√¢m Technique Author\u0026rsquo;s explanation Reference Project Selection Leverage the advantage of static apps (React) to reach smaller images than Java/Python. Extreme Base Image Use lipanski/docker-static-website:latest, a BusyBox build trimmed down to only an HTTP server (~92 KB). Maximum Pre-compression Pre-compress all assets with Gzip level 9, delete original files, final image ~300 KB. Healthcheck Write \u0026quot;OK\u0026quot; to dist/health for a quick probe endpoint. Dockerfile ‚Äì Lightest Docker Image\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 # syntax=docker/dockerfile:1.7 # ============================================================================== # Build Stage - Using Node Alpine for minimal size # ============================================================================== FROM node:22.21.1-alpine3.21@sha256:af8023ec879993821f6d5b21382ed915622a1b0f1cc03dbeb6804afaf01f8885 AS builder # Install pnpm with specific version from package.json and gzip for pre-compression ENV PNPM_HOME=\u0026#34;/pnpm\u0026#34; ENV PATH=\u0026#34;$PNPM_HOME:$PATH\u0026#34; RUN corepack enable \u0026amp;\u0026amp; \\ corepack prepare pnpm --activate WORKDIR /app # Copy package files for dependency installation (optimized layer caching) COPY package.json pnpm-lock.yaml ./ # Install dependencies with cache mount for faster rebuilds # Installs all dependencies (including devDependencies needed for build: typescript, vite, tailwindcss, etc.) RUN --mount=type=cache,id=pnpm,target=/pnpm/store \\ pnpm install --frozen-lockfile # Copy only necessary source files (exclude tests, docs, config files not needed for build) COPY tsconfig.json tsconfig.node.json vite.config.ts tailwind.config.ts postcss.config.js ./ COPY index.html ./ COPY public ./public COPY src ./src # Build the application RUN pnpm run build \u0026amp;\u0026amp; \\ # Verify build output exists test -d dist \u0026amp;\u0026amp; test -f dist/index.html \u0026amp;\u0026amp; \\ # Remove bundle visualizer output (not needed in production, saves ~100KB compressed) rm -f dist/stats.html \u0026amp;\u0026amp; \\ # Create a minimal health check endpoint (1 byte file for ultra-fast response) echo \u0026#34;OK\u0026#34; \u0026gt; dist/health \u0026amp;\u0026amp; \\ # Pre-compress all static files with gzip (level 9 = maximum compression) find dist -type f \\( \\ -name \u0026#34;*.html\u0026#34; -o \\ -name \u0026#34;*.css\u0026#34; -o \\ -name \u0026#34;*.js\u0026#34; -o \\ -name \u0026#34;*.json\u0026#34; -o \\ -name \u0026#34;*.xml\u0026#34; -o \\ -name \u0026#34;*.txt\u0026#34; -o \\ -name \u0026#34;*.svg\u0026#34; \\ \\) -exec sh -c \u0026#39;gzip -9 \u0026#34;{}\u0026#34;\u0026#39; \\; # ============================================================================== # Production Stage - Using lipanski/docker-static-website for extreme minimal footprint (92.5 KB base) # ============================================================================== FROM lipanski/docker-static-website:latest AS production # Add OCI labels for metadata LABEL org.opencontainers.image.title=\u0026#34;Vite React Template\u0026#34; \\ org.opencontainers.image.description=\u0026#34;Production-ready Vite React application with extreme minimal footprint\u0026#34; \\ org.opencontainers.image.version=\u0026#34;0.4.0\u0026#34; \\ org.opencontainers.image.licenses=\u0026#34;MIT OR Apache-2.0\u0026#34; \\ org.opencontainers.image.base.name=\u0026#34;lipanski/docker-static-website:latest\u0026#34; # Copy built assets from builder stage # lipanski/docker-static-website serves from /home/static COPY --from=builder /app/dist /home/static # Expose port (BusyBox httpd uses port 3000 by default) EXPOSE 3000 # The base image already has CMD set to run BusyBox httpd # It automatically serves .gz files when Accept-Encoding: gzip is present # No additional configuration needed - inherited from base image 2. Dockerfile TOP 1 (React) ‚Äì Nguy·ªÖn H·ªØu Ph∆∞∆°ng Technique Author\u0026rsquo;s explanation Reference FROM SCRATCH \u0026amp; Static Linking Final stage is scratch, so Nginx must be built statically in Stage 2. Nginx Binary Optimization Disable \u0026gt;30 modules, shrink binary ~76% (5.2 MB). Binary Compression (UPX) Use upx --best --lzma, reduce another ~56%. Parallel Compression Run Gzip and Brotli in parallel to shift CPU to build-time. Base Image Security Pin SHA256 for all base images. Minimal Healthcheck Use nginx -t -q, no curl/wget required. Dockerfile TOP 1 (React)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 # syntax=docker/dockerfile:1.7 # Multi-arch support: Automatically provided by buildx ARG BUILDPLATFORM ARG TARGETPLATFORM ARG TARGETARCH ARG BUILD_DATE ARG GIT_COMMIT=unknown ARG NGINX_VERSION=1.26.2 ARG NODE_VERSION=20 ARG ALPINE_VERSION=3.20 # ============================================================================== # Stage 1: Application Build # ============================================================================== FROM node:${NODE_VERSION}-alpine@sha256:2d5e8a8a51bc341fd5f2eed6d91455c3a3d147e91a14298fc564b5dc519c1666 AS builder WORKDIR /app # Setup pnpm with corepack ENV PNPM_HOME=\u0026#34;/pnpm\u0026#34; \\ PATH=\u0026#34;$PNPM_HOME:$PATH\u0026#34; RUN corepack enable \u0026amp;\u0026amp; corepack prepare pnpm@9.12.2 --activate # Install dependencies with cache mount COPY package.json pnpm-lock.yaml .npmrc ./ RUN --mount=type=cache,id=pnpm,target=/pnpm/store \\ pnpm install --frozen-lockfile --prefer-offline # Copy source and build configuration COPY tsconfig.json tsconfig.node.json vite.config.ts ./ COPY postcss.config.js tailwind.config.ts biome.json ./ COPY index.html ./ COPY public ./public COPY src ./src # Build and clean artifacts ENV NODE_ENV=production RUN pnpm build \u0026amp;\u0026amp; \\ find dist -type f \\( -name \u0026#34;*.map\u0026#34; -o -name \u0026#34;.*\u0026#34; \\) -delete \u0026amp;\u0026amp; \\ rm -f dist/stats.html # ============================================================================== # Stage 2: Static Nginx Binary Builder # ============================================================================== FROM alpine:${ALPINE_VERSION}@sha256:765942a4039992336de8dd5db680586e1a206607dd06170ff0a37267a9e01958 AS nginx-builder ARG NGINX_VERSION ARG TARGETPLATFORM ARG BUILDPLATFORM ENV NGINX_SHA256=627fe086209bba80a2853a0add9d958d7ebbdffa1a8467a5784c9a6b4f03d738 # Log build platform info for multi-arch RUN echo \u0026#34;Building on $BUILDPLATFORM for $TARGETPLATFORM\u0026#34; # Install build dependencies RUN apk add --no-cache \\ gcc g++ musl-dev make linux-headers curl \\ pcre-dev pcre2-dev zlib-dev zlib-static \\ openssl-dev openssl-libs-static upx # Download and verify nginx WORKDIR /tmp RUN curl -fSL \u0026#34;https://nginx.org/download/nginx-${NGINX_VERSION}.tar.gz\u0026#34; -o nginx.tar.gz \u0026amp;\u0026amp; \\ echo \u0026#34;${NGINX_SHA256} nginx.tar.gz\u0026#34; | sha256sum -c - # Build fully static nginx with minimal modules RUN tar -xzf nginx.tar.gz \u0026amp;\u0026amp; \\ cd \u0026#34;nginx-${NGINX_VERSION}\u0026#34; \u0026amp;\u0026amp; \\ ./configure \\ --prefix=/usr/local/nginx \\ --sbin-path=/usr/local/nginx/sbin/nginx \\ --conf-path=/etc/nginx/nginx.conf \\ --pid-path=/run/nginx.pid \\ --lock-path=/run/nginx.lock \\ --error-log-path=/dev/stderr \\ --http-log-path=/dev/stdout \\ --user=nobody \\ --group=nobody \\ # Performance features --with-threads \\ --with-file-aio \\ --with-http_ssl_module \\ --with-http_v2_module \\ --with-http_gzip_static_module \\ --with-http_stub_status_module \\ --with-pcre \\ --with-pcre-jit \\ # Static linking and optimization --with-cc-opt=\u0026#39;-static -Os -ffunction-sections -fdata-sections\u0026#39; \\ --with-ld-opt=\u0026#39;-static -Wl,--gc-sections\u0026#39; \\ # Disable unnecessary modules --without-http_charset_module \\ --without-http_ssi_module \\ --without-http_userid_module \\ --without-http_auth_basic_module \\ --without-http_mirror_module \\ --without-http_autoindex_module \\ --without-http_geo_module \\ --without-http_map_module \\ --without-http_split_clients_module \\ --without-http_referer_module \\ --without-http_rewrite_module \\ --without-http_proxy_module \\ --without-http_fastcgi_module \\ --without-http_uwsgi_module \\ --without-http_scgi_module \\ --without-http_grpc_module \\ --without-http_memcached_module \\ --without-http_limit_conn_module \\ --without-http_limit_req_module \\ --without-http_empty_gif_module \\ --without-http_browser_module \\ --without-http_upstream_hash_module \\ --without-http_upstream_ip_hash_module \\ --without-http_upstream_least_conn_module \\ --without-http_upstream_random_module \\ --without-http_upstream_keepalive_module \\ --without-http_upstream_zone_module \\ --without-mail_pop3_module \\ --without-mail_imap_module \\ --without-mail_smtp_module \\ --without-stream_limit_conn_module \\ --without-stream_access_module \\ --without-stream_geo_module \\ --without-stream_map_module \\ --without-stream_split_clients_module \\ --without-stream_return_module \\ --without-stream_set_module \\ --without-stream_upstream_hash_module \\ --without-stream_upstream_least_conn_module \\ --without-stream_upstream_random_module \\ --without-stream_upstream_zone_module \u0026amp;\u0026amp; \\ make -j\u0026#34;$(nproc)\u0026#34; \u0026amp;\u0026amp; \\ make install # Optimize binary: strip symbols + UPX compression RUN strip --strip-all /usr/local/nginx/sbin/nginx \u0026amp;\u0026amp; \\ upx --best --lzma /usr/local/nginx/sbin/nginx \u0026amp;\u0026amp; \\ /usr/local/nginx/sbin/nginx -V # ============================================================================== # Stage 3: Asset Compression # ============================================================================== FROM alpine:${ALPINE_VERSION}@sha256:765942a4039992336de8dd5db680586e1a206607dd06170ff0a37267a9e01958 AS compressor RUN apk add --no-cache brotli gzip findutils WORKDIR /app COPY --from=builder /app/dist ./dist # Parallel compression: gzip + brotli for all text-based assets RUN find dist -type f \\ \\( -name \u0026#34;*.html\u0026#34; -o -name \u0026#34;*.css\u0026#34; -o -name \u0026#34;*.js\u0026#34; -o \\ -name \u0026#34;*.json\u0026#34; -o -name \u0026#34;*.svg\u0026#34; -o -name \u0026#34;*.xml\u0026#34; \\) \\ -print0 | xargs -0 -P\u0026#34;$(nproc)\u0026#34; -I {} sh -c \u0026#39;gzip -9 -k -f \u0026#34;{}\u0026#34; \u0026amp;\u0026amp; brotli -q 11 -f \u0026#34;{}\u0026#34;\u0026#39; # ============================================================================== # Stage 4: Minimal Filesystem Preparation # ============================================================================== FROM alpine:${ALPINE_VERSION}@sha256:765942a4039992336de8dd5db680586e1a206607dd06170ff0a37267a9e01958 AS rootfs # Create directory structure RUN mkdir -p \\ /rootfs/etc/nginx/conf.d \\ /rootfs/usr/share/nginx/html \\ /rootfs/var/log/nginx \\ /rootfs/var/cache/nginx \\ /rootfs/usr/local/nginx/{client_body,proxy,fastcgi,uwsgi,scgi}_temp \\ /rootfs/tmp \\ /rootfs/run \u0026amp;\u0026amp; \\ chmod 1777 /rootfs/tmp # Create minimal user database (nobody user) RUN echo \u0026#34;nobody:x:65534:65534:nobody:/:/sbin/nologin\u0026#34; \u0026gt; /rootfs/etc/passwd \u0026amp;\u0026amp; \\ echo \u0026#34;nobody:x:65534:\u0026#34; \u0026gt; /rootfs/etc/group # Copy nginx configuration COPY nginx.conf /rootfs/etc/nginx/conf.d/default.conf COPY --from=nginx-builder /etc/nginx/mime.types /rootfs/etc/nginx/mime.types COPY --from=compressor /app/dist /rootfs/usr/share/nginx/html # Create main nginx.conf RUN cat \u0026gt; /rootfs/etc/nginx/nginx.conf \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; worker_processes auto; error_log stderr warn; pid /run/nginx.pid; events { worker_connections 1024; use epoll; multi_accept on; } http { include /etc/nginx/mime.types; default_type application/octet-stream; access_log /dev/stdout; # Performance optimizations sendfile on; tcp_nopush on; tcp_nodelay on; keepalive_timeout 65; server_tokens off; # Compression gzip on; gzip_static on; gzip_vary on; gzip_types text/plain text/css application/json application/javascript text/xml application/xml image/svg+xml; include /etc/nginx/conf.d/*.conf; } EOF # Set proper ownership RUN chown -R 65534:65534 \\ /rootfs/usr/share/nginx/html \\ /rootfs/var/log/nginx \\ /rootfs/var/cache/nginx \\ /rootfs/usr/local/nginx \\ /rootfs/tmp \\ /rootfs/run # ============================================================================== # Stage 5: Final Distroless Image (FROM SCRATCH) # ============================================================================== FROM scratch # Re-declare build args for metadata ARG BUILD_DATE ARG GIT_COMMIT=unknown # OCI metadata labels LABEL org.opencontainers.image.title=\u0026#34;Vite React - Distroless\u0026#34; \\ org.opencontainers.image.description=\u0026#34;Distroless minimal image (\u0026lt;6MB) - UPX compressed\u0026#34; \\ org.opencontainers.image.version=\u0026#34;2.2.0-distroless-upx\u0026#34; \\ org.opencontainers.image.created=\u0026#34;${BUILD_DATE}\u0026#34; \\ org.opencontainers.image.revision=\u0026#34;${GIT_COMMIT}\u0026#34; \\ org.opencontainers.image.base.name=\u0026#34;scratch\u0026#34; \\ org.opencontainers.image.source=\u0026#34;https://github.com/riipandi/vite-react-template\u0026#34; \\ org.opencontainers.image.licenses=\u0026#34;MIT OR Apache-2.0\u0026#34; \\ maintainer=\u0026#34;contest-2025-optimized\u0026#34; # Copy static nginx binary and minimal filesystem COPY --from=nginx-builder /usr/local/nginx/sbin/nginx /usr/sbin/nginx COPY --from=rootfs /rootfs / # Run as non-root user (nobody = 65534) USER 65534:65534 EXPOSE 3000 # Lightweight healthcheck using nginx config test HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \\ CMD [\u0026#34;/usr/sbin/nginx\u0026#34;, \u0026#34;-t\u0026#34;, \u0026#34;-q\u0026#34;] STOPSIGNAL SIGTERM ENTRYPOINT [\u0026#34;/usr/sbin/nginx\u0026#34;] CMD [\u0026#34;-g\u0026#34;, \u0026#34;daemon off;\u0026#34;] 3. Dockerfile TOP 2 (React) ‚Äì Tr·∫ßn Qu·ªëc To√†n Technique Author\u0026rsquo;s explanation Reference Native C Healthcheck Write a C program that opens a socket to port 3000, returns exit code 0/1. Collect Shared Libraries Use ldd to copy the required libs for Nginx when running in scratch. Minimal Nginx Config Disable http_rewrite, http_proxy, mail_*\u0026hellip; because it only serves static. Non-root User Create a UID 101 user to run inside the image for better security. Dockerfile TOP 2 (React)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 # STAGE 1: The Builder (Custom Nginx Build) FROM alpine:3.19 AS builder # Multi-arch support ARG TARGETARCH RUN apk add --no-cache build-base pcre2-dev zlib-dev openssl-dev # Download and verify Nginx source with SHA256 checksum ARG NGINX_VERSION=1.27.0 ARG NGINX_SHA256=b7230e3cf87eaa2d4b0bc56aadc920a960c7873b9991a1b66ffcc08fc650129c ADD --checksum=sha256:${NGINX_SHA256} https://nginx.org/download/nginx-${NGINX_VERSION}.tar.gz /tmp/ RUN tar -xzf /tmp/nginx-${NGINX_VERSION}.tar.gz -C /tmp # Configure Nginx with minimal modules (static file serving only) RUN cd /tmp/nginx-${NGINX_VERSION} \u0026amp;\u0026amp; \\ ./configure \\ --prefix=/etc/nginx \\ --sbin-path=/usr/sbin/nginx \\ --conf-path=/etc/nginx/nginx.conf \\ --pid-path=/var/run/nginx.pid \\ --error-log-path=/dev/stderr \\ --http-log-path=/dev/stdout \\ --user=nginx \\ --group=nginx \\ --without-http_rewrite_module \\ --without-http_gzip_module \\ --without-http_proxy_module \\ --without-http_fastcgi_module \\ --without-http_uwsgi_module \\ --without-http_scgi_module \\ --without-mail_pop3_module \\ --without-mail_imap_module \\ --without-mail_smtp_module \u0026amp;\u0026amp; \\ make \u0026amp;\u0026amp; \\ make install \u0026amp;\u0026amp; rm -rf /tmp/* \u0026amp;\u0026amp; strip /usr/sbin/nginx # Create file Nginx main config (include snippet file) RUN { \\ mkdir -p /etc/nginx/conf.d \u0026amp;\u0026amp; \\ cat \u0026gt; /etc/nginx/nginx.conf; \\ } \u0026lt;\u0026lt;EOF events { worker_connections 1024; } http { include /etc/nginx/mime.types; client_body_temp_path /var/cache/nginx/client_body_temp; include /etc/nginx/conf.d/default.conf; } EOF RUN mkdir -p /etc/nginx/conf.d \u0026amp;\u0026amp; \\ cat \u0026gt; /etc/nginx/mime.types \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; types { text/html html; text/css css; application/javascript js; image/png png; application/json json; } EOF # Create minimal user/group files (no full /etc/passwd needed in scratch image) RUN echo \u0026#34;nginx:x:101:101:nginx:/var/cache/nginx:/sbin/nologin\u0026#34; \u0026gt; /etc/passwd \u0026amp;\u0026amp; \\ echo \u0026#34;nginx:x:101:\u0026#34; \u0026gt; /etc/group # Build static healthcheck binary (no external dependencies like wget/curl needed) RUN { \\ cat \u0026gt; /tmp/healthcheck.c \u0026amp;\u0026amp; \\ gcc -static -O2 -o /healthcheck /tmp/healthcheck.c; \\ } \u0026lt;\u0026lt;EOF #include \u0026lt;netdb.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;sys/socket.h\u0026gt; #include \u0026lt;netinet/in.h\u0026gt; int main() { struct hostent *h = gethostbyname(\u0026#34;localhost\u0026#34;); if (!h) return 1; int sock = socket(AF_INET, SOCK_STREAM, 0); if (sock \u0026lt; 0) return 1; struct sockaddr_in addr; addr.sin_family = AF_INET; addr.sin_port = htons(3000); addr.sin_addr = *(struct in_addr *)h-\u0026gt;h_addr_list[0]; int result = connect(sock, (struct sockaddr *)\u0026amp;addr, sizeof(addr)); close(sock); return (result == 0) ? 0 : 1; } EOF # Collect shared libraries required by Nginx (supports multi-arch) RUN mkdir -p /staging/lib /staging/usr/lib \u0026amp;\u0026amp; \\ ldd /usr/sbin/nginx | tr -s \u0026#39;[:space:]\u0026#39; \u0026#39;\\n\u0026#39; | grep \u0026#39;^/\u0026#39; | \\ xargs -I \u0026#39;{}\u0026#39; sh -c \u0026#39;mkdir -p /staging$(dirname {}) \u0026amp;\u0026amp; cp -L {} /staging$(dirname {})\u0026#39; # Create mount point directories for tmpfs (writable dirs in read-only container) RUN mkdir -p /var/cache/nginx /var/run /tmp \u0026amp;\u0026amp; chown -R 101:101 /var/cache/nginx /var/run /tmp # STAGE 2: App Builder (Build React App) FROM node:20.11.0-alpine3.19 AS app_builder WORKDIR /app RUN corepack enable \u0026amp;\u0026amp; corepack prepare pnpm@9.12.2 --activate # Copy dependencies files first for caching COPY package.json pnpm-lock.yaml ./ RUN --mount=type=cache,target=/root/.local/share/pnpm/store,sharing=locked \\ pnpm install --frozen-lockfile --prefer-offline # Copy config files COPY tsconfig.json tsconfig.node.json vite.config.ts ./ COPY postcss.config.js tailwind.config.ts index.html ./ # Copy source code (avoid copying tests, stories, etc.) COPY public/ ./public/ COPY src/ ./src/ RUN pnpm build \u0026amp;\u0026amp; \\ test -f dist/index.html || (echo \u0026#34;Build failed\u0026#34; \u0026amp;\u0026amp; exit 1) # STAGE 3: Production Image (FROM scratch) FROM scratch LABEL org.opencontainers.image.title=\u0026#34;Vite React Template\u0026#34; \\ org.opencontainers.image.description=\u0026#34;Minimal Vite React SPA with custom Nginx built from scratch\u0026#34; \\ org.opencontainers.image.version=\u0026#34;1.0.0\u0026#34; \\ org.opencontainers.image.licenses=\u0026#34;MIT\u0026#34; \\ org.opencontainers.image.base.name=\u0026#34;scratch\u0026#34; \\ org.opencontainers.image.authors=\u0026#34;Contest 2025\u0026#34; # Copy file user/group COPY --from=builder /etc/passwd /etc/group /etc/ # Copy mount points to image COPY --chown=101:101 --from=builder /var/cache/nginx /var/cache/nginx COPY --chown=101:101 --from=builder /var/run /var/run COPY --chown=101:101 --from=builder /tmp /tmp # Copy Nginx and shared libraries COPY --from=builder /usr/sbin/nginx /usr/sbin/nginx COPY --from=builder /staging/ / # Copy main Nginx config (just created) COPY --from=builder /etc/nginx/nginx.conf /etc/nginx/nginx.conf # Copy file config from project into included location COPY nginx.conf /etc/nginx/conf.d/default.conf # Copy remaining necessary files COPY --from=builder /etc/nginx/mime.types /etc/nginx/mime.types COPY --from=app_builder /app/dist /usr/share/nginx/html COPY --from=builder /healthcheck /healthcheck USER 101:101 EXPOSE 3000 HEALTHCHECK --interval=30s --timeout=3s --retries=3 \\ CMD [\u0026#34;/healthcheck\u0026#34;] CMD [\u0026#34;/usr/sbin/nginx\u0026#34;, \u0026#34;-g\u0026#34;, \u0026#34;daemon off;\u0026#34;] 4. Dockerfile TOP 3 (React) ‚Äì Go + FastHTTP Technique Explanation Reference Go Server (FastHTTP) Use github.com/valyala/fasthttp instead of Nginx. Asset Embedding //go:embed dist to embed all assets into the binary. Maximum Binary Compression Build statically, strip, then upx --ultra-brute --lzma. Integrated Healthcheck The Go binary handles it when run with -health. Dockerfile TOP 3 (React)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 # Stage 1: Build frontend FROM node:20-alpine AS builder RUN corepack enable pnpm WORKDIR /app COPY package.json pnpm-lock.yaml ./ RUN pnpm install --frozen-lockfile COPY tsconfig*.json vite.config.ts postcss.config.js tailwind.config.ts ./ COPY index.html ./ COPY src ./src COPY public ./public RUN pnpm build # Clean up dist (remove source maps, licenses, stats, robots.txt, etc.) RUN find /app/dist -type f -name \u0026#39;*.map\u0026#39; -delete \\ \u0026amp;\u0026amp; find /app/dist -type f -name \u0026#39;*.LICENSE.*\u0026#39; -delete \\ \u0026amp;\u0026amp; find /app/dist -type f -name \u0026#39;*.txt\u0026#39; -delete \\ \u0026amp;\u0026amp; find /app/dist -type f -name \u0026#39;stats.html\u0026#39; -delete \\ \u0026amp;\u0026amp; find /app/dist -type f -name \u0026#39;robots.txt\u0026#39; -delete \\ \u0026amp;\u0026amp; find /app/dist -type f -name \u0026#39;_redirects\u0026#39; -delete # Stage 2: Build Go server with FastHTTP FROM golang:1.21-alpine AS go-builder WORKDIR /app # C√†i strip v√† upx RUN apk add --no-cache binutils upx # Copy dist files ƒë·ªÉ embed COPY --from=builder /app/dist ./dist # T·∫°o go.mod v√† main.go v·ªõi FastHTTP RUN cat \u0026gt; go.mod \u0026lt;\u0026lt; \u0026#39;GOMOD\u0026#39; module server go 1.21 require github.com/valyala/fasthttp v1.51.0 GOMOD RUN cat \u0026gt; main.go \u0026lt;\u0026lt; \u0026#39;GOSRC\u0026#39; package main import ( \u0026#34;embed\u0026#34; \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; \u0026#34;path\u0026#34; \u0026#34;strings\u0026#34; \u0026#34;github.com/valyala/fasthttp\u0026#34; ) //go:embed dist var distFiles embed.FS func main() { port := os.Getenv(\u0026#34;PORT\u0026#34;) if port == \u0026#34;\u0026#34; { port = \u0026#34;3000\u0026#34; } // FastHTTP handler handler := func(ctx *fasthttp.RequestCtx) { pathStr := string(ctx.Path()) // Check if it\u0026#39;s a static asset (has file extension) if strings.Contains(path.Base(pathStr), \u0026#34;.\u0026#34;) { // Try to serve static file file, err := distFiles.Open(\u0026#34;dist\u0026#34; + pathStr) if err == nil { defer file.Close() // Set content type based on extension ext := path.Ext(pathStr) switch ext { case \u0026#34;.js\u0026#34;: ctx.SetContentType(\u0026#34;application/javascript\u0026#34;) case \u0026#34;.css\u0026#34;: ctx.SetContentType(\u0026#34;text/css\u0026#34;) case \u0026#34;.svg\u0026#34;: ctx.SetContentType(\u0026#34;image/svg+xml\u0026#34;) case \u0026#34;.png\u0026#34;: ctx.SetContentType(\u0026#34;image/png\u0026#34;) case \u0026#34;.jpg\u0026#34;, \u0026#34;.jpeg\u0026#34;: ctx.SetContentType(\u0026#34;image/jpeg\u0026#34;) case \u0026#34;.ico\u0026#34;: ctx.SetContentType(\u0026#34;image/x-icon\u0026#34;) default: ctx.SetContentType(\u0026#34;application/octet-stream\u0026#34;) } // Copy file content to response ctx.Response.SetBodyStream(file, -1) return } } // SPA fallback - serve index.html for all routes file, err := distFiles.Open(\u0026#34;dist/index.html\u0026#34;) if err != nil { ctx.SetStatusCode(404) ctx.SetBodyString(\u0026#34;Not Found\u0026#34;) return } defer file.Close() ctx.SetContentType(\u0026#34;text/html; charset=utf-8\u0026#34;) ctx.Response.SetBodyStream(file, -1) } // healthcheck if len(os.Args) \u0026gt; 1 \u0026amp;\u0026amp; os.Args[1] == \u0026#34;-health\u0026#34; { _, _, err := fasthttp.Get(nil, \u0026#34;http://127.0.0.1:\u0026#34;+port) if err != nil { os.Exit(1) } os.Exit(0) } log.Printf(\u0026#34;FastHTTP server on :%s\u0026#34;, port) log.Fatal(fasthttp.ListenAndServe(\u0026#34;:\u0026#34;+port, handler)) } GOSRC # Download dependencies v√† build v·ªõi t·ªëi ∆∞u extreme RUN go mod tidy RUN CGO_ENABLED=0 GOOS=linux go build -ldflags=\u0026#34;-s -w\u0026#34; -trimpath -o server main.go RUN strip server RUN upx --ultra-brute --lzma server # Stage 3: Ultra-minimal runtime (scratch) FROM scratch # Copy binary (ch·ª©a c·∫£ static files) COPY --from=go-builder /app/server /server EXPOSE 3000 # Th√™m USER 1000 USER 1000 HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \\ CMD [\u0026#34;/server\u0026#34;, \u0026#34;-health\u0026#34;] CMD [\u0026#34;/server\u0026#34;] 5. Dockerfile TOP (React) ‚Äì Self-compiled Nginx on Alpine Technique Explanation Reference Compiled Nginx Build Nginx 1.27.3 from source, enable modules needed for SPA (gzip_static, ssl). Pre-compression gzip -k -9 for assets, serve via gzip_static on. Inline Security Headers Add X-Frame-Options, X-Content-Type-Options, X-XSS-Protection. Healthcheck Use wget to check HTTP 200 on port 3000. Dockerfile TOP (React)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 # ========================= # Giai ƒëo·∫°n 1: Node base # ========================= # M√¥i tr∆∞·ªùng build cho Vite/React: ch·ªâ c√†i nh·ªØng th·ª© t·ªëi thi·ªÉu ƒë·ªÉ bi√™n d·ªãch FROM alpine:3.20@sha256:765942a4039992336de8dd5db680586e1a206607dd06170ff0a37267a9e01958 AS node-base RUN apk add --no-cache nodejs npm git python3 g++ make \u0026amp;\u0026amp; \\ npm install -g pnpm@9.12.2 \u0026amp;\u0026amp; npm cache clean --force # ========================= # Giai ƒëo·∫°n 2: Builder # ========================= # D√πng cache mount cho pnpm ƒë·ªÉ tƒÉng t·ªëc build l·∫°i; x√≥a source map; n√©n gzip s·∫µn FROM node-base AS builder WORKDIR /app COPY package.json pnpm-lock.yaml ./ RUN --mount=type=cache,id=pnpm-custom,target=/root/.local/share/pnpm/store \\ pnpm install --frozen-lockfile --prefer-offline # Sao ch√©p c·∫•u h√¨nh \u0026amp; m√£ ngu·ªìn COPY index.html ./ COPY vite.config.ts tsconfig.json tsconfig.node.json ./ COPY postcss.config.js tailwind.config.ts ./ COPY public ./public COPY src ./src # Build \u0026amp; t·ªëi ∆∞u artefact tƒ©nh RUN pnpm run build \u0026amp;\u0026amp; \\ find /app/dist -name \u0026#34;*.map\u0026#34; -type f -delete || true \u0026amp;\u0026amp; \\ find /app/dist -type f \\( -name \u0026#39;*.html\u0026#39; -o -name \u0026#39;*.js\u0026#39; -o -name \u0026#39;*.css\u0026#39; -o -name \u0026#39;*.svg\u0026#39; -o -name \u0026#39;*.json\u0026#39; \\) \\ -exec gzip -k -9 {} \\; # ====================================== # Giai ƒëo·∫°n 3: Nginx builder (t·ª± bi√™n d·ªãch) # ====================================== # Bi√™n d·ªãch nginx 1.27.3 v·ªõi module t·ªëi thi·ªÉu cho SPA + n√©n tƒ©nh FROM alpine:3.20@sha256:765942a4039992336de8dd5db680586e1a206607dd06170ff0a37267a9e01958 AS nginx-builder RUN apk add --no-cache --virtual .build-deps gcc libc-dev make pcre2-dev zlib-dev openssl-dev linux-headers \u0026amp;\u0026amp; \\ wget -O /tmp/nginx.tar.gz https://nginx.org/download/nginx-1.27.3.tar.gz \u0026amp;\u0026amp; \\ tar -xzf /tmp/nginx.tar.gz -C /tmp \u0026amp;\u0026amp; cd /tmp/nginx-1.27.3 \u0026amp;\u0026amp; \\ ./configure \\ --prefix=/etc/nginx \\ --sbin-path=/usr/sbin/nginx \\ --modules-path=/usr/lib/nginx/modules \\ --conf-path=/etc/nginx/nginx.conf \\ --error-log-path=/var/log/nginx/error.log \\ --http-log-path=/var/log/nginx/access.log \\ --pid-path=/var/run/nginx.pid \\ --lock-path=/var/run/nginx.lock \\ --http-client-body-temp-path=/var/cache/nginx/client_temp \\ --http-proxy-temp-path=/var/cache/nginx/proxy_temp \\ --user=nginx --group=nginx \\ --with-http_ssl_module --with-http_v2_module \\ --with-http_gzip_static_module --with-http_stub_status_module \\ --with-threads --with-file-aio \\ --without-http_autoindex_module --without-http_browser_module \\ --without-http_geo_module --without-http_map_module \\ --without-http_memcached_module --without-http_userid_module \\ --without-mail_pop3_module --without-mail_imap_module \\ --without-mail_smtp_module --without-http_split_clients_module \\ --without-http_uwsgi_module --without-http_scgi_module \\ --without-http_grpc_module \u0026amp;\u0026amp; \\ make -j$(nproc) \u0026amp;\u0026amp; make install \u0026amp;\u0026amp; strip /usr/sbin/nginx \u0026amp;\u0026amp; \\ rm -rf /tmp/nginx* \u0026amp;\u0026amp; apk del .build-deps # ====================================== # Giai ƒëo·∫°n 4: Runtime base t·ªëi gi·∫£n # ====================================== # Ch·ªâ gi·ªØ runtime deps; t·∫°o user non-root; chu·∫©n b·ªã th∆∞ m·ª•c v√† quy·ªÅn FROM alpine:3.20@sha256:765942a4039992336de8dd5db680586e1a206607dd06170ff0a37267a9e01958 AS custom-runtime-base RUN apk add --no-cache pcre2 zlib openssl tzdata \u0026amp;\u0026amp; \\ addgroup -g 101 -S nginx \u0026amp;\u0026amp; \\ adduser -S -D -H -u 101 -h /var/cache/nginx -s /sbin/nologin -G nginx -g nginx nginx \u0026amp;\u0026amp; \\ mkdir -p /var/cache/nginx /var/log/nginx /etc/nginx/conf.d /usr/share/nginx/html \u0026amp;\u0026amp; \\ chown -R nginx:nginx /var/cache/nginx /var/log/nginx /usr/share/nginx/html COPY --from=nginx-builder /usr/sbin/nginx /usr/sbin/nginx COPY --from=nginx-builder /etc/nginx /etc/nginx # ====================================== # Giai ƒëo·∫°n 5: Runtime cu·ªëi # ====================================== FROM custom-runtime-base AS runtime LABEL org.opencontainers.image.title=\u0026#34;SvnFrs-Dockerfile_Contest_2025\u0026#34; \\ org.opencontainers.image.description=\u0026#34;SPA production tr√™n Alpine v·ªõi Nginx t·ª± bi√™n d·ªãch, t·ªëi ∆∞u v√† b·∫£o m·∫≠t\u0026#34; \\ org.opencontainers.image.version=\u0026#34;1.0.0\u0026#34; \\ org.opencontainers.image.licenses=\u0026#34;MIT\u0026#34; \\ org.opencontainers.image.created=\u0026#34;2025-10-28\u0026#34; \\ org.opencontainers.image.base.name=\u0026#34;alpine:3.20\u0026#34; # ·ª®ng d·ª•ng tƒ©nh ƒë√£ build COPY --from=builder --chown=nginx:nginx /app/dist /usr/share/nginx/html # C·∫•u h√¨nh Nginx t·ªëi thi·ªÉu (inline) b·∫≠t gzip_static ƒë·ªÉ ph·ª•c v·ª• file .gz RUN echo \u0026#39;server { \\ listen 3000; server_name localhost; \\ root /usr/share/nginx/html; index index.html; \\ gzip_static on; gzip_vary on; \\ location / { try_files $uri $uri/ /index.html; expires 1y; add_header Cache-Control \u0026#34;public, immutable\u0026#34;; } \\ location = /index.html { expires -1; add_header Cache-Control \u0026#34;no-cache\u0026#34;; } \\ add_header X-Frame-Options \u0026#34;SAMEORIGIN\u0026#34; always; \\ add_header X-Content-Type-Options \u0026#34;nosniff\u0026#34; always; \\ add_header X-XSS-Protection \u0026#34;1; mode=block\u0026#34; always; \\ }\u0026#39; \u0026gt; /etc/nginx/conf.d/default.conf \u0026amp;\u0026amp; \\ echo \u0026#39;worker_processes auto; error_log /var/log/nginx/error.log warn; pid /var/run/nginx.pid; \\ events { worker_connections 1024; } http { include /etc/nginx/mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; include /etc/nginx/conf.d/*.conf; }\u0026#39; \u0026gt; /etc/nginx/nginx.conf \u0026amp;\u0026amp; \\ echo \u0026#39;types { \\ text/html html htm shtml; text/css css; text/javascript js; application/json json; \\ image/svg+xml svg svgz; image/x-icon ico; image/png png; image/jpeg jpeg jpg; \\ font/woff2 woff2; application/wasm wasm; \\ }\u0026#39; \u0026gt; /etc/nginx/mime.types # Th∆∞ m·ª•c t·∫°m/ng·∫ßm c·ªßa Nginx + ph√¢n quy·ªÅn tr∆∞·ªõc khi chuy·ªÉn USER RUN mkdir -p /var/cache/nginx/client_temp \\ /var/cache/nginx/proxy_temp \\ /etc/nginx/fastcgi_temp \\ /etc/nginx/proxy_temp \\ /etc/nginx/client_body_temp \\ /etc/nginx/uwsgi_temp \\ /etc/nginx/scgi_temp \u0026amp;\u0026amp; \\ chown -R nginx:nginx /var/cache/nginx \\ /var/log/nginx \\ /usr/share/nginx/html \\ /etc/nginx/fastcgi_temp \\ /etc/nginx/proxy_temp \\ /etc/nginx/client_body_temp \\ /etc/nginx/uwsgi_temp \\ /etc/nginx/scgi_temp \u0026amp;\u0026amp; \\ chmod -R 755 /var/cache/nginx \\ /etc/nginx/fastcgi_temp \\ /etc/nginx/proxy_temp \\ /etc/nginx/client_body_temp \\ /etc/nginx/uwsgi_temp \\ /etc/nginx/scgi_temp \u0026amp;\u0026amp; \\ touch /var/run/nginx.pid \u0026amp;\u0026amp; \\ chown nginx:nginx /var/run/nginx.pid # Healthcheck r·∫ª: HTTP 200 ·ªü c·ªïng 3000 RUN apk add --no-cache wget HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \\ CMD wget --quiet --tries=1 --spider http://localhost:3000/ || exit 1 EXPOSE 3000 STOPSIGNAL SIGQUIT USER nginx CMD [\u0026#34;nginx\u0026#34;, \u0026#34;-g\u0026#34;, \u0026#34;daemon off;\u0026#34;] Deployment Notes The Dockerfiles above are architectural references only; when applying them to your Node.js/React project, keep the core principles: multi-stage build, pre-compress, non-root, pin SHA. ","date":"2025-11-18T00:00:00Z","image":"https://tech.nguuyen.io.vn/images/docker-optimization/docker-optimization-react.webp","permalink":"https://tech.nguuyen.io.vn/en/posts/docker-optimization/docker-opt-nodejs/","title":"Optimize Docker for Node.js"},{"content":"üìå Introduction Nginx is an open-source web server that can serve static and dynamic web applications. It can run as a web server, load balancer, reverse proxy, or HTTP cache, and integrates with existing applications to build complete systems or distribute web apps via IP or domain.\nThis guide shows how to install Nginx on Ubuntu 24.04 and set up a sample web app on your server.\n‚úÖ Prerequisites Before you begin, make sure you:\nüöÄ Provision an Ubuntu 24.04 server. üåç Create an A record pointing your domain or subdomain to the server IP (e.g., app.example.com). üîê Access the server via SSH and create a non-root sudo user. üîÑ Update the system packages. ‚öôÔ∏è Install NGINX on Ubuntu 24.04 The latest NGINX package is available in Ubuntu 24.04\u0026rsquo;s default APT repository. Follow these steps to update the system and install NGINX.\nüîÑ Update package lists\n1 sudo apt update üì¶ Install NGINX\n1 sudo apt install nginx -y üîç Check installed NGINX version\n1 sudo nginx -version Sample output:\n1 nginx version: nginx/1.24.0 (Ubuntu) ‚öôÔ∏è Manage the NGINX service NGINX uses the nginx systemd service to manage runtime and processes. Use the commands below to enable and manage the service.\nüöÄ Enable NGINX to start on boot\n1 sudo systemctl enable nginx Result:\n1 2 Synchronizing state of nginx.service with SysV service script with /usr/lib/systemd/systemd-sysv-install. Executing: /usr/lib/systemd/systemd-sysv-install enable nginx ‚ñ∂Ô∏è Start NGINX\n1 sudo systemctl start nginx ‚èπÔ∏è Stop NGINX\n1 sudo systemctl stop nginx üîÑ Restart NGINX\n1 sudo systemctl restart nginx üîç Check NGINX status\n1 sudo systemctl status nginx Sample output:\n1 2 3 4 5 6 7 8 9 10 11 12 13 ‚óè nginx.service - A high performance web server and a reverse proxy server Loaded: loaded (/usr/lib/systemd/system/nginx.service; enabled; preset: enabled) Active: active (running) since Wed 2024-06-26 10:55:50 UTC; 1min 0s ago Docs: man:nginx(8) Process: 2397 ExecStartPre=/usr/sbin/nginx -t -q -g daemon on; master_process on; (code=exited, status=0/SUCCESS) Process: 2399 ExecStart=/usr/sbin/nginx -g daemon on; master_process on; (code=exited, status=0/SUCCESS) Main PID: 2400 (nginx) Tasks: 2 (limit: 1068) Memory: 1.7M (peak: 2.4M) CPU: 13ms CGroup: /system.slice/nginx.service ‚îú‚îÄ2400 \u0026#34;nginx: master process /usr/sbin/nginx -g daemon on; master_process on;\u0026#34; ‚îî‚îÄ2401 \u0026#34;nginx: worker process\u0026#34; If Active: active (running) appears, NGINX is running. If you see Active: active (failed), stop any process using HTTP port 80, then restart NGINX. üåê Create an Nginx virtual host An Nginx virtual host serves web app files from a specific directory using a domain name. Follow these steps to set up a sample virtual host securely.\nüìÇ Create a new virtual host config file\nIn /etc/nginx/sites-available, create a new config file, for example: app.example.com.conf. 1 $ sudo nano /etc/nginx/sites-available/app.example.com.conf Add the following configuration: 1 2 3 4 5 6 7 8 9 10 11 12 13 server { listen 80; listen [::]:80; server_name app.example.com; root /var/www/app.example.com; index index.html; location / { try_files $uri $uri/ =404; } } Save and close the file. ‚úÖ Test Nginx configuration\nCheck the configuration for errors: 1 $ sudo nginx -t Result: 1 2 nginx: the configuration file /etc/nginx/nginx.conf syntax is ok nginx: configuration file /etc/nginx/nginx.conf test is successful üîó Enable the virtual host\nLink the config file into /etc/nginx/sites-enabled: 1 $ sudo ln -s /etc/nginx/sites-available/app.example.com.conf /etc/nginx/sites-enabled/ üìÅ Create the web root directory\n1 $ sudo mkdir -p /var/www/app.example.com üìù Create a sample HTML file\nCreate index.html in the web root: 1 $ sudo nano /var/www/app.example.com/index.html Add the following content: 1 2 3 4 5 6 \u0026lt;html\u0026gt; \u0026lt;head\u0026gt;\u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Hello Hoang Duong\u0026lt;/h1\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Save and close the file. üîÑ Restart Nginx\n1 $ sudo systemctl restart nginx üåç Verify the virtual host\nUse curl to test: 1 $ curl http://app.example.com Result: 1 Hello Hoang Duong üîí Secure the Nginx web server SSL certificates encrypt communication between the browser and your server over HTTPS. By default, Nginx listens on insecure HTTP port 80. Follow these steps to obtain a trusted SSL certificate from Let\u0026rsquo;s Encrypt and enable HTTPS.\nInstall Certbot ‚Äì Let\u0026rsquo;s Encrypt client üîß\nInstall Certbot using Snap: 1 $ sudo snap install --classic certbot Verify the Certbot version: 1 $ sudo certbot --version üìå Sample output:\n1 certbot 2.11.0 üõ°Ô∏è Generate an SSL certificate for Nginx Create a new SSL certificate for your domain. Replace app.example.com with your real domain: 1 $ sudo certbot --nginx -d app.example.com --agree-tos This will:\n‚úÖ Generate a valid SSL certificate ‚úÖ Automatically configure Nginx to use SSL ‚úÖ Enable HTTPS on your web server Test automatic renewal üîÑ\nLet\u0026rsquo;s Encrypt certificates last 90 days. Check auto-renewal with: 1 $ sudo certbot renew --dry-run If no errors appear, the certificate will renew automatically.\nüåê Verify the site with HTTPS Open the browser and visit: 1 https://app.example.com If you see the lock icon üîí, SSL is installed successfully! üöÄ\nüî• Configure UFW firewall rules Uncomplicated Firewall (UFW) is installed and enabled by default on Ubuntu 24.04. Follow these steps to allow HTTP and HTTPS traffic.\nAllow HTTP (Port 80) üîå\nRun: 1 $ sudo ufw allow 80/tcp Allow HTTPS (Port 443) üîê\nRun: 1 $ sudo ufw allow 443/tcp Check firewall status üî¢\nRun: 1 $ sudo ufw status üëâ Sample output:\n1 2 3 4 5 6 7 8 9 10 Status: active To Action From -- ------ ---- 22/tcp ALLOW Anywhere 80/tcp ALLOW Anywhere 443/tcp ALLOW Anywhere 22/tcp (v6) ALLOW Anywhere (v6) 80/tcp (v6) ALLOW Anywhere (v6) 443/tcp (v6) ALLOW Anywhere (v6) üõ°Ô∏è After setup, the server only allows HTTP and HTTPS connections.\nüéØ Conclusion Congratulations on installing Nginx on Ubuntu 24.04 and configuring a web server for your applications. Nginx supports multiple virtual hosts to deploy applications securely. You can also integrate Nginx with MySQL and PHP to build dynamic web apps. For more configuration options, visit the official Nginx documentation. üöÄüòä\n","date":"2025-03-10T00:00:00Z","image":"https://tech.nguuyen.io.vn/images/website/how-to-install-nginx.webp","permalink":"https://tech.nguuyen.io.vn/en/posts/installation-guides/how-to-install-nginx/","title":"How to Install Nginx on Ubuntu 24.04 ‚öôÔ∏è"},{"content":"üîÄ What is Nginx? NGINX is a reliable open-source web server\nNginx is an open-source web server that uses an asynchronous, event-driven architecture. It was originally built for HTTP caching, later expanded to support reverse proxy, HTTP load balancing, and mail protocols such as IMAP4, POP3, and SMTP.\nReleased in October 2014, Nginx is used by large companies like Google, Adobe, Netflix, and WordPress because it can handle thousands of concurrent connections.\n‚öôÔ∏è How Nginx works NGINX works similarly to other servers\nNginx follows an asynchronous processing model, unlike the sequential processing used by traditional web servers.\nEach process has multiple worker connections to handle requests. Worker connections pass requests to worker processes, which forward them to the master process. With this model, one worker connection can handle up to 1024 requests at once, enabling Nginx to serve thousands of requests efficiently. üî• Nginx features NGINX offers powerful features for web development\nNginx provides many standout features:\n‚ö° Handles over 10,000 concurrent connections with low memory usage. üìÇ Serves static files and file indexing. üîÑ Load balancing and reverse proxy with caching. üöÄ Supports FastCGI, uWSGI, SCGI, and Memcached. üõ†Ô∏è Modular architecture with automatic gzip compression. üîê SSL/TLS encryption support. üîÄ URL rewrites using regular expressions. üåê WebSocket support and connection limits. üì° IPv6 compatibility. ‚öñÔ∏è Nginx vs Apache Compared to Apache, NGINX has many advantages\nüñ•Ô∏è Apache server: Processes requests using a forked threaded model or keep-alive. Handles both static and dynamic content. üåç Nginx server: Uses a non-blocking event loop. Serves static content more efficiently than Apache. Faster request handling and better resource usage. Requires a separate processor for dynamic content. üîé How to check if a website uses Nginx You can use available tools to check if a website runs Nginx\nYou can check if a website runs Nginx by inspecting HTTP headers:\nOpen the website in Chrome. Press Ctrl + Shift + I or F12 to open DevTools. Switch to the Network tab. Select any request and inspect Headers. You can also use tools like Pingdom or GTmetrix to check.\nüéØ Conclusion Nginx has become one of the most popular web servers thanks to high performance, the ability to handle thousands of concurrent connections, and powerful features. Whether you need to serve static content, balance load, or run a reverse proxy, Nginx is a strong choice. Hopefully this guide helps you understand Nginx and how it works. If you are considering an optimized web server for your project, try Nginx and explore its benefits! üöÄüòä\n","date":"2025-03-10T00:00:00Z","image":"https://tech.nguuyen.io.vn/images/website/nginx-introduction-guide.webp","permalink":"https://tech.nguuyen.io.vn/en/posts/website/nginx-introduction-guide/","title":"Introduction to Nginx üöÄ"},{"content":"üåê Why point a domain to hosting? Domain and hosting are the two essentials that keep a website online.\nDomain is the website address. Hosting stores the website data. Why you need to point a domain Connect the domain to hosting so the site appears on the internet. Let people access the site using a memorable address. How it works üîß Get nameserver or IP information from your hosting provider. Configure it in your domain management dashboard. Three simplest ways to point a domain to hosting üöÄ Point using nameservers Steps:\nLog in to your domain management panel. Select the domain you want to point. Select the domain to point to hosting\nReplace the default nameservers with the hosting provider\u0026rsquo;s nameservers (found in the hosting welcome email). Replace default nameservers with your hosting nameservers\nWait for DNS propagation (minutes to hours, up to 24h). Verify by visiting the domain. Point using IP (A record) Steps:\nLog in to your domain management panel. Select the domain and open DNS settings\nOpen the DNS settings. Add two A records: @ ‚Üí Hosting IP (from the provider email). www ‚Üí Hosting IP. Add two A records\nSave changes and wait for DNS propagation (minutes to hours). Point using an intermediate nameserver To point a domain via an intermediate nameserver such as Cloudflare, Namecheap FreeDNS, or Incapsula, follow these steps:\nSteps:\nCreate an account with the intermediate nameserver and confirm required DNS records. In your domain registrar, change the nameservers to the intermediate provider. Wait for propagation. Common issues when pointing a domain to hosting ‚ö†Ô∏è You may run into issues that prevent your site from working correctly. Understanding these will help you troubleshoot faster.\nüö´ Wrong hosting domain When registering hosting, the provider asks for your domain. If it is incorrect or mismatched, the website may not work.\nüö´ Using both methods at once If you already pointed a domain via nameservers, do not switch to IP (A record) at the same time. It can cause DNS conflicts.\nüö´ Incorrect DNS record type Common DNS record types include:\nA record: Points the domain to the hosting IP. CNAME: Points one domain to another. MX record: Configures email. Entering the wrong record type can break connectivity.\nüö´ Wrong hosting IP Hosting IPs are long and easy to mistype. Copy directly from the provider email to avoid confusion with a personal IP.\nIf errors occur, double-check your settings and wait for DNS updates to finish.\nVideo guide üé• For a visual walkthrough, watch the video below. It covers steps from getting nameserver details to updating settings in your domain manager.\n‚Äï Source, F8 Official ‚úîÔ∏è\nConclusion üåü Pointing a domain to hosting is essential for your website to work online. Whether you use nameservers, A records, or an intermediate nameserver, double-check your settings and allow time for DNS propagation. If you encounter issues, review the configuration and try again üîÑ. Good luck, and feel free to leave a comment if you need help! üí¨üòä\n","date":"2025-02-27T00:00:00Z","image":"https://tech.nguuyen.io.vn/images/website/domain-to-hosting-setup.webp","permalink":"https://tech.nguuyen.io.vn/en/posts/website/domain-to-hosting-setup/","title":"Point a Domain to Hosting üåê"},{"content":"How to upload a website to hosting via File Manager and cPanel from A to Z Uploading a website to hosting is an essential step to bring your site online. It is not just about transferring files but also ensuring everything runs smoothly and securely. This guide walks you through the steps, from preparing files and databases to using the right upload tools.\nKey points to remember Before you start uploading, keep these essentials in mind:\n‚úÖ What to prepare before uploading: Website files, database, control panel access, and an FTP client like FileZilla.\n‚úÖ Ways to upload: Choose a reliable host, then use File Manager, FTP, WordPress plugins, SSH, or provider support.\n‚úÖ Why upload to hosting: Makes your site public, improves security, boosts speed, and simplifies management.\n‚úÖ Four steps to upload a website:\nUpload website files to public_html via File Manager or FTP. Verify files are in the correct location. Create a database in cPanel, import via phpMyAdmin, and configure the connection. Visit the website to confirm everything works. What do you need before uploading a website? Prepare the following:\nWebsite files: These can be the contents of your old public_html folder, source code, or the latest backup. üìå Note for React/Vue.js deployments\nWhen building a React or Vue.js app, you do not upload the full source code, only the static build output.\nüîπ Build and zip steps:\nRun the build command: React: 1 npm run build Vue.js: 1 npm run build The output folder is usually build/ (React) or dist/ (Vue.js). Zip the build or dist folder before uploading. üì§ Uploading to the server:\nWith cPanel, extract directly in public_html. With SSH/SFTP, upload and unzip using: 1 unzip build.zip -d /var/www/html/ Ensure your Apache/Nginx config points to the folder containing index.html. Database file (if applicable). Control panel access for the new hosting account. FTP client such as FileZilla. To upload, access your hosting control panel via an FTP client like FileZilla. If your site already exists, use the CMS or cPanel backup feature to download it and upload to the new host.\nChoose a reliable hosting provider Quality hosting affects both speed and long-term performance. Choose carefully based on:\nLive support: 24/7 customer support for quick issue resolution. Account control: Full access to cPanel or equivalent tools. Scalability: Ability to upgrade storage, bandwidth, or domains. Clear refund policy: Flexible refunds for testing services. Free add-ons: SSL, scheduled backups, or free migrations. ICANN-accredited domains: Protect your brand with trusted domain registration. Note: Review provider reputation and user feedback on forums or review sites before deciding.\nChoose a website upload method There are five main ways to upload a website depending on your tools and needs:\nUse File Manager File Manager is a web-based file tool built into cPanel or other hosting dashboards.\nPros:\nFree. Easy to use. Cons:\nUpload size limits (usually 256 MB). Can only extract archives within that size limit. Note: For larger files, use FTP and unzip via SSH.\nFigure 1: Access File Manager\nUse FTP FTP (File Transfer Protocol) lets you upload files using an FTP client like FileZilla. This is efficient and has no upload size limit.\nPros:\nNo upload size limits. Faster for large files. Cons:\nRequires an FTP client. Requires FTP credentials from your host. Lower security, use SFTP for better protection. Figure 2: FTP details\nUse WordPress migration plugins If you use WordPress, plugins like All in One WP Migration can move your entire site automatically.\nPros:\nEasy to use with minimal technical knowledge. Simple drag and drop workflow. Cons:\nUpload size limits (usually 256 MB). For larger files, use FTP + SSH. Note: After upload, move files out of subfolders into public_html so the site loads correctly.\nFigure 3: Use WordPress Migration Plugin\nUse SSH (Secure Shell) SSH lets you upload and manage files via command line, ideal for large files or high speed.\nPros:\nFast with no size limits. Extract archives directly on the server. Cons:\nRequires basic command-line skills and SSH access. Figure 4: Upload via SSH\nUse Import Site (automatic importer) Some hosting providers offer an Import Site tool that uploads and extracts a website into public_html.\nPros:\nQuick and easy upload. Minimal technical effort. Cons:\nDepends on whether your host supports the tool. Tip: Ask your hosting provider if they support site import.\nFigure 5: Import Site tool\nAsk hosting support Most providers offer migration help, especially if you are switching hosts.\nPros:\nSaves time and avoids technical mistakes. Done by experts. Cons:\nMay be a paid service. Depends on support availability. Tip: Check the provider\u0026rsquo;s migration policy before requesting support.\nFigure 6: Ask hosting support\nüìå Notes after uploading\nMove all data from subfolders into public_html so the site loads correctly. Check file structure and database connections to avoid runtime errors. Choose the best method based on file size, platform, and your skills. Why you should upload a website to hosting Uploading to hosting is required to make your site public, but it also brings major benefits in security, performance, and reliability:\nPublish your website globally When hosted, your website becomes accessible worldwide via your domain name. Otherwise, only your local machine can access it.\nSecurity and data safety Professional hosting provides strong security measures such as SSL certificates, firewalls, and data protection tools. This keeps your site and brand data safe from attacks and breaches.\nSpeed and performance Hosting servers are optimized for high traffic and fast internet speeds, improving page load times and user experience while reducing lag.\nFigure 7: Speed and performance\nStability and reliability Hosting providers manage the hardware and network, ensuring high uptime so your website stays available without long downtime.\nEasy management Hosting dashboards make it easy to back up data, upgrade software, and optimize performance.\nScalability and long-term growth Hosting services scale with your traffic and resource needs, unlike personal machines that struggle with growth.\nFigure 8: Scalability and growth\nIn short, hosting your website improves security, performance, stability, and manageability. It is the foundation for sustainable growth and easy global access.\nStep 1: Choose how to upload website files Method 1: Upload via File Manager in cPanel Method 2: Upload via FTP client After you choose a method, follow the guide below.\n\u0026mdash;Method 1: Upload via File Manager in cPanel\u0026mdash; (Easiest)\nAccess cPanel and follow these steps:\nStep 1: Click File Manager under the Files section.\nChoose File Manager Illustration: Choose File Manager\nChoose File Manager\nStep 2: In File Manager, open the public_html folder.\nOpen public_html Illustration: Open public_html\nOpen public_html\nStep 3: Click Upload inside public_html.\nClick Upload Illustration: Click Upload\nClick Upload\nStep 4: Use Select File to pick files or drag and drop into the upload area.\nSelect file Illustration: Select file\nSelect file\nStep 5: In this example, drag and drop wordpress.zip.\nSelect wordpress zip Illustration: Select wordpress zip (or dist.zip/build.zip)\nSelect wordpress zip\nStep 6: After upload, return to File Manager. The archive appears in public_html. Right-click and choose Extract.\nExtract file Illustration: Extract file\nExtract file\nStep 7: Choose the extraction location. In this example, use /public_html.\nChoose extraction location Illustration: Choose extraction location\nChoose extraction location\nStep 8: After extraction, you will see the files in public_html. This is the website root.\nReturn to the original folder Illustration: Return to the original folder\nReturn to the original folder\nStep 9: The website is now uploaded. Enter your URL in a browser to visit.\nLanguage selection during WordPress setup (If applicable)\n\u0026mdash;Method 2: Upload via FTP client\u0026mdash;\nSome users prefer FTP clients like FileZilla, SmartFTP, CoreFTP, or similar tools. In this guide, we use FileZilla.\nNotes before uploading via FTP:\nYour skills: If you are new to hosting, File Manager is easier. If you are experienced, FTP is faster. Website size: Large websites are easier with FTP because there are no upload limits. Special requirements: If you need special server configuration, check with your hosting provider. Step 1: Get FTP information via FTP Access. If you forgot the password, reset it under Change account password.\nGet FTP Access details Illustration: Get FTP Access details\nGet FTP Access details\nStep 2: Open FileZilla, enter FTP details, and click Quickconnect.\nClick Quickconnect Illustration: Click Quickconnect\nClick Quickconnect\nStep 3: After connecting, locate your website files and drag them from the left pane to the right pane, targeting public_html. Unzip archives before upload because FTP cannot extract files.\nDrag and drop files Illustration: Drag and drop files\nDrag and drop files\nStep 4: You can also upload archives via FTP, then extract using File Manager.\nUpload archives via FTP Illustration: Upload archives via FTP\nUpload archives via FTP\nStep 5: After upload, enter the website URL in a browser to confirm installation or customize it.\nStep 2: Verify files are inside public_html After uploading, check that all files are in public_html. If you extracted a backup into a subfolder, users would need to visit example.com/something instead of example.com. Fix this by moving files:\nOpen the folder with the website files. Select all files, right-click, and choose Move. Select public_html and click Proceed. If your site has been running already, upload the database as well.\nAfter confirming file placement, open your domain in a browser. If DNS is not updated, you can:\nEdit the hosts file to simulate DNS changes. Use online tools to check DNS status. Install browser plugins for virtual hosts. If you need to move a site from a subfolder into public_html, use File Manager or FTP. Remember to upload the database if needed.\nStep 3: Upload the database to hosting Perform this step only if your website uses a database.\nCreate a database in cPanel Create a new database in MySQL Databases and record:\nMySQL Database MySQL User MySQL Host MySQL Password Create a new database\nOpen phpMyAdmin In phpMyAdmin, import your MySQL database. If importing into an existing database, clear it first to avoid errors.\nOpen phpMyAdmin\nImport the database Go to the Import tab and upload your SQL file (.sql, .sql.zip, or .sql.gz). Click Choose File, then Go to start. When phpMyAdmin shows Import has been successfully finished, 302 queries executed, the upload is done.\nImport database\nUpdate configuration for database connection After upload, open your PHP configuration file and fill in host, database name, username, and password. The file name and location depend on your software. For WordPress, the file is wp-config.php in public_html.\n‚ö† Notes:\nLarge databases should be split into smaller files to speed up uploads. If your database contains special characters, convert them to ASCII before upload. If import errors occur, check for corruption. Create a fresh database and try again. After uploading, test your website to ensure everything works. Update configuration for database connection After uploading, open the PHP config file to enter the host, database name, username, and password.\nThe file name and location depend on your software. For example, WordPress uses wp-config.php in public_html.\nStep 4: Check if the website is stable To ensure stability after upload and domain mapping, perform the following checks:\nüîç Check website access Access via domain or IP: If it loads, the site is live. Wait for DNS propagation: New DNS changes can take 24 hours. Check immediately using: üñ•Ô∏è Hosts file: Edit the hosts file to simulate DNS changes. üåê Online tools: Use online DNS checkers. üîå Browser plugins: Use plugins for virtual hosts. üõ†Ô∏è Test website functions Visit multiple pages and verify links. Ensure features work as expected. ‚ö° Check load speed Use Google PageSpeed Insights or GTmetrix. üö® Check for errors 404: Page not found. 500: Hosting server error. 503: Server maintenance. üåç Test across browsers and devices Try Chrome, Firefox, Safari, Edge on desktop, mobile, tablet. ‚è≥ Test at different times Performance can vary throughout the day. üëâ If issues appear and you are unsure how to fix them, contact your hosting provider. üöÄ\nüìå Conclusion Uploading a website to hosting is a key step for stable online operation. By preparing correctly and following the steps above, you ensure your website is ready to serve users effectively.\nüí° If you have questions or need help, leave a comment below. I will respond soon.\nThanks for reading! üöÄ\n","date":"2025-02-26T00:00:00Z","image":"https://tech.nguuyen.io.vn/images/website/upload-website-on-hosting.webp","permalink":"https://tech.nguuyen.io.vn/en/posts/website/how-to-deploy-a-website/","title":"Deploy a Website to Hosting the Simplest Way (HTML/CSS or React/Vue.js) üî•"},{"content":"üìä Application Monitoring Application monitoring is the continuous tracking and analysis of software applications to ensure they run optimally, detect incidents, and provide deep insights into system performance. Monitoring covers critical metrics such as:\n‚è≥ Response time ‚ùå Error rate üñ•Ô∏è Resource usage (CPU, RAM, Disk) üîÑ Transaction performance Application monitoring tools collect and analyze data to detect anomalies, alert on potential issues, and provide a complete view of application behavior. This allows teams to proactively resolve incidents, optimize performance, and improve user experience.\nüîç Jaeger - Distributed tracing tool Jaeger is an open-source distributed tracing system developed by Uber, designed to track and troubleshoot complex microservices architectures.\nüîπ Key features: üåê Distributed request tracing üîé Service dependency analysis üìä Root cause identification üõ†Ô∏è OpenTracing support for easy integration üîπ Example: Running Jaeger with Docker 1 2 3 4 5 6 7 8 9 10 11 docker run -d --name jaeger \\ -e COLLECTOR_ZIPKIN_HTTP_PORT=9411 \\ -p 5775:5775/udp \\ -p 6831:6831/udp \\ -p 6832:6832/udp \\ -p 5778:5778 \\ -p 16686:16686 \\ -p 14268:14268 \\ -p 14250:14250 \\ -p 9411:9411 \\ jaegertracing/all-in-one:1.37 After installation, open Jaeger UI at http://localhost:16686.\nüìå References:\nüìñ Jaeger documentation üõ†Ô∏è GitHub - jaegertracing üåé New Relic - Application performance monitoring New Relic is a cloud observability platform that provides a comprehensive view of software and infrastructure. It supports real-time performance monitoring, data analysis, and automated alerts.\nüîπ Key features: üìà Application performance monitoring (APM) üîé Log analysis and error tracing üöÄ Web and mobile user experience monitoring ü§ñ AI-powered analytics for incident detection üîπ Example: Installing New Relic Agent in Node.js 1 npm install newrelic --save Then add require('newrelic') at the top of your main file:\n1 2 3 4 5 require(\u0026#39;newrelic\u0026#39;); const express = require(\u0026#39;express\u0026#39;); const app = express(); app.get(\u0026#39;/\u0026#39;, (req, res) =\u0026gt; res.send(\u0026#39;Hello, New Relic!\u0026#39;)); app.listen(3000, () =\u0026gt; console.log(\u0026#39;App running on port 3000\u0026#39;)); üìå References:\nüåê New Relic website üé• New Relic platform demo üê∂ Datadog - Full-stack monitoring solution Datadog is a monitoring and analytics platform for large-scale applications. It covers infrastructure monitoring, application performance, logs management, and user experience.\nüîπ Key features: üîó 400+ integrations with DevOps tools üìä Unified dashboards for full system visibility ‚ö†Ô∏è Intelligent alerting üì° Cloud-native monitoring support üîπ Example: Installing Datadog Agent 1 2 DD_AGENT_MAJOR_VERSION=7 DD_API_KEY=\u0026lt;YOUR_API_KEY\u0026gt; \\ DD_SITE=\u0026#34;datadoghq.com\u0026#34; bash -c \u0026#34;$(curl -L https://s3.amazonaws.com/dd-agent/scripts/install_script.sh)\u0026#34; After installation, access the Datadog dashboard to view metrics.\nüìå References:\nüåê Datadog website üìñ Datadog documentation üéØ Conclusion Application monitoring is crucial for ensuring system performance and reliability. Tools such as Jaeger, New Relic, and Datadog provide comprehensive solutions to track, analyze, and optimize software systems effectively. Choosing the right tool helps you manage applications better, detect issues early, and improve user experiences.\nüöÄ Deploy monitoring to optimize your application performance!\nüëâ Next step: Learn about Artifacts in software development. Artifacts are the files or products produced during the development and deployment process.\n","date":"2025-02-25T00:00:00Z","image":"https://tech.nguuyen.io.vn/images/devops/devops-step-eighteen.webp","permalink":"https://tech.nguuyen.io.vn/en/posts/devops/devops-step-eighteen/","title":"Application Monitoring üìä"},{"content":"üèóÔ∏è What are Artifacts? Artifacts are products generated throughout the software development lifecycle. They can include:\nüìú Source code: Files that contain application logic. üèóÔ∏è Binaries: Compiled executables or libraries. üìñ Documentation: User guides, API specs. ‚öôÔ∏è Configuration files: Settings needed to run the application. üõ†Ô∏è Test results: Reports from testing pipelines. Managing artifacts helps ensure consistency, traceability, and more efficient software delivery.\nüè¢ Popular Artifact Management Tools üöÄ 1. Artifactory Artifactory is a DevOps solution for storing, managing, and distributing artifacts. It supports many formats such as Docker, npm, Maven, Python, Go, and more.\nüîß Install Artifactory with Docker: 1 2 3 4 5 # Pull Artifactory image docker pull releases-docker.jfrog.io/jfrog/artifactory-oss:latest # Run container docker run --name artifactory -d -p 8081:8081 releases-docker.jfrog.io/jfrog/artifactory-oss:latest After installation, you can access the web interface at http://localhost:8081.\nüì¶ 2. Nexus Repository Manager Nexus is one of the most popular tools for binary artifacts, especially in Java environments.\nüèóÔ∏è Install Nexus on Linux: 1 2 3 4 wget https://download.sonatype.com/nexus/3/latest-unix.tar.gz tar -xvf latest-unix.tar.gz cd nexus-3.* ./bin/nexus start After it starts, you can access Nexus at http://localhost:8081.\n‚òÅÔ∏è 3. Cloudsmith Cloudsmith is a cloud-based artifact management platform that supports many package formats such as Docker, Helm, npm, and pip.\n‚ö° Upload packages to Cloudsmith: 1 2 3 4 5 # Install Cloudsmith CLI pip install cloudsmith-cli # Push a package to the repository cloudsmith push python my-org/my-repo my-package-1.0.0.tar.gz Cloudsmith simplifies package distribution and management.\nüéØ Conclusion Artifact management is critical in the software development process. Using tools like Artifactory, Nexus, and Cloudsmith ensures organized, secure, and efficient storage and deployment. Depending on project needs, you can choose the tool that best fits your DevOps workflow.\nüõ†Ô∏è What tools do you use to manage artifacts? Share your experience!\nüëâ Next step: Learn about GitOps - the process of provisioning and configuring resources (servers, networks, storage, accounts) so that systems or applications can operate effectively.\n","date":"2025-02-25T00:00:00Z","image":"https://tech.nguuyen.io.vn/images/devops/devops-step-nineteen.webp","permalink":"https://tech.nguuyen.io.vn/en/posts/devops/devops-step-nineteen/","title":"Artifact Management in Software Development üè¢"},{"content":"Introduction Cloud design patterns are reusable solutions for common problems in cloud architectures. These patterns address scalability, reliability, security, and performance challenges in distributed systems.\nThey provide best practices for designing and deploying cloud applications, including data management, messaging, resiliency, and deployment. Common examples include:\nCircuit Breaker: Prevents cascading failures by temporarily breaking connections after repeated errors. CQRS (Command Query Responsibility Segregation): Separates read and write operations to improve performance. Sidecar Pattern: Splits auxiliary application components into separate processes or containers for more flexibility. üîπ Availability Availability is the percentage of time a system operates as expected, often called uptime. It can be affected by hardware or software failures, infrastructure issues, cyberattacks, or excessive load.\nCloud providers typically define service level agreements (SLA) that specify guaranteed uptime. For example, a company might promise 99.99% availability.\nüîπ Example: A system that guarantees 99.99% uptime can only be down for 52 minutes per year.\nüîπ Data Management Data management is a key factor in cloud applications and influences most quality attributes. Data is often stored in multiple locations to improve performance, scale, or availability. This introduces challenges such as:\nMaintaining data consistency when synchronizing across servers. Securing data during storage, transit, and access. Scalability to handle growth demands. üîπ Example: Banking systems must ensure transactions remain consistent across data centers to avoid incorrect account balances.\nüîπ Design and Implementation Good design helps systems stay maintainable, consistent, and reusable across multiple scenarios. Decisions made during design and implementation directly affect cost and overall cloud application quality.\nKey design principles:\nConsistency: Components should follow a defined structure. Scalability: Systems must handle high traffic without performance degradation. Reusability: Components should be usable across different applications. üîπ Example: An e-commerce system uses microservices architecture to scale individual services independently (payments, cart, product search).\nüîπ Management and Monitoring DevOps management and monitoring covers the lifecycle from planning, development, testing, deployment, to operations. A solid monitoring system tracks the status of applications, services, and infrastructure in production.\nüîπ Key monitoring components:\nReal-time streaming: Observe systems in real time. Historical replay: Store history for incident analysis. Visualization: Present data visually to evaluate system health. üîπ Example: Using Prometheus + Grafana to monitor Kubernetes container performance.\nüî• Conclusion Cloud design patterns optimize system architecture by providing solutions for common challenges such as performance, security, and scalability. Choosing the right patterns helps businesses build resilient, flexible, and maintainable systems.\n‚úÖ Wrap-up: You have completed the DevOps journey, combining Development and Operations to speed up software delivery, improve reliability, and optimize workflows. Thank you for following along, and we hope this knowledge supports your next steps! üöÄ\n","date":"2025-02-25T00:00:00Z","image":"https://tech.nguuyen.io.vn/images/devops/devops-step-twenty-two.webp","permalink":"https://tech.nguuyen.io.vn/en/posts/devops/devops-step-twenty-two/","title":"Cloud Design Patterns üåü"},{"content":"Introduction GitOps is a method for managing infrastructure and application deployments by using Git as the single source of truth. It automates deployments, ensures consistency, and improves change tracking.\nGitOps extends DevOps practices by applying version control and CI/CD principles to infrastructure management. Instead of manual changes, every update flows through pull requests and is automatically synchronized with live environments.\nüöÄ Benefits of GitOps ‚úÖ Version control: Every change is tracked in Git, making it easy to audit or roll back when needed.\n‚úÖ Automated deployments: Tools like ArgoCD or FluxCD sync environments automatically.\n‚úÖ Improved security: All changes go through Git, preventing direct access to production systems.\n‚úÖ High resilience: Systems can recover quickly by re-applying Git state after incidents.\n‚úÖ Better team collaboration: Changes are reviewed via pull requests, enabling smoother teamwork.\nüõ†Ô∏è Popular GitOps Tools ArgoCD - Powerful continuous deployment ArgoCD is a continuous deployment (CD) tool for Kubernetes. It automates deployments by watching application state in Git and syncing it with the live cluster.\nüîπ Key features:\nAutomatic sync between Git and environment state. Visual web UI for deployment status. Supports Helm, Kustomize, and other configuration tools. üîπ Example ArgoCD application\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: my-app namespace: argocd spec: destination: namespace: default server: https://kubernetes.default.svc source: repoURL: https://github.com/my-org/my-repo.git targetRevision: HEAD path: manifests syncPolicy: automated: selfHeal: true prune: true FluxCD - Automated deployment management FluxCD is a GitOps tool that deploys applications to Kubernetes by watching Git repositories and updating clusters automatically.\nüîπ Key features:\nTracks Git repositories and deploys automatically. Supports Helm, Kustomize, and many CI/CD systems. Provides automatic container image updates. üîπ Example FluxCD configuration:\n1 2 3 4 5 6 7 8 9 10 apiVersion: source.toolkit.fluxcd.io/v1beta1 kind: GitRepository metadata: name: my-app-repo namespace: flux-system spec: interval: 1m0s url: https://github.com/my-org/my-repo.git ref: branch: main üî• ArgoCD vs FluxCD Feature ArgoCD FluxCD Visual UI ‚úÖ Yes ‚ùå No Helm support ‚úÖ Yes ‚úÖ Yes Automatic container image updates ‚ùå No ‚úÖ Yes Canary/Rollback deployments ‚úÖ Yes ‚úÖ Yes üìå Conclusion GitOps simplifies application deployments by using Git as the central control layer. With tools like ArgoCD and FluxCD, teams can automate deployments, ensure consistency, and improve system resiliency.\nüîó References:\nüìñ ArgoCD documentation üìñ FluxCD documentation üìò GitOps guide Have you tried GitOps in your projects? Share your experience! üöÄ\nüëâ Next step: Learn about Service Mesh - a software infrastructure layer that manages communication between services in microservices systems. It provides load balancing, security, observability, traffic control, and fault handling so services can communicate efficiently without changing application code.\n","date":"2025-02-25T00:00:00Z","image":"https://tech.nguuyen.io.vn/images/devops/devops-step-twenty.webp","permalink":"https://tech.nguuyen.io.vn/en/posts/devops/devops-step-twenty/","title":"GitOps - Automated Deployment with Git üåü"},{"content":"üìå Introduction Service Mesh is a software infrastructure layer that manages communication between services in microservices systems. It provides load balancing, security, observability, traffic control, and fault handling, enabling services to communicate efficiently without changing application code.\nPopular Service Mesh tools include Istio, Linkerd, and Consul.\nüöÄ Benefits of Service Mesh ‚úÖ Automatic service-to-service communication management - No application code changes required. ‚úÖ Enhanced security - TLS encryption, service authentication, RBAC policies. ‚úÖ Performance optimization - Smart load balancing and traffic control. ‚úÖ Comprehensive observability - Logs, metrics, and tracing for monitoring. ‚úÖ High resilience - Automatic fault detection and recovery.\nüõ†Ô∏è Popular Service Mesh Tools 1Ô∏è‚É£ Istio - The most powerful solution Istio is an open-source Service Mesh platform that provides control, security, and observability for Kubernetes services.\nüîπ Key features:\nmTLS support for secure service communication. Smart load balancing, retries, and timeouts. API gateway management and access policies. üîπ Istio Gateway example:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: my-gateway spec: selector: istio: ingressgateway servers: - port: number: 80 name: http protocol: HTTP hosts: - \u0026#34;example.com\u0026#34; üìñ Istio documentation\n2Ô∏è‚É£ Linkerd - Lightweight and fast Linkerd is a lightweight Service Mesh optimized for Kubernetes, designed to reduce latency and resource usage.\nüîπ Key features:\nAutomatic TLS (mTLS) for all communications. Built-in tracing and metrics. Quick installation with a single CLI command. üîπ Linkerd configuration example:\n1 2 3 4 5 6 7 8 9 10 11 apiVersion: linkerd.io/v1alpha2 kind: ServiceProfile metadata: name: my-service.default.svc.cluster.local spec: routes: - name: \u0026#34;GET /health\u0026#34; condition: method: GET pathRegex: \u0026#34;/health\u0026#34; isRetryable: true üìñ Linkerd documentation\n3Ô∏è‚É£ Consul - Service Mesh with service discovery Consul is not only a Service Mesh, but also provides service discovery, configuration management, and secure communication between services.\nüîπ Key features:\nService discovery for Kubernetes and traditional systems. Integration with Envoy Proxy for traffic management. Supports multi-cluster and hybrid cloud environments. üîπ Consul service registration example:\n1 2 3 4 5 6 7 { \u0026#34;service\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;web\u0026#34;, \u0026#34;port\u0026#34;: 8080, \u0026#34;connect\u0026#34;: { \u0026#34;sidecar_service\u0026#34;: {} } } } üìñ Consul documentation\nüî• Service Mesh Comparison Feature Istio Linkerd Consul Visual UI ‚úÖ Yes ‚ùå No ‚úÖ Yes mTLS support ‚úÖ Yes ‚úÖ Yes ‚úÖ Yes High performance ‚ö†Ô∏è Needs tuning ‚úÖ Fast ‚ö†Ô∏è Moderate Service discovery ‚ùå No ‚ùå No ‚úÖ Yes üìå Conclusion Service Mesh simplifies managing communication between microservices, ensuring security, observability, and high performance.\nüí° If you need the most powerful solution, try Istio. üí° If you prioritize performance and ease of setup, choose Linkerd. üí° If you need service discovery and multi-cloud, consider Consul.\nüìñ References: üîó Istio documentation üîó Linkerd documentation üîó Consul documentation\nHave you tried any Service Mesh deployments? Share your experience! üöÄ\nüëâ Final step: Learn about Cloud Design Patterns - a set of proven patterns for building efficient, flexible, and scalable cloud applications. They address common challenges like high availability, fault tolerance, elastic scaling, and security in cloud environments.\n","date":"2025-02-25T00:00:00Z","image":"https://tech.nguuyen.io.vn/images/devops/devops-step-twenty-one.webp","permalink":"https://tech.nguuyen.io.vn/en/posts/devops/devops-step-twenty-one/","title":"Service Mesh - Managing Microservices Communication üåê"},{"content":"üîç What is CI/CD? CI/CD (Continuous Integration/Continuous Deployment) is a methodology that helps automate the software development process, reducing errors, speeding up deployment, and improving product quality.\nContinuous Integration (CI): Continuous integration, helps detect errors early by automatically testing whenever there are changes in the source code. Continuous Deployment (CD): Continuous deployment, ensures software is released automatically and quickly. üåü Popular CI/CD Tools üîß Jenkins Jenkins is a popular open-source automation server that helps build, test, and deploy software automatically.\nüìå Key Features:\nSupports many plugins, easy integration with DevOps tools. Intuitive web interface, easy to configure. Supports both Windows and Linux environments. üí° Example: Configuring a simple pipeline in Jenkinsfile\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 pipeline { agent any stages { stage(\u0026#39;Build\u0026#39;) { steps { echo \u0026#39;Building the application...\u0026#39; } } stage(\u0026#39;Test\u0026#39;) { steps { echo \u0026#39;Running tests...\u0026#39; } } stage(\u0026#39;Deploy\u0026#39;) { steps { echo \u0026#39;Deploying the application...\u0026#39; } } } } üìñ Useful Resources:\nüîó Jenkins Homepage üé• Jenkins Tutorial from A-Z üîÑ CircleCI CircleCI is a powerful CI/CD platform that supports many programming languages and integrates well with GitHub and Bitbucket.\nüìå Key Features:\nSupports parallel builds to increase processing speed. Good integration with Docker and Kubernetes. Available in cloud and self-hosted versions. üí° Example: Configuring a CircleCI pipeline in .circleci/config.yml\n1 2 3 4 5 6 7 8 9 version: 2.1 jobs: build: docker: - image: circleci/node:14 steps: - checkout - run: npm install - run: npm test üìñ Useful Resources:\nüîó CircleCI Homepage üé• CircleCI Tutorial Video üõ†Ô∏è GitLab CI/CD GitLab CI/CD is a built-in system in GitLab that allows automating testing and deployment processes right within the Git repository.\nüìå Key Features:\nSupports full pipeline automation from build, test to deploy. Built into GitLab, no external tools needed. Supports running pipelines on Docker containers. üí° Example: Configuring GitLab CI/CD pipeline in .gitlab-ci.yml\n1 2 3 4 5 6 7 8 9 10 stages: - build - test - deploy test: stage: test script: - npm install - npm test üìñ Useful Resources:\nüîó GitLab Homepage üé• GitLab CI/CD Tutorial ‚ö° GitHub Actions GitHub Actions is a CI/CD tool integrated directly into GitHub, helping automate testing and deployment processes whenever there are changes in the repository.\nüìå Key Features:\nTightly integrated with GitHub, easy workflow setup. Supports many operating systems and programming languages. Can use marketplace with thousands of available actions. üí° Example: GitHub Actions workflow in .github/workflows/main.yml\n1 2 3 4 5 6 7 8 9 10 11 12 13 name: Node.js CI on: [push, pull_request] jobs: build: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - name: Setup Node.js uses: actions/setup-node@v3 with: node-version: 14 - run: npm install - run: npm test üìñ Useful Resources:\nüîó GitHub Actions Homepage üé• GitHub Actions Tutorial Video üéØ Conclusion CI/CD helps accelerate the software development process, minimize errors, and improve product quality. Depending on your needs and working environment, you can choose Jenkins, CircleCI, GitLab CI/CD, or GitHub Actions to deploy your CI/CD system. üöÄ\nüëâ Next step: Learn about Secret Management - the process of storing, managing, and protecting sensitive information such as passwords, API keys, certificates, and access tokens to prevent data leakage and ensure system security. üí°\n","date":"2025-02-24T00:00:00Z","image":"https://tech.nguuyen.io.vn/images/devops/devops-step-thirteen.webp","permalink":"https://tech.nguuyen.io.vn/en/posts/devops/devops-step-thirteen/","title":"CI/CD - Continuous Integration and Continuous Deployment üöÄ"},{"content":"üîß Configuration Management Configuration management is the process of managing and maintaining consistency of components within an information technology system. In the software field, it includes monitoring, tracking, and managing system configuration changes throughout the product lifecycle. Applying configuration management helps increase synchronization, reduce error risks, and ensure compliance with standards in CI/CD (Continuous Integration and Continuous Deployment) processes.\nüåê Popular Configuration Management Tools üöÄ Ansible Ansible is an open automation tool, primarily used for configuration management, application deployment, and task automation.\nüîÑ Features:\nUses YAML (playbooks) to define desired states. Works without requiring agent installation. Suitable for small to large scale environments. üí° Ansible usage example:\n1 2 3 4 5 6 7 8 9 10 11 12 - name: Install and start Nginx hosts: all become: yes tasks: - name: Install Nginx apt: name: nginx state: present - name: Start Nginx service: name: nginx state: started üìñ Useful free resources:\nüìö Complete Ansible Course for Beginners üîó Official Ansible Website üé• Ansible in 100 Seconds üåü Chef Chef (now part of Progress Chef) is one of the first configuration management tools. It uses Ruby language and emphasizes idempotence (ensuring running n times produces the same result).\nüîÑ Features:\nClient/server based. Has Chef-Solo for standalone deployment. Suitable for enterprise environments. üí° Chef usage example:\n1 2 3 4 5 6 7 package \u0026#39;nginx\u0026#39; do action :install end service \u0026#39;nginx\u0026#39; do action [:enable, :start] end üìñ Useful free resources:\nüîó Official Chef Website üìö Chef Tutorial üé• Chef Tutorial Video üè∞ Puppet Puppet is a declarative configuration management tool that operates in a client/server model and supports multiple operating systems.\nüîÑ Features:\nLarge scale management. Periodic configuration checking and application. Integration with many DevOps tools. üí° Puppet usage example:\n1 2 3 4 5 6 7 8 package { \u0026#39;nginx\u0026#39;: ensure =\u0026gt; installed, } service { \u0026#39;nginx\u0026#39;: ensure =\u0026gt; running, enable =\u0026gt; true, } üìñ Useful free resources:\nüìö Complete Puppet Course üîó Official Puppet Website üé• Great Puppet Articles üéâ Conclusion Configuration management is an important element in the DevOps process, helping ensure consistent system operation, minimize errors, and enhance reliability. Depending on needs and scale, businesses can choose Ansible, Chef, or Puppet to manage infrastructure effectively.\nüëâ Next step: Learn about Continuous Integration and Continuous Deployment (CI/CD), which is an automation process in software development that helps integrate, test, and deploy applications continuously.\n","date":"2025-02-24T00:00:00Z","image":"https://tech.nguuyen.io.vn/images/devops/devops-step-twelve.webp","permalink":"https://tech.nguuyen.io.vn/en/posts/devops/devops-step-twelve/","title":"Configuration Management üîß"},{"content":"üîç What is Container Orchestration? Container orchestration is the process of managing and automating the container lifecycle, including deployment, scaling, and networking across multiple servers. It is critical for running complex applications in production environments.\nBy using tools like Kubernetes, Docker Swarm, and Apache Mesos, organizations can ensure high availability, scalability, and reliability for their applications. Container orchestration automates operational tasks and provides a strong foundation for microservices, cloud-native development, and DevOps.\nüìö Free resources:\nüì¶ What is Container Orchestration? üöÄ What is Kubernetes? üê≥ Docker Swarm üé• Kubernetes introduction ‚ò∏Ô∏è Kubernetes Kubernetes is the most popular open-source platform for container management. It allows container deployment across multiple servers, defining availability, deployment logic, and scaling through YAML.\nKubernetes originated from Borg, Google\u0026rsquo;s internal platform, and has become a critical skill for DevOps engineers. Many organizations now have Platform Engineering teams dedicated to supporting Kubernetes for product teams.\nüìö Free resources:\nüó∫Ô∏è In-depth Kubernetes roadmap üåê Official Kubernetes website üìñ Kubernetes overview üé• Complete Kubernetes course - beginner to advanced üìå Example:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion: apps/v1 kind: Deployment metadata: name: my-app spec: replicas: 3 selector: matchLabels: app: my-app template: metadata: labels: app: my-app spec: containers: - name: my-app image: my-app-image:latest ‚òÅÔ∏è GKE / EKS / AKS üîπ GKE - Google Kubernetes Engine Google Kubernetes Engine (GKE) is a managed Kubernetes service by Google Cloud. It helps deploy, manage, and scale container applications with Kubernetes without managing the cluster manually.\nüîπ EKS - Amazon Elastic Kubernetes Service Amazon Elastic Kubernetes Service (EKS) is a Kubernetes service provided by AWS. It automatically manages the Kubernetes control plane and integrates with other AWS services.\nüîπ AKS - Azure Kubernetes Service Azure Kubernetes Service (AKS) is Microsoft Azure‚Äôs Kubernetes service. AKS supports monitoring, security, autoscaling, and integration with Azure DevOps.\nüìö Free resources:\n‚òÅÔ∏è Google Kubernetes Engine (GKE) üüß Amazon Elastic Kubernetes Service (EKS) üîµ Azure Kubernetes Service (AKS) üé• AWS EKS tutorial üé• What is Google Kubernetes Engine? üöÄ ECS / Fargate ECS is a container management service that runs on AWS EC2, giving you control over the server infrastructure.\nFargate is a serverless container service that runs containers without managing servers or clusters.\nüìö Free resources:\nüìÑ AWS Fargate documentation üìÑ AWS ECS documentation üé• AWS Fargate overview üé• AWS ECS tutorial üìå Example:\n1 2 3 4 5 6 7 8 9 10 11 { \u0026#34;family\u0026#34;: \u0026#34;my-task\u0026#34;, \u0026#34;containerDefinitions\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;my-container\u0026#34;, \u0026#34;image\u0026#34;: \u0026#34;my-container-image:latest\u0026#34;, \u0026#34;memory\u0026#34;: 512, \u0026#34;cpu\u0026#34;: 256 } ] } üê≥ Docker Swarm Docker Swarm is a cluster of Docker nodes (physical or virtual machines). The cluster is managed by a swarm manager, and the participating machines are nodes.\nüìö Free resources:\nüìÑ Docker Swarm documentation üîß Manage Docker Swarm with Portainer üì¶ Docker Swarm with GlusterFS storage üé• Docker Swarm introduction | step-by-step üìå Example:\n1 2 3 4 5 # Initialize Swarm $ docker swarm init # Deploy a service on Swarm $ docker service create --name web -p 80:80 nginx ‚úÖ Conclusion Container orchestration plays a vital role in managing containerized applications, helping simplify operations, optimize resources, and improve availability. Tools such as Kubernetes, Docker Swarm, ECS, and Fargate offer flexible solutions for modern enterprises. Choosing the right tool enables organizations to maximize the power of containerization and cloud computing. üöÄ\nüëâ Next step: Learn about Application Monitoring - tracking, measuring, and analyzing application performance, status, and behavior to detect incidents, optimize performance, and ensure a great user experience. Popular tools include Prometheus, Grafana, Datadog, and New Relic for collecting data from applications, servers, and infrastructure to provide alerts and detailed reports.\n","date":"2025-02-24T00:00:00Z","image":"https://tech.nguuyen.io.vn/images/devops/devops-step-seventeen.webp","permalink":"https://tech.nguuyen.io.vn/en/posts/devops/devops-step-seventeen/","title":"Container Orchestration üö¢"},{"content":"üîç What is Infrastructure Monitoring? Infrastructure monitoring is the process of tracking the performance and status of systems to detect incidents early and optimize operations. This is a broad field with many tools, each with its own strengths and weaknesses. Understanding these tools helps you choose the right solution for your monitoring goals.\nüöÄ Popular Monitoring Tools Grafana üìù Overview: Grafana is an open-source web application for data analysis and visualization. It connects to many data sources such as time series databases, relational databases, and cloud services.\nüîπ Key features:\nPowerful visualization with many chart types. Supports a wide range of plugins. Real-time alerting system. User authentication and role-based access control. üìå Example: Use Grafana to monitor CPU and RAM usage to detect and handle overload incidents.\nüìö Useful resources:\nüîó Grafana homepage üìñ Installation and usage guides Prometheus üìù Overview: Prometheus is an open-source monitoring and alerting tool, especially well-suited for microservices and containerized systems like Kubernetes.\nüîπ Key features:\nMulti-dimensional data model. Powerful PromQL query language. Pull-based data collection model. Smart alert management with Alertmanager. üìå Example: Use Prometheus to collect and analyze API request counts to identify peak traffic and optimize performance.\nüìö Useful resources:\nüîó Prometheus homepage üé• Prometheus introduction Zabbix üìù Overview: Zabbix is an open-source monitoring platform that supports comprehensive tracking for system components such as servers, networks, applications, and services.\nüîπ Key features:\nMultiple data collection methods: Agent, SNMP, IPMI, custom scripts. Real-time alerts and notifications. Detailed dashboards and reporting system. Scales well for large environments. üìå Example: Use Zabbix to monitor server status, detect downtime, and send alerts immediately.\nüìö Useful resources:\nüîó Zabbix homepage üìñ Zabbix documentation ‚úÖ Conclusion Each monitoring tool has its own strengths:\nGrafana: Strong data visualization. Prometheus: Great for container and microservices environments. Zabbix: End-to-end monitoring for large systems. Depending on your specific needs, you can combine multiple tools to build an optimal monitoring system. üöÄ\nüëâ Next step: Learn about Logs Management - the process of collecting, storing, processing, and analyzing logs from systems, applications, and devices to track activity, detect incidents, ensure security, and support faster troubleshooting.\n","date":"2025-02-24T00:00:00Z","image":"https://tech.nguuyen.io.vn/images/devops/devops-step-fifteen.webp","permalink":"https://tech.nguuyen.io.vn/en/posts/devops/devops-step-fifteen/","title":"Infrastructure Monitoring üìä"},{"content":"üîπ What is Provisioning? Provisioning refers to the process of setting up and configuring the necessary IT infrastructure to support an application or service. This includes allocating and preparing resources such as servers, storage, networking, and software environments.\nWhile provisioning can be done manually, in modern DevOps, this process is typically automated using tools like Terraform, Pulumi, CloudFormation. Using Infrastructure-as-Code (IaC) helps define the entire provisioning process in version-controlled script files, ensuring consistency, reducing human errors, and improving scalability and disaster recovery.\nüìñ Free resources to learn:\nüìÑ What is provisioning? - RedHat üìÑ What is provisioning? - IBM üé• Open Answers: What is provisioning? üèóÔ∏è Terraform - Powerful IaC Solution Terraform is an open-source Infrastructure-as-Code (IaC) tool developed by HashiCorp that helps define, deploy, and manage infrastructure on multi-cloud or on-premises environments using declarative configuration files.\nüåü Benefits of Using Terraform ‚úÖ Multi-platform support: AWS, Azure, Google Cloud, Kubernetes, etc. ‚úÖ State management: Helps track infrastructure resources. ‚úÖ Scalability and reusability: Easy to modularize configurations. ‚úÖ CI/CD integration: Automates infrastructure deployment.\nüî® Example: Creating an EC2 instance on AWS 1 2 3 4 5 6 7 8 9 10 11 provider \u0026#34;aws\u0026#34; { region = \u0026#34;us-east-1\u0026#34; } resource \u0026#34;aws_instance\u0026#34; \u0026#34;web\u0026#34; { ami = \u0026#34;ami-12345678\u0026#34; instance_type = \u0026#34;t2.micro\u0026#34; tags = { Name = \u0026#34;Terraform-Instance\u0026#34; } } üìñ Free resources to learn:\nüìç Detailed Terraform Roadmap üé• Complete Terraform Course üìÑ Official Terraform Documentation üìñ How to Scale Your Terraform Infrastructure üî• Explore Top Terraform Articles üîπ AWS CDK - An Alternative? AWS Cloud Development Kit (AWS CDK) is an open-source framework for provisioning AWS infrastructure using code in languages like TypeScript, Python, Java, C#, Go. AWS CDK uses CloudFormation to deploy resources safely and repeatedly.\nüìñ Free resources to learn:\nüé• AWS CDK Course for Beginners üìÑ Official AWS CDK Documentation üìÇ AWS CDK Examples üî• Explore Top AWS Articles üìå Conclusion Terraform is the leading tool for Infrastructure-as-Code, providing flexibility and powerful automation capabilities across multiple cloud platforms. If you work extensively with AWS and want to deploy using programming languages, AWS CDK is also a viable option to consider.\nDepending on project requirements, you can choose the appropriate tool to manage infrastructure more effectively. üöÄ\nüëâ Next step: Learn about Configuration Management - the process of managing, monitoring, and automating system, software, and infrastructure configurations to ensure consistency, stability, and easy control throughout their lifecycle. It helps track and control changes, reduce errors from manual configuration, and support rapid deployment. Popular tools include Ansible, Puppet, Chef, and SaltStack.\n","date":"2025-02-24T00:00:00Z","image":"https://tech.nguuyen.io.vn/images/devops/devops-step-eleven.webp","permalink":"https://tech.nguuyen.io.vn/en/posts/devops/devops-step-eleven/","title":"Infrastructure Provisioning with Terraform üü™üî≥"},{"content":"üîç What is Logs Management? Logs management is the process of collecting, aggregating, analyzing, storing, and retrieving logs from applications and infrastructure systems. Logs contain critical information about system activity, helping detect incidents, optimize performance, and ensure security compliance.\nüöÄ Popular Logs Management Tools Elastic Stack (ELK Stack) üìù Overview: Elastic Stack, formerly known as the ELK Stack, includes Elasticsearch (search and analytics), Logstash (data processing), Kibana (visualization), and Beats (data collection). It is a popular solution for logs management with high scalability.\nüîπ Key features:\nReal-time log search and analysis. Supports multiple data formats. Scales well for enterprise environments. Provides a visual interface with Kibana. Use Elastic Stack to collect logs from web servers, analyze errors, and create dashboards that track traffic patterns.\nüìö Useful resources:\nüîó Elastic Stack homepage üé• Elastic Stack overview Loki üìù Overview: Loki is a log aggregation system developed by Grafana Labs, optimized for Kubernetes and containers. Loki stores log labels instead of full-text indexes, saving resources.\nüîπ Key features:\nTight integration with Grafana. Uses LogQL, a query language similar to PromQL. Optimized for Kubernetes and container environments. Resource-efficient compared to other solutions. Use Loki to track logs from containers in Kubernetes to detect application errors quickly.\nüìö Useful resources:\nüîó Loki homepage üìñ Loki documentation Graylog üìù Overview: Graylog is an open-source logs management platform that supports real-time log collection, storage, and analysis. It provides a friendly web interface and supports multiple log data types.\nüîπ Key features:\nSupports log collection protocols such as Syslog and GELF. Powerful search interface. Alerting capabilities for incident detection. Real-time log queries. Use Graylog to monitor security system logs and detect intrusion behavior.\nüìö Useful resources:\nüîó Graylog homepage üé• Graylog usage guides üî• Example Code Below is an example of sending logs to a log collection system using JavaScript:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 const winston = require(\u0026#39;winston\u0026#39;); const logger = winston.createLogger({ level: \u0026#39;info\u0026#39;, format: winston.format.json(), transports: [ new winston.transports.Console(), new winston.transports.File({ filename: \u0026#39;app.log\u0026#39; }) ] }); // Write logs logger.info(\u0026#39;Application started successfully\u0026#39;); logger.warn(\u0026#39;Warning: Invalid input data\u0026#39;); logger.error(\u0026#39;Error: Unable to connect to the database\u0026#39;); ‚úÖ Conclusion Logs management plays a vital role in monitoring systems and resolving incidents quickly. Tools such as Elastic Stack, Loki, and Graylog provide powerful solutions for log collection, analysis, and visualization. Choosing the right tool will help improve operational efficiency. üöÄ\nüëâ Next step: Learn about Container Orchestration - the process of automatically managing, deploying, scaling, and coordinating containers in infrastructure environments, ensuring availability, scalability, and optimized resource usage. Common tools for container orchestration include Kubernetes, Docker Swarm, and Apache Mesos.\n","date":"2025-02-24T00:00:00Z","image":"https://tech.nguuyen.io.vn/images/devops/devops-step-sixteen.webp","permalink":"https://tech.nguuyen.io.vn/en/posts/devops/devops-step-sixteen/","title":"Logs Management üìú"},{"content":"üîç What is Secret Management? Secret Management is the process of protecting, storing, and securely distributing sensitive information such as passwords, API keys, and certificates in an organization\u0026rsquo;s information technology system. It helps prevent unauthorized access while ensuring authorized systems and users can use the information when needed.\nSecret Management typically includes features:\nData encryption during storage and transmission. Access control, only allowing authorized people or systems to access. Key rotation mechanism to change passwords periodically. Integration with DevOps systems to automate security processes. üîó Additional References:\nüìÑ How to manage secrets in web applications üìÑ Why DevSecOps needs secret management üé• DevOps tips for managing secrets in production environments üèõÔ∏è HashiCorp Vault HashiCorp Vault is a secret management tool that helps protect sensitive data such as passwords, API keys, and encryption keys.\nüîë Key Features: Centralized management helps easily control secrets. Supports multiple authentication methods such as LDAP, Kubernetes. Provides dynamic secrets helps create temporary secrets when needed. Strong data encryption during storage and transmission. üîó Examples:\nStore and manage TLS certificates for a microservices architecture.\nIntegrate with Kubernetes to provide dynamic secrets for Pods each time they start.\nCreate temporary passwords for databases to reduce the risk of information leakage.\nüõ† Practical Examples: 1Ô∏è‚É£ Provision dynamic passwords for PostgreSQL using Vault\nSuppose you have a PostgreSQL database and want to provision a temporary account:\n1 vault write database/creds/postgres-role This command will create a new account with limited lifetime.\n2Ô∏è‚É£ Use Vault to manage API keys in Node.js applications\nYou can retrieve API keys from Vault in your source code:\n1 2 3 const { execSync } = require(\u0026#39;child_process\u0026#39;); const secret = execSync(\u0026#39;vault kv get -field=value secret/my-api-key\u0026#39;).toString(); console.log(`API Key: ${secret}`); üîó Useful Resources:\nüåç HashiCorp Vault Homepage üõ†Ô∏è HashiCorp Vault Source Code on GitHub üé• HashiCorp Vault Introduction in 180 seconds üé• HashiCorp Vault Tutorial for Beginners ‚òÅÔ∏è Cloud Secret Management Tools üî• AWS Secrets Manager Provides secure secret storage and management service on AWS with automatic password rotation capability and integration with other AWS services.\nüåç Google Cloud Secret Manager Secret management solution on Google Cloud, allowing automatic password rotation and easy access control.\nüî∑ Azure Key Vault Microsoft Azure service that helps securely store and manage keys, passwords, and digital certificates.\nüîó Examples:\nAWS Secrets Manager: Store API keys of a web application, ensuring API keys are not exposed in source code.\nHashiCorp Vault: Manage database access passwords for PostgreSQL.\nAzure Key Vault: Store and secure application encryption keys.\nüõ† Practical Examples: 1Ô∏è‚É£ Use AWS Secrets Manager to manage API keys\nSuppose you have a web application that needs to access a third-party service through an API key. Instead of storing the API key in source code, you can store it in AWS Secrets Manager and call it when needed:\n1 aws secretsmanager get-secret-value --secret-id my-api-key 2Ô∏è‚É£ Create dynamic secrets with HashiCorp Vault\nHashiCorp Vault can create temporary passwords for databases to enhance security. For example, you can create a temporary account for PostgreSQL:\n1 vault write database/creds/my-role üîó Useful Resources:\nüìÑ AWS Secrets Manager ‚Äì AWS Secret Management Service üìÑ Google Cloud Secret Manager ‚Äì Google Cloud Secret Management Service üìÑ Azure Key Vault ‚Äì Azure Key and Secret Management Service üé• AWS Secrets Manager Demo ‚Äì AWS Secrets Manager Tutorial üé• Google Cloud Secret Manager ‚Äì Google Cloud Secret Manager Introduction üé• Azure Key Vault and How to Use ‚Äì Azure Key Vault Tutorial üîê Sealed Secrets Sealed Secrets is a tool for Kubernetes that helps encrypt sensitive data into SealedSecrets, which can be safely stored even in public environments like GitHub.\nüõ†Ô∏è How it works: Encryption: User creates a SealedSecret from a Kubernetes Secret. Storage: SealedSecret can be committed to Git. Decryption: Only the Kubernetes Controller in the cluster can decrypt and restore it to a regular Kubernetes Secret. üí° Highlights:\nUses asymmetric encryption ensuring only the controller can decrypt data. Supports GitOps helps manage secrets safely in Git repository. Easy integration with Kubernetes to protect sensitive data. üîó Useful Resources:\nüõ†Ô∏è Sealed Secrets on GitHub üìÑ Sealed Secrets Documentation üîÑ Integrating Secret Management into CI/CD CI/CD tools like Azure DevOps, Travis CI, and AWS CodePipeline support secret management by integrating with Secret Management systems to protect sensitive information during deployment.\nüèóÔ∏è Azure DevOps Azure DevOps provides Azure Key Vault to securely store and manage secrets. Pipelines in Azure DevOps can retrieve secrets from Key Vault for use during deployment.\nüöÄ Travis CI Travis CI supports environment variable encryption, helping protect API keys and sensitive information during builds.\nüå©Ô∏è AWS CodePipeline AWS CodePipeline can integrate with AWS Secrets Manager to retrieve secrets during application deployment.\nüéØ Conclusion Secret management plays an important role in modern system security. Using tools such as HashiCorp Vault, AWS Secrets Manager, Google Cloud Secret Manager, Azure Key Vault, Sealed Secrets, Azure DevOps, Travis CI, and AWS CodePipeline helps ensure the safety of sensitive information, supports secure DevOps deployment, and complies with security standards.\nüëâ Next step: Monitor infrastructure Infrastructure Monitoring - the process of tracking and analyzing the performance, availability, and status of information technology infrastructure components such as servers, networks, storage, databases, and cloud computing services, helping to detect incidents early, optimize resources, and ensure stable system operation. üöÄ\n","date":"2025-02-24T00:00:00Z","image":"https://tech.nguuyen.io.vn/images/devops/devops-step-fourteen.webp","permalink":"https://tech.nguuyen.io.vn/en/posts/devops/devops-step-fourteen/","title":"Secret Management üîí"},{"content":"üöÄ What is Serverless? Serverless is a cloud computing model where service providers fully manage the infrastructure, allowing developers to focus solely on writing code. The system automatically allocates resources based on demand and charges only for actual resource usage. Serverless architecture is commonly applied for microservices applications, event processing and helps minimize operational costs.\nüìñ Free resources to learn:\nüìÑ What is Serverless? üé• Introduction to Serverless üåç Great articles about Serverless ‚ö° AWS Lambda AWS Lambda is a serverless service from AWS that allows running code without managing servers. Lambda automatically scales based on demand, supports multiple programming languages, and easily integrates with other AWS services. It\u0026rsquo;s suitable for data processing, task automation, building microservices.\nüñ•Ô∏è Example: Deploying a function on AWS Lambda\n1 2 3 4 5 6 import json def lambda_handler(event, context): return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps(\u0026#39;Hello from AWS Lambda!\u0026#39;) } üìñ Free resources to learn:\nüîó Introduction to AWS Lambda üé• AWS Lambda Tutorial from A-Z üåç Great articles about AWS Lambda üåç Cloudflare Cloudflare is a company that provides CDN, security, performance optimization services for websites. Cloudflare acts as a reverse proxy, helping to speed up page loading and protect websites from attacks. The company was founded in 2009 and went public in 2019.\nüìñ Free resources to learn:\nüîó Cloudflare Homepage üé• Introduction to Cloudflare üåç Great articles about Cloudflare üåê Vercel Vercel is a frontend deployment platform that helps deploy web applications to the cloud quickly. It supports React, Next.js, Vue, Angular, integrates with GitHub, and allows deployment with just a push command.\nüìñ Free resources to learn:\nüîó Vercel Homepage üìñ Official Vercel Documentation üé• Vercel Usage Guide üåç Great articles about Vercel üìå Conclusion Serverless helps automate deployment, reduce costs, and easily scale. Popular platforms:\nAWS Lambda: Event processing without servers. Cloudflare: CDN and website security. Vercel: Fast frontend deployment. Platform selection depends on your needs, technology, and budget. üöÄ\nüëâ Next step: Learn about Provisioning - the process of providing and configuring resources (servers, networks, storage, accounts) so that systems or applications can operate effectively.\n","date":"2025-02-24T00:00:00Z","image":"https://tech.nguuyen.io.vn/images/devops/devops-step-ten.webp","permalink":"https://tech.nguuyen.io.vn/en/posts/devops/devops-step-ten/","title":"Serverless and Related Platforms ‚òÅÔ∏è"},{"content":"Setting up important network components This article will help you understand important network components:\nüîπ Forward Proxy\nüîπ Reverse Proxy\nüîπ Load Balancer\nüîπ Firewall\nüîπ Caching Server\nüîπ Web Server\n‚öñÔ∏è Load Balancer Load Balancer works like a \u0026ldquo;traffic cop\u0026rdquo; standing in front of servers and directing client requests to appropriate servers. This helps optimize speed, efficiently utilize resources, and avoid overload situations.\nüîπ If a server fails, the Load Balancer will redirect traffic to the remaining servers.\nüîπ Can be deployed with algorithms like Round Robin, Least Connections, IP Hash\u0026hellip;\nüîç Example Load Balancer configuration with Nginx: 1 2 3 4 5 6 7 8 9 10 11 upstream backend_servers { server backend1.example.com; server backend2.example.com; } server { listen 80; location / { proxy_pass http://backend_servers; } } üìö Further reading:\nüìÑ What is Load Balancing?\nüìÑ Load Balancing Algorithms\nüìÑ Nginx Reverse Proxy \u0026amp; Load Balancing\nüé• Video: How does Load Balancer work?\nüîÅ Forward Proxy Forward Proxy is an intermediary server standing between client and internet, forwarding requests from client to destination server. It helps with anonymity, security, access control, and content caching.\nüîπ Commonly used in enterprise networks to monitor and control access.\nüîπ Supports bypassing censorship and geographical restrictions.\nüîç Example Forward Proxy configuration with Squid: 1 2 3 4 5 6 7 8 9 10 11 apt update \u0026amp;\u0026amp; apt install squid -y # Edit configuration file nano /etc/squid/squid.conf # Add simple configuration http_access allow all http_port 3128 # Restart service systemctl restart squid üìö Further reading:\nüìÑ What is Forward Proxy?\nüìÑ Forward Proxy vs Reverse Proxy comparison\nüé• Video: How does Proxy work?\nüîÑ Reverse Proxy Reverse Proxy is an intermediary server that receives requests from clients and forwards them to appropriate backend servers. It helps with load balancing, caching, security, and SSL termination.\nüîπ Helps hide backend server information to enhance security.\nüîπ Supports traffic distribution and application performance optimization.\nüîç Example Reverse Proxy configuration with Nginx: 1 2 3 4 5 6 7 8 9 10 server { listen 80; server_name example.com; location / { proxy_pass http://backend_server; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; } } üìö Further reading:\nüìÑ What is Reverse Proxy?\nüìÑ Nginx Reverse Proxy Guide\nüé• Video: Reverse Proxy and practical applications\nüî• Firewall Firewall is a network security device that monitors and filters incoming/outgoing traffic based on organization\u0026rsquo;s security policies.\nüîπ Prevents unauthorized access to internal systems.\nüîπ Supports data traffic control rules.\nüîç Example Firewall configuration with UFW (Uncomplicated Firewall): 1 2 3 4 5 6 7 8 9 10 11 # Install UFW apt install ufw -y # Open SSH port ufw allow 22/tcp # Block all other connections ufw default deny incoming # Enable UFW ufw enable üìö Further reading:\nüìÑ What is Firewall?\nüìÑ Common Firewall types\nüé• Video: Introduction to Firewall\nüåê Nginx Nginx is an open-source web server, widely used for its ability to handle many concurrent connections with high performance.\nüîπ Supports web server, reverse proxy, load balancing, caching.\nüîπ Suitable for microservices systems and containers.\nüîç Example simple Nginx configuration: 1 2 3 4 5 6 server { listen 80; server_name example.com; root /var/www/html; index index.html; } üìö Further reading:\nüìÑ Nginx installation guide on Ubuntu\nüé• Video: Nginx in 100 seconds\nüèõÔ∏è Apache Apache is one of the most popular web servers, supporting many extension modules and compatible with many operating systems.\nüîπ Easy to configure with .conf files.\nüîπ Supports SSL/TLS, user authentication, URL rewriting\u0026hellip;\nüîç Example simple Apache configuration: 1 2 3 4 5 6 7 8 \u0026lt;VirtualHost *:80\u0026gt; ServerName example.com DocumentRoot /var/www/html \u0026lt;Directory /var/www/html\u0026gt; AllowOverride All Require all granted \u0026lt;/Directory\u0026gt; \u0026lt;/VirtualHost\u0026gt; üìö Further reading:\nüìÑ Apache homepage\nüé• Video: Installing Apache on Ubuntu\n‚úÖ Conclusion üîπ Load Balancer helps distribute traffic efficiently, reducing server load.\nüîπ Forward Proxy supports anonymity, caching, and access control from client.\nüîπ Reverse Proxy helps enhance security, caching, and optimize backend systems.\nüîπ Firewall protects systems from unauthorized access.\nüîπ Nginx \u0026amp; Apache are two popular web servers, serving web content and applications.\nBy implementing these components, you can build a powerful, secure, and efficient network system. üöÄ\nüëâ Next step: Learn about Networking Protocols - a set of rules and standards that define how devices in a network communicate with each other. They ensure data is transmitted accurately, securely, and efficiently between different systems.\n","date":"2025-02-23T00:00:00Z","image":"https://tech.nguuyen.io.vn/images/devops/devops-step-seven.webp","permalink":"https://tech.nguuyen.io.vn/en/posts/devops/devops-step-seven/","title":"Application Gateway üåê"},{"content":"üåê Cloud Providers Cloud service providers offer an API layer to abstract infrastructure, enabling resource deployment based on security standards and billing models. While cloud services actually run on servers in data centers, abstraction layers create the feeling of interacting with a single platform. The ability to quickly deploy, configure, and secure resources has made cloud a critical factor in the success and complexity of modern DevOps.\nüìñ Free resources to learn:\nüìÑ Cloud Service Provider üìÑ What are Cloud Providers? üåç Great articles about Cloud ‚òÅÔ∏è AWS (Amazon Web Services) AWS has been the leading cloud computing platform since 2011, far ahead of Azure and Google Cloud. AWS provides over 200 services, operating on a global scale. AWS delivers flexible and cost-effective computing solutions, including: computing power, data storage, content distribution, etc.\nüñ•Ô∏è Example: Create an EC2 instance using AWS CLI\n1 aws ec2 run-instances --image-id ami-12345678 --count 1 --instance-type t2.micro --key-name MyKeyPair --security-groups MySecurityGroup üìñ Free resources to learn:\nüé• 100 hours AWS course - 2024 üîó AWS Homepage üìÑ Guide to creating AWS account üåç Great articles about AWS üí† Microsoft Azure Azure is Microsoft\u0026rsquo;s cloud computing platform, providing IaaS, PaaS, SaaS along with many services like analytics, AI, machine learning, security. Azure supports multiple tools and programming languages, helping businesses develop rapidly.\nüñ•Ô∏è Example: Deploy application on Azure App Service\n1 az webapp create --resource-group MyResourceGroup --plan MyAppServicePlan --name MyUniqueApp --runtime \u0026#34;PYTHON:3.8\u0026#34; üìñ Free resources to learn:\nüîó Azure Homepage üìñ Microsoft Azure Guide üé• Azure Fundamentals Certification (AZ-900) üåç Great articles about Azure ‚òÅÔ∏è Google Cloud Platform (GCP) Google Cloud provides over 150 services, running on the same infrastructure as Google products like Search, Gmail, YouTube. Services include: VMs, databases, AI/ML, Kubernetes, etc.\nüñ•Ô∏è Example: Create a VM on Google Cloud\n1 gcloud compute instances create my-instance --machine-type=e2-medium --image-project=debian-cloud --image-family=debian-11 üìñ Free resources to learn:\nüîó Google Cloud Homepage üìñ 5 tips to become Google Cloud Certified üé• Google Cloud Platform Course - 2023 üåç Great articles about Google Cloud üåä DigitalOcean DigitalOcean is a cloud infrastructure provider focused on simplicity, low cost, ease of use. DigitalOcean provides services like virtual machines (Droplets), databases, Kubernetes, object storage, suitable for startups and developers.\nüñ•Ô∏è Example: Create a Droplet on DigitalOcean using API\n1 curl -X POST -H \u0026#34;Content-Type: application/json\u0026#34; -H \u0026#34;Authorization: Bearer YOUR_TOKEN\u0026#34; -d \u0026#39;{\u0026#34;name\u0026#34;:\u0026#34;example-droplet\u0026#34;,\u0026#34;region\u0026#34;:\u0026#34;nyc3\u0026#34;,\u0026#34;size\u0026#34;:\u0026#34;s-1vcpu-1gb\u0026#34;,\u0026#34;image\u0026#34;:\u0026#34;ubuntu-20-04-x64\u0026#34;}\u0026#39; \u0026#34;https://api.digitalocean.com/v2/droplets\u0026#34; üìñ Free resources to learn:\nüîó DigitalOcean Homepage üìÑ DigitalOcean\u0026rsquo;s Hacktoberfest üé• Kubernetes on DigitalOcean Tutorial üåç Great articles about DigitalOcean üìå Conclusion Cloud service providers like AWS, Azure, GCP, DigitalOcean provide flexible solutions for all server, storage, AI, DevOps needs. Each platform has its own advantages:\nAWS: Comprehensive, most services available. Azure: Good integration with Microsoft ecosystem. GCP: Optimized for AI, big data. DigitalOcean: Simple, suitable for startups. Choosing the right platform depends on your goals, budget, technical requirements. üöÄ\nüëâ Next step: Learn about Serverless - a cloud computing model that allows running applications without managing servers. Cloud providers automatically allocate resources, scale, and charge based on actual resource usage, helping optimize costs and simplify deployment.\n","date":"2025-02-23T00:00:00Z","image":"https://tech.nguuyen.io.vn/images/devops/devops-step-nine.webp","permalink":"https://tech.nguuyen.io.vn/en/posts/devops/devops-step-nine/","title":"Cloud Services üå©Ô∏è"},{"content":"üì¶ Containers, Docker and LXC Containers are lightweight, portable, and isolated environments that help package applications along with all their dependencies, ensuring consistent deployment across different environments. Container technology simplifies application deployment processes, supports microservices architecture models, and optimizes system resources.\nüèóÔ∏è What are Containers? Containers are an operating system-level virtualization method that allows running multiple isolated applications on the same kernel. Unlike virtual machines (VMs) that require separate operating systems for each environment, containers only use the host operating system\u0026rsquo;s kernel, helping reduce resource costs and increase performance.\nüéØ Key Characteristics of Containers üèãÔ∏è Lightweight: Share kernel with host operating system, reducing resource consumption. üöÄ Portable: Run consistently across multiple platforms from personal computers to cloud. üîí Isolated: Applications and libraries are packaged separately. üìà High Performance: No need to boot separate operating systems like virtual machines. üê≥ Docker - The Most Popular Container Platform Docker is an open-source platform that helps automate application deployment using container technology. Docker helps package applications with all necessary libraries and configurations to run across different environments.\n‚ú® Notable Features of Docker üì¶ Docker Engine: Tool for creating and running containers. üîÑ Docker Compose: Manage multiple containers in one application. üèóÔ∏è Docker Hub: Repository for storing and sharing container images. üîç Docker Usage Example: 1 docker run -d -p 80:80 nginx The above command will run an Nginx container on port 80.\nüìö Useful Resources:\nüìñ Docker Documentation üé• Docker in 5 Minutes üñ•Ô∏è LXC - Linux Containers LXC (Linux Containers) is an operating system-level virtualization method that allows running multiple isolated Linux systems on the same kernel.\nüõ†Ô∏è LXC Characteristics: üèóÔ∏è Creates environments similar to virtual machines but with higher performance. ‚ö° Faster startup compared to traditional VMs. üîç Uses Linux technologies like cgroups and namespaces. üìå Example of Creating an LXC Container: 1 2 lxc-create -n my-container -t ubuntu lxc-start -n my-container -d üìö Useful Resources:\nüìñ LXC Homepage üé• LXC Usage Guide üéØ Conclusion Containers help deploy applications quickly, efficiently, and save resources. Docker is a popular choice for application development, while LXC is more suitable for full operating system simulation. Choose the tool that fits your needs! üöÄ\nüëâ Next Step: Learn about Application Gateway - an application-layer traffic management service that helps optimize, secure, and control access flow between clients and backends. It can act as a reverse proxy, protecting the system and ensuring requests are processed correctly.\n","date":"2025-02-23T00:00:00Z","image":"https://tech.nguuyen.io.vn/images/devops/devops-step-six.webp","permalink":"https://tech.nguuyen.io.vn/en/posts/devops/devops-step-six/","title":"Containers, Docker and LXC üèóÔ∏è"},{"content":"üñß Networking Protocols Networking protocols are a set of standardized rules that help data be transmitted, received, and understood correctly across computer networks. They define the format, timing, sequence, and error control during data transmission. Some important protocols include:\nTCP/IP: The foundational protocol suite for Internet communication. HTTP/HTTPS: Hypertext transfer protocol used for the web. FTP/SFTP: File transfer protocols. SMTP/POP3/IMAP: Email transmission protocols. DNS: Domain name resolution protocol. DHCP: Automatic IP address allocation protocol. SSL/TLS: Data security protocols. UDP: Connectionless, fast transmission protocol. üåç Domain Name System (DNS) DNS (Domain Name System) is a domain name resolution system that helps convert memorable domain names (e.g., www.example.com) into IP addresses (192.168.1.1) that computers can understand.\nüîπ Example DNS configuration in Linux: 1 2 3 4 5 6 7 8 9 # Check DNS for a domain name nslookup example.com dig example.com # Edit hosts file to map domain names sudo nano /etc/hosts # Add the following line: 192.168.1.100 mycustomdomain.com üîó Reference resources:\nüìÑ How DNS works üé• DNS explanation video üåê HTTP Protocol HTTP (Hypertext Transfer Protocol) is a data transmission protocol on the web following a request-response model.\nüîπ Example sending HTTP requests with cURL: 1 2 3 4 5 6 7 # Send GET request curl -X GET https://jsonplaceholder.typicode.com/posts/1 # Send POST request curl -X POST https://jsonplaceholder.typicode.com/posts \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;title\u0026#34;: \u0026#34;Hello\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;World\u0026#34;}\u0026#39; üîó Reference resources:\nüìÑ Learn about HTTP üé• HTTP tutorial video üîí HTTPS and Security (SSL/TLS) HTTPS is the secure version of HTTP, using SSL/TLS to encrypt data, ensuring safety during transmission over the Internet.\nüîπ Example setting up HTTPS with Nginx: 1 2 3 4 5 6 server { listen 443 ssl; server_name example.com; ssl_certificate /etc/letsencrypt/live/example.com/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/example.com/privkey.pem; } üîó Reference resources:\nüìÑ What is HTTPS? üé• How HTTPS works video üîë SSH - Secure Connection SSH (Secure Shell) is a protocol that enables secure connection to remote servers.\nüîπ Example using SSH for remote connection: 1 2 3 4 5 # Connect to remote server ssh user@example.com # Copy files from server to local machine scp user@example.com:/path/to/file ./localfile üîó Reference resources:\nüìÑ SSH tutorial üé• How SSH works video üéØ Conclusion Networking protocols are the foundation of all online systems, from web browsing to sending emails. Understanding and knowing how to use them helps improve system security and performance. Try applying the commands above to check and configure your system! üöÄ\nüëâ Next step: Learn about Cloud Providers - companies that provide cloud computing services, allowing individuals and businesses to access resources like servers, storage, databases, AI, and other services over the internet without investing in hardware infrastructure.\n","date":"2025-02-23T00:00:00Z","image":"https://tech.nguuyen.io.vn/images/devops/devops-step-eight.webp","permalink":"https://tech.nguuyen.io.vn/en/posts/devops/devops-step-eight/","title":"Networking Protocols üñß"},{"content":"üåç Translation in Progress We are gradually translating the entire website to English to serve our international readers better.\nCurrently Available in English: About page DevOps series (in progress) Selected technical articles Coming Soon: Complete DevOps tutorial series Docker optimization guides Nginx installation guides All technical content Thank you for your patience as we work to make all content accessible in English! üöÄ\n","date":"2025-02-23T00:00:00Z","permalink":"https://tech.nguuyen.io.vn/en/posts/devops/notice/","title":"Notice"},{"content":"üåç Source Code Hosting Services (Repo Hosting Services) When working in teams, you need a remote place to store source code so everyone can access it, create their own branches, and create or review pull requests. These services typically include issue tracking, code review, and continuous integration (CI/CD). Popular options include GitHub, GitLab, Bitbucket, and AWS CodeCommit.\nüìö Free resources to learn more:\nüîó GitHub üîó GitLab üîó BitBucket üé• GitHub vs GitLab vs Bitbucket - Which one to choose? üêô GitHub GitHub is a Git-based source code management platform that provides cloud-based code repository hosting services. It supports features like bug tracking, task management, and project wikis. GitHub enables code review through pull requests, issue tracking, and supports collaborative programming with features like fork and star.\nGitHub supports both public and private repositories, making it a popular choice for both open-source projects and personal development. The GitHub ecosystem includes:\nüöÄ GitHub Actions: Workflow automation. üì¶ GitHub Packages: Software package management. üåê GitHub Pages: Free static website hosting. üìö Free resources to learn more:\nüó∫Ô∏è Git \u0026amp; GitHub Roadmap üîó GitHub Homepage üìñ How to use Git in professional development teams üé• What is GitHub? üì∞ Great articles about GitHub ü¶ä GitLab GitLab is a comprehensive DevOps tool that provides Git repository management along with wikis, issue tracking, and built-in CI/CD features. It\u0026rsquo;s a complete DevOps platform covering all stages from planning, development, testing to deployment and monitoring.\nGitLab supports both cloud and self-hosted versions, suitable for organizations with high security requirements. Some notable GitLab features include:\nüîÑ Built-in CI/CD: Supports automated testing and deployment. üì¶ Container \u0026amp; Package Registry: Package management and storage. üîé Source code security scanning: Detects vulnerabilities in code. üìö Free resources to learn more:\nüîó GitLab Homepage üìñ Official GitLab Documentation üé• What is GitLab and why use it? üì∞ Great articles about GitLab üèóÔ∏è Bitbucket Bitbucket is Atlassian\u0026rsquo;s source code repository hosting service that supports both Git and Mercurial. It integrates tightly with other Atlassian tools like Jira and Trello, making project management easier. Bitbucket offers both cloud and self-hosted versions.\nSome notable Bitbucket features:\nüîç Code Review \u0026amp; Pull Requests: Supports code review. üîÑ Bitbucket Pipelines: Built-in CI/CD. üìñ Wiki \u0026amp; Issue Tracking: Documentation management and issue tracking. üîê Free private repository support: Suitable for small teams. üìö Free resources to learn more:\nüîó Bitbucket Homepage üìñ Bitbucket Overview üìö Introduction to Git and Bitbucket üé• Bitbucket Cloud Tutorial üì∞ Great articles about Bitbucket üìå Conclusion Choosing a source code repository hosting service depends on your development team\u0026rsquo;s needs. If you need a popular platform with a vast ecosystem, GitHub is a strong choice. If you want a fully integrated DevOps solution, GitLab would be more suitable. If you\u0026rsquo;re already using the Atlassian ecosystem, Bitbucket would be the best choice.\nConsider your project needs and desired level of integration to make the right decision! üöÄ\nüëâ Next step: Learn about Containers that help package applications along with all libraries, configurations, and dependencies to run consistently across different environments.\n","date":"2025-02-23T00:00:00Z","image":"https://tech.nguuyen.io.vn/images/devops/devops-step-five.webp","permalink":"https://tech.nguuyen.io.vn/en/posts/devops/devops-step-five/","title":"Source Code Hosting Services üê±"},{"content":"üñ•Ô∏è Terminal Knowledge Terminal is a text interface that allows users to interact with computer systems through CLI (Command Line Interface). It is an essential tool for system management, command execution, and task automation.\nüìö Free Resources üìÑ Article: What is CLI? üîç Google Search ‚ñ∂Ô∏è YouTube Search üîπ Example:\n1 2 ls -l # List files in current directory pwd # Display current directory path üìä Process Monitoring Process monitoring is the continuous observation and analysis of processes in IT systems to ensure performance, efficiency, and compliance. It helps track important metrics such as resource usage and behavior of individual processes or applications running in the system.\nüèÜ Recommended Tools üü£ lsof - Lists information about files opened by processes. üìö Free Resources üìÑ Lsof Cheat Sheet üìñ lsof Documentation ‚ñ∂Ô∏è Video: Linux Crash Course - lsof Command üìù Great Articles on Monitoring üîπ Example:\n1 2 lsof -i :80 # List processes using port 80 ps aux # Display all running processes üöÄ Performance Monitoring Performance monitoring helps collect, analyze, and report key performance metrics from applications, networks, servers, and databases.\nüèÜ Recommended Tools üü£ vmstat - Virtual memory and system performance monitoring tool. üìö Free Resources üìñ Linux Commands: Exploring Virtual Memory with vmstat üìÑ vmstat Documentation ‚ñ∂Ô∏è vmstat Tutorial üìù Great Articles on Monitoring üîπ Example:\n1 vmstat 5 10 # Update system status every 5 seconds for 10 times üåê Networking Tools Networking tools support monitoring, analyzing, troubleshooting, and managing network systems.\nüèÜ Recommended Tools üü£ Wireshark - Deep packet analysis. üü£ Nmap - Network scanning and security testing. Ping - Basic connectivity testing. Traceroute - Determine packet path through network. Netstat - Display network connections. Tcpdump - Command-line packet capture and analysis. Iperf - Network performance testing. Netcat - Perform various network tasks. Nslookup/Dig - DNS queries. PuTTY - Remote connection via SSH or Telnet. üîπ Example:\n1 2 ping google.com # Test connection to Google nmap -sS 192.168.1.1 # Scan internal server ports ‚úÇÔ∏è Text Manipulation Tools that support editing, processing, and converting text data.\nüèÜ Recommended Tools üü£ sed - Stream editor for data manipulation. üü£ awk - Pattern scanning and data extraction. üü£ grep - Text search using regular expressions. cut, sort, tr, uniq - Supporting commands for text data processing. üîπ Example:\n1 2 grep \u0026#34;error\u0026#34; logfile.txt # Find \u0026#34;error\u0026#34; in logfile.txt awk \u0026#39;{print $1}\u0026#39; data.txt # Get first column from data.txt ‚ö° Bash Scripts Bash is a powerful shell on Unix/Linux that helps execute commands and automate tasks.\nüîπ Example:\n1 2 #!/bin/bash echo \u0026#34;Hello, World!\u0026#34; ‚úçÔ∏è Editors Text editors are essential tools for editing and managing text files.\nüèÜ Recommended Tools üü£ Vim - Powerful, highly customizable, suitable for programmers. üü£ Emacs - Flexible with many supporting plugins. Sublime Text - High speed, user-friendly interface. Visual Studio Code - Open source, supports debugging, extensions, integrated development tools. üîπ Example:\n1 2 vim myfile.txt # Open file with Vim nano myfile.txt # Open file with Nano üîö Conclusion Understanding and mastering these tools helps you work more efficiently in Linux and DevOps environments. Tools üü£ marked are the most popular and powerful ones, recommended by many experts. You can learn more through the accompanying free resources. If there\u0026rsquo;s anything that needs clarification or addition, please provide feedback so I can update accordingly!\nüëâ Next step: Advance your knowledge of Version Control Systems to effectively track, manage, and collaborate on source code.\n","date":"2025-02-23T00:00:00Z","image":"https://tech.nguuyen.io.vn/images/devops/devops-step-three.webp","permalink":"https://tech.nguuyen.io.vn/en/posts/devops/devops-step-three/","title":"Terminal Knowledge üìü"},{"content":"üîÑ Version Control Systems Version control systems (VCS) are tools that help track changes to source code and files over time. They support team collaboration, manage change history, and maintain multiple versions of source code. There are two main types of VCS:\nCentralized VCS (CVCS): Uses a central repository, examples include Subversion (SVN), CVS. Distributed VCS (DVCS): Each user has a complete copy of the repository, including the entire history. The most popular example is Git. Git is a powerful distributed version control system that allows offline work, supports fast branching and merging operations, enhancing collaboration capabilities.\nüõ†Ô∏è Git - The Most Popular Version Control Tool üîπ Installing Git If you haven\u0026rsquo;t installed Git yet, you can download it from git-scm.com or use the following commands:\n1 2 3 sudo apt install git # Ubuntu/Debian yum install git # CentOS/RHEL brew install git # macOS Verify Git installation:\n1 2 3 4 git --version # output: # git version 2.47.1.windows.1 üöÄ Basic Git Commands Below are common Git commands, organized from basic to advanced:\nInitialization \u0026amp; Configuration 1 git init # Initialize Git repository 1 2 git config --global user.name \u0026#34;Your Name\u0026#34; # Configure name git config --global user.email \u0026#34;email@example.com\u0026#34; # Configure email Working With Repository 1 git clone \u0026lt;repo_url\u0026gt; # Clone a remote repository to local machine 1 git status # Check file status Adding \u0026amp; Saving Changes 1 git add \u0026lt;file\u0026gt; # Add file to staging area 1 git commit -m \u0026#34;Change description\u0026#34; # Save changes to history Working With Remote Repository 1 git remote add origin \u0026lt;repo_url\u0026gt; # Link remote repository 1 git push -u origin main # Push changes to main branch 1 git pull origin main # Update latest changes from remote repository Working With Branches 1 git branch new-feature # Create new branch 1 git checkout new-feature # Switch to new branch 1 git merge new-feature # Merge branch into current branch Tracking History 1 git log # View commit history 1 git diff # Compare changes between versions üìö Free Git Learning Resources üìñ Official Git Documentation üìÑ Git Cheat Sheet ‚ñ∂Ô∏è Git Tutorial Video for Beginners üìù Article: What is Version Control System? üîö Conclusion Using Git makes source code management easier, supports effective team collaboration, and protects important project data. Understanding and mastering Git is an essential skill for every programmer.\nüëâ Next step: Learn about GitHub \u0026amp; GitLab to manage Git repositories on cloud platforms.\n","date":"2025-02-23T00:00:00Z","image":"https://tech.nguuyen.io.vn/images/devops/devops-step-four.webp","permalink":"https://tech.nguuyen.io.vn/en/posts/devops/devops-step-four/","title":"Version Control Systems üõ†Ô∏è"},{"content":"üìå Why is Linux important in DevOps? Linux is the foundation of most server systems, containers (Docker, Kubernetes), and cloud platforms. DevOps professionals need to master Linux to:\n‚úÖ Manage servers effectively. ‚úÖ Write automation scripts. ‚úÖ Handle files, users, and processes. ‚úÖ Optimize systems and security. ‚öôÔ∏è What is an Operating System? An operating system (OS) is software that manages computer hardware and software resources, providing common services for programs. It acts as an intermediary between applications and hardware, handling tasks such as:\nüîπ Memory management. üîπ Process scheduling. üîπ File system management. üîπ Device control. üåç Popular operating systems: üíª Personal computers: Windows, macOS, Linux (Ubuntu, Fedora,\u0026hellip;) üì± Mobile devices: iOS, Android üñ•Ô∏è Servers: Ubuntu Server, Red Hat Enterprise Linux, Windows Server Each operating system has different characteristics, interfaces, and compatibility capabilities. They play crucial roles in system security, performance optimization, and providing consistent user experiences.\nüõ†Ô∏è Basic Linux Commands Here are some important Linux commands:\nüîç System Information 1 2 3 uname -a # Display operating system information uptime # System uptime free -m # Check RAM memory üìÇ File \u0026amp; Directory Management 1 2 3 ls -l # List files with detailed information mkdir mydir # Create new directory rm -rf mydir # Delete directory and its contents üöÄ Process Management 1 2 3 top # Display running processes ps aux # List all processes kill -9 PID # Stop process by PID üë§ User Management 1 2 3 whoami # View current user sudo useradd devops # Create new user sudo passwd devops # Set password for user üìú Bash Script for System Resource Monitoring 1 2 3 4 5 6 7 8 9 10 #!/bin/bash echo \u0026#34;==== System Information ====\u0026#34; uname -a echo \u0026#34;==== System Uptime ====\u0026#34; uptime echo \u0026#34;==== RAM Memory ====\u0026#34; free -m ‚ñ∂Ô∏è How to run the script: 1 2 chmod +x system_check.sh # Grant execute permission to script ./system_check.sh # Run script in terminal üìö Learning Resources Here are some free resources to learn more about operating systems:\nüìñ Operating Systems - Wiki üìñ All you need to know about OS üìñ Learn Operating Systems üé• What are Operating Systems? üé• Operating Systems üéØ Conclusion ‚úÖ Linux is a mandatory skill in DevOps. ‚úÖ Learn to use terminal \u0026amp; Bash scripting. üëâ Next step: Learn more about terminal and CLI usage to work effectively with systems.\n","date":"2025-02-22T00:00:00Z","image":"https://tech.nguuyen.io.vn/images/devops/devops-step-two.webp","permalink":"https://tech.nguuyen.io.vn/en/posts/devops/devops-step-two/","title":"Learning Linux \u0026 Operating Systems üñ•Ô∏è"},{"content":"üìå Why do you need to choose a programming language? In DevOps, you will need to use programming languages to:\n‚úÖ Write automation scripts. ‚úÖ Manage servers and cloud. ‚úÖ Create tools to support CI/CD. ‚úÖ Build and deploy Infrastructure as Code (IaC). Choosing the right language helps you work more efficiently with systems, automate many processes, and improve software development speed.\nüî• Suitable languages for DevOps üêç Python (Main recommendation) üîπ Reasons to choose Python: Easy-to-read syntax, easy to learn. Rich libraries supporting automation like fabric, paramiko, boto3 (AWS SDK), pyinfra. Strong support in Cloud management (AWS, GCP, Azure). üîπ Practical applications: Write automated code deployment scripts. Create server management bots. Build system management APIs. üìù Example: Automated SSH deployment script with Paramiko 1 2 3 4 5 6 7 8 9 10 11 import paramiko def deploy_code(host, user, password, command): client = paramiko.SSHClient() client.set_missing_host_key_policy(paramiko.AutoAddPolicy()) client.connect(hostname=host, username=user, password=password) stdin, stdout, stderr = client.exec_command(command) print(stdout.read().decode()) client.close() deploy_code(\u0026#39;192.168.1.100\u0026#39;, \u0026#39;ubuntu\u0026#39;, \u0026#39;yourpassword\u0026#39;, \u0026#39;git pull origin main \u0026amp;\u0026amp; systemctl restart app\u0026#39;) üñ•Ô∏è Bash (Need to know basics) üîπ Reasons to choose Bash: The most popular shell script on Linux. Helps you work quickly with the system. Optimized for server management and small task automation. üîπ Practical applications: Write automated server update scripts. Create periodic cron jobs. Manage users and permissions on Linux. üìù Example: Automated server update script 1 2 #!/bin/bash sudo apt update \u0026amp;\u0026amp; sudo apt upgrade -y üöÄ Go (Golang) (If working with Kubernetes) üîπ Reasons to choose Go: High performance, easy to compile into compact binaries. Kubernetes and many DevOps tools like Terraform are written in Go. üîπ Practical applications: Write container management tools. Create plugins for Kubernetes. Build custom DevOps tools. üìù Example: Display system information with Go 1 2 3 4 5 6 7 8 9 10 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; ) func main() { hostname, _ := os.Hostname() fmt.Println(\u0026#34;Hostname:\u0026#34;, hostname) } ‚öôÔ∏è Groovy (If working with Jenkins) üîπ Reasons to choose Groovy: The main language for writing pipelines in Jenkins. Flexible syntax, easy to extend and integrate with Java. üîπ Practical applications: Write CI/CD pipelines for Jenkins. Create system management scripts. Automate build, test, deploy steps. üìù Example: Basic pipeline in Jenkinsfile 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 pipeline { agent any stages { stage(\u0026#39;Build\u0026#39;) { steps { echo \u0026#39;Building the project...\u0026#39; sh \u0026#39;mvn clean package\u0026#39; } } stage(\u0026#39;Test\u0026#39;) { steps { echo \u0026#39;Running tests...\u0026#39; sh \u0026#39;mvn test\u0026#39; } } stage(\u0026#39;Deploy\u0026#39;) { steps { echo \u0026#39;Deploying application...\u0026#39; sh \u0026#39;./deploy.sh\u0026#39; } } } } üéØ Conclusion ‚úÖ Python + Bash is the best choice to start with DevOps. ‚úÖ If working with Kubernetes, learn Go as well. ‚úÖ If working with Jenkins, learn Groovy to write pipelines. üëâ Next step: Learn the basics of Linux \u0026amp; operating systems.\n","date":"2025-02-21T00:00:00Z","image":"https://tech.nguuyen.io.vn/images/devops/devops-step-one.webp","permalink":"https://tech.nguuyen.io.vn/en/posts/devops/devops-step-one/","title":"Choosing a Programming Language ü§ó"},{"content":"Understanding DevOps DevOps is the combination of software development and system operations, aimed at enhancing collaboration and automation in the software development and deployment process.\nLearn a Programming Language Mastering at least one programming language is essential for effective automation and system management.\nPopular Languages\nPython Go Ruby Master Operating System Knowledge Linux: Popular operating system in server environments. Windows: Important in enterprises using Microsoft infrastructure. Learn About Computer Networks and Security Key topics to focus on:\nNetwork Protocols: HTTP, HTTPS, FTP, TCP/IP. Network Security: Firewalls, VPN, SSL/TLS. Use Source Code Management Tools Effective source code management is a crucial element in DevOps.\nGit: Popular version control system. Understand Configuration Management and Infrastructure as Code Configuration automation helps maintain consistency and efficiency.\nAnsible: Automation tool. Terraform: Infrastructure management by defining it in source code. Master Containerization and Orchestration Docker: Popular container platform. Kubernetes: Powerful container orchestration system. Set Up and Manage CI/CD Jenkins: Open-source automation server. GitLab CI/CD: Effective CI/CD support. Monitoring and Logging Prometheus: Monitoring system. ELK stack: Log analysis toolkit. Learn About Cloud Services Popular providers:\nAWS Google Cloud Microsoft Azure Conclusion Becoming a DevOps engineer requires broad knowledge and deep practical skills. Keep learning and practicing continuously to achieve your goals.\nNote: This roadmap is compiled from multiple sources and real-world experience, aiming to provide you with the most comprehensive and detailed view of the path to becoming a DevOps engineer. üéØüöÄ\nDevOps Roadmap 2025 ","date":"2025-02-20T00:00:00Z","image":"https://tech.nguuyen.io.vn/images/devops/devops-roadmap.webp","permalink":"https://tech.nguuyen.io.vn/en/posts/devops/devops-roadmap/","title":"DevOps Engineer Roadmap üòé"},{"content":"Introduction to DevOps DevOps is a methodology that combines software development (Development - Dev) and system operations (Operations - Ops) to optimize the process of developing, deploying, and operating applications. DevOps helps development and operations teams work together more effectively through tools, automated processes, and collaborative culture.\nWhy is DevOps Important? Accelerated Development Speed DevOps helps automate processes like testing, deployment, and monitoring, reducing time-to-market for products.\nImproved Product Quality Integration of automated testing and CI/CD helps detect bugs early, minimizing risks during software deployment.\nEnhanced Reliability Monitoring and logging tools help quickly identify issues, reduce downtime, and ensure systems operate stably.\nBetter Team Collaboration DevOps breaks down barriers between development and operations teams, creating a more effective collaborative work environment.\nKey Components of DevOps CI/CD (Continuous Integration \u0026amp; Continuous Deployment) CI/CD automates the process of source code integration, testing, and deployment, minimizing errors when releasing products to production environments.\nInfrastructure as Code (IaC) IaC allows infrastructure management as code, making it easy to deploy and scale systems.\nMonitoring and Logging Tools like Prometheus, Grafana, ELK Stack help monitor and analyze logs to quickly handle incidents.\nContainerization and Orchestration Docker and Kubernetes help package, manage, and scale applications flexibly.\nPopular DevOps Tools CI/CD: Jenkins, GitHub Actions, GitLab CI/CD IaC: Terraform, Ansible, CloudFormation Monitoring: Prometheus, Grafana, ELK Stack Container \u0026amp; Orchestration: Docker, Kubernetes Source Code Management: Git, GitHub, GitLab Conclusion DevOps is an important methodology that helps improve development speed, product quality, and optimize system operations. In the upcoming articles of this series, we will explore each aspect of DevOps in depth, from CI/CD, Infrastructure as Code to system monitoring.\nDocker Example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Use Node.js as the base FROM node:18 # Set working directory in container WORKDIR /app # Copy package.json and install dependencies COPY package.json . RUN npm install # Copy entire source code into container COPY . . # Expose port 3000 for the application EXPOSE 3000 # Command to run the application CMD [\u0026#34;npm\u0026#34;, \u0026#34;start\u0026#34;] üí° Follow the blog to stay updated with the latest DevOps articles!\n","date":"2025-02-19T00:00:00Z","image":"https://tech.nguuyen.io.vn/images/devops/devops-intro.webp","permalink":"https://tech.nguuyen.io.vn/en/posts/devops/devops-intro/","title":"What is DevOps?"}]